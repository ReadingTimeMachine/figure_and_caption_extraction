{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64e9aaa0",
   "metadata": {},
   "source": [
    "# Pull images to classify with MakeSense.ai\n",
    "\n",
    "Randomly pulled, tracked, for use with [MakeSense.ai](https://www.makesense.ai/) as a possible annotation interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "304b0cff-4b2d-43fd-9929-6c18d859ad3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "96d8c242-733f-408f-a9d5-4ef82131fec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# where to look for already completed MakeSense annotations\n",
    "#check_makesense = '/Users/jillnaiman/Dropbox/wwt_image_extraction/FigureLocalization/Annotations/MakeSenseAnnotations/'\n",
    "check_makesense = '/Users/jillnaiman/Dropbox/wwt_image_extraction/FigureLocalization/BenchMarks/Annotations_pmcnoncom/MakeSenseAnnotations/'\n",
    "\n",
    "# where to save new annotations\n",
    "save_makesense = check_makesense\n",
    "#save_makesense = '/Users/jillnaiman/Dropbox/wwt_image_extraction/FigureLocalization/Annotations/MakeSenseAnnotations/test/'\n",
    "\n",
    "# which model and weights to use?\n",
    "binary_dirs = 'binaries_model11_tfrecordz/'\n",
    "weightsFileDir = config.save_weights_dir +'saved_weights/'+'20211216_model11tfz/'\n",
    "weightsFile = 'training_1model11_tfrec_model_l0.1890166.h5' # figure/table, fig/table captions\n",
    "\n",
    "# how many you wanna grab?\n",
    "nRandom = 2\n",
    "\n",
    "# invert colors?\n",
    "invert_colors = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fcecce21-a03b-42c5-9ddc-420d8b3b1507",
   "metadata": {},
   "outputs": [],
   "source": [
    "# want to use different than the defaults? if not, set to None\n",
    "ocr_results_dir = '/Users/jillnaiman/Dropbox/wwt_image_extraction/FigureLocalization/BenchMarks/OCR_processing_pmcnoncom/'\n",
    "images_jpeg_dir = '/Users/jillnaiman/Dropbox/wwt_image_extraction/FigureLocalization/BenchMarks/Pages_pmcnoncom/RandomSingleFromPDFIndexed/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fe46488c-61d8-4614-9648-2e89ab957f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels for annotations:\n",
    "labels = ['figure', 'figure caption', 'table', 'math formula', 'sub fig caption', 'colorbar', 'NotSure', 'no label'] # no label is for nothing on the page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e777e9c5-2725-485e-a9b6-be276e4dd945",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "import shutil\n",
    "from PIL import Image\n",
    "\n",
    "from annotation_utils import get_all_ocr_files, collect_ocr_process_results\n",
    "from mega_yolo_utils import build_predict\n",
    "from feature_generation_utils import generate_single_feature\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d90a9bf5-b4e2-456c-9a6f-844037aa1372",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a25526ed-db16-4084-a7b8-cb5fdb10c140",
   "metadata": {},
   "outputs": [],
   "source": [
    "if images_jpeg_dir is None: images_jpeg_dir = config.images_jpeg_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ecf055e9-09d3-494d-89d2-a384ddfac96c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save labels\n",
    "# save feature list\n",
    "with open(save_makesense +'saved_labels.pickle', 'wb') as ff:\n",
    "    pickle.dump([labels], ff)   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "10e13cb9-bc63-480a-adff-2298dda967bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all dones already\n",
    "\n",
    "# generate donesfile from list\n",
    "lfiles = glob(check_makesense+'labels*csv')\n",
    "filenames = []; labelss = []; xmins = []; ymins=[]; xmaxs=[]; ymaxs=[]; widths=[]\n",
    "fnameSave = []; heights = []\n",
    "for il,l in enumerate(lfiles):\n",
    "    d = pd.read_csv(l, names = ['label','xmin','ymin','xmax','ymax', 'fname','x','y'])\n",
    "    #d = d.drop_duplicates(subset='fname')\n",
    "    for idd, dd in enumerate(d['fname'].values):\n",
    "        fn = dd[:dd.rfind('.')+1]\n",
    "        if fn[-1] == '.': fn = fn[:-1]\n",
    "        filenames.append(fn)\n",
    "        fnameSave.append(dd)\n",
    "    if il == 0:\n",
    "        dfTmp = d.copy()\n",
    "    else:\n",
    "        dfTmp = dfTmp.append(d)\n",
    "# get unique\n",
    "fnameSave,uind = np.unique(fnameSave,return_index=True)\n",
    "# loop and grab\n",
    "for idd, dd in enumerate(fnameSave):\n",
    "    mask = dfTmp['fname'] == dd\n",
    "    labelss.append(dfTmp.loc[mask]['label'].values); xmins.append(dfTmp.loc[mask]['xmin'].values)\n",
    "    ymins.append(dfTmp.loc[mask]['ymin'].values); \n",
    "    # widths and heights!!\n",
    "    xmaxs.append(dfTmp.loc[mask]['xmax'].values+dfTmp.loc[mask]['xmin'].values); \n",
    "    ymaxs.append(dfTmp.loc[mask]['ymax'].values+dfTmp.loc[mask]['ymin'].values)\n",
    "    widths.append(dfTmp.loc[mask]['x'].values[0]); heights.append(dfTmp.loc[mask]['y'].values[0])\n",
    "dones = pd.DataFrame({'filename':np.array(filenames)[uind], 'labels':labelss, 'xmin':xmins, \n",
    "                      'ymin':ymins, 'xmax':xmaxs,'ymax':ymaxs, 'width':widths, 'height':heights})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6cab2b2c-5364-43db-b834-52fbfa82380b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>labels</th>\n",
       "      <th>xmin</th>\n",
       "      <th>ymin</th>\n",
       "      <th>xmax</th>\n",
       "      <th>ymax</th>\n",
       "      <th>width</th>\n",
       "      <th>height</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [filename, labels, xmin, ymin, xmax, ymax, width, height]\n",
       "Index: []"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2b82b4a3-887c-420b-af9e-fb8e1a3a5596",
   "metadata": {},
   "outputs": [],
   "source": [
    "donefiles = dones['filename'].values\n",
    "donefiles\n",
    "# debug\n",
    "donefiles = ['ofx163.1581.PMC5630905_p1.jpeg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ba6ab230-008d-44f2-a49c-e76f580ffd87",
   "metadata": {},
   "outputs": [],
   "source": [
    "ocrFiles = get_all_ocr_files(ocr_results_dir=ocr_results_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "988cafef-07cd-49be-9efe-0d8b4663bfc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "retreiving OCR data, this can take a moment...\n",
      "--- OCR retrieval: on 0 of 3\n"
     ]
    }
   ],
   "source": [
    "print('retreiving OCR data, this can take a moment...')\n",
    "ws1, paragraphs1, squares1, html1, rotations1,colorbars1 = collect_ocr_process_results(ocrFiles)\n",
    "# create dataframe\n",
    "ws, paragraphs, squares, html, rotations,colorbars = [],[],[],[],[],[]\n",
    "for w,p,s,h,r,c in zip(ws1,paragraphs1, squares1, html1, rotations1,colorbars1):\n",
    "    if w not in donefiles:\n",
    "        ws.append(w); paragraphs.append(p);squares.append(s)\n",
    "        html.append(h); rotations.append(r); colorbars.append(c)\n",
    "df = pd.DataFrame({'ws':ws, 'paragraphs':paragraphs, 'squares':squares, \n",
    "                   'hocr':html, 'rotation':rotations, 'colorbars':colorbars})#, 'pdfwords':pdfwords})\n",
    "df = df.drop_duplicates(subset='ws')\n",
    "df = df.set_index('ws')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "956ae0ad-1eb1-490e-a232-d296e77ee93a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if nRandom < len(ws):\n",
    "    # grab randomly\n",
    "    ind = np.random.choice(range(len(ws)),nRandom,replace=False)\n",
    "    #ws = np.array(ws)[ind]\n",
    "else:\n",
    "    #ws = np.array(ws)\n",
    "    ind = np.arange(0,len(ws))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "091fa776-4d9e-4d6e-8d51-e473d5098fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfout = df.iloc[ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8dc291fb-fb72-4698-8161-1a5c873d579c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paragraphs</th>\n",
       "      <th>squares</th>\n",
       "      <th>hocr</th>\n",
       "      <th>rotation</th>\n",
       "      <th>colorbars</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ws</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10-1055-s-0039-1677738.PMC6382497_p0.jpeg</th>\n",
       "      <td>[(268, 77, 106, 23), (272, 112, 100, 30), (272...</td>\n",
       "      <td>[]</td>\n",
       "      <td>&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;\\n&lt;!DOCT...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[[[315, 2022], [318, 2069], [618, 2066], [615,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SAJC-7-249.PMC6190391_p0.jpeg</th>\n",
       "      <td>[(156, 182, 2236, 60), (156, 182, 2236, 60), (...</td>\n",
       "      <td>[[[153, 2874], [494, 2874], [494, 3204], [153,...</td>\n",
       "      <td>&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;\\n&lt;!DOCT...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[[[161, 2879], [161, 2916], [486, 2916], [486,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                  paragraphs  \\\n",
       "ws                                                                                             \n",
       "10-1055-s-0039-1677738.PMC6382497_p0.jpeg  [(268, 77, 106, 23), (272, 112, 100, 30), (272...   \n",
       "SAJC-7-249.PMC6190391_p0.jpeg              [(156, 182, 2236, 60), (156, 182, 2236, 60), (...   \n",
       "\n",
       "                                                                                     squares  \\\n",
       "ws                                                                                             \n",
       "10-1055-s-0039-1677738.PMC6382497_p0.jpeg                                                 []   \n",
       "SAJC-7-249.PMC6190391_p0.jpeg              [[[153, 2874], [494, 2874], [494, 3204], [153,...   \n",
       "\n",
       "                                                                                        hocr  \\\n",
       "ws                                                                                             \n",
       "10-1055-s-0039-1677738.PMC6382497_p0.jpeg  <?xml version=\"1.0\" encoding=\"UTF-8\"?>\\n<!DOCT...   \n",
       "SAJC-7-249.PMC6190391_p0.jpeg              <?xml version=\"1.0\" encoding=\"UTF-8\"?>\\n<!DOCT...   \n",
       "\n",
       "                                                                                    rotation  \\\n",
       "ws                                                                                             \n",
       "10-1055-s-0039-1677738.PMC6382497_p0.jpeg  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "SAJC-7-249.PMC6190391_p0.jpeg              [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "\n",
       "                                                                                   colorbars  \n",
       "ws                                                                                            \n",
       "10-1055-s-0039-1677738.PMC6382497_p0.jpeg  [[[315, 2022], [318, 2069], [618, 2066], [615,...  \n",
       "SAJC-7-249.PMC6190391_p0.jpeg              [[[161, 2879], [161, 2916], [486, 2916], [486,...  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfout.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "588dd2af-5eb5-4f14-8f0f-5e9e228ebfc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ws = dfout.index\n",
    "# ws\n",
    "#dfout.index.values.astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c001d727-e5cb-49f3-9bde-2cc8f4e67edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save label file for annotations\n",
    "storeTmps = config.tmp_storage_dir + 'MakeSense/'\n",
    "if not os.path.exists(storeTmps):\n",
    "    os.mkdir(storeTmps)\n",
    "# remove, remake\n",
    "shutil.rmtree(storeTmps)\n",
    "os.mkdir(storeTmps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74538e7a-483c-494d-92b3-4e3b215b274a",
   "metadata": {},
   "source": [
    "Copy over random images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8a3d8eae-071d-4081-83d9-66f53c0c2eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# start on empty\n",
    "if not os.path.exists(storeTmps+'images/'):\n",
    "    os.makedirs(storeTmps+'images/')\n",
    "# delete and remake\n",
    "shutil.rmtree(storeTmps+'images/')\n",
    "os.makedirs(storeTmps+'images/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "049ef9f7-48b9-40f4-8eda-3e1424940e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# move files\n",
    "imgSizes = []; imgSizesOrig = []\n",
    "for iw,w in enumerate(dfout.index.values.astype('str')):\n",
    "    #print(w)\n",
    "    w = images_jpeg_dir + w\n",
    "    if not invert_colors:\n",
    "        shutil.copyfile(w, storeTmps+'images/'+ w.split('/')[-1])\n",
    "        imgSizes.append(Image.open(w).convert('RGB').size)\n",
    "    else: # invert B/W\n",
    "        print('not implemented!!')\n",
    "        import sys; sys.exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f13b3de3-2f13-4058-8b0d-13a016021b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write label file\n",
    "with open(storeTmps + 'labels.txt','w') as f:\n",
    "    for l in labels:\n",
    "        f.write(l.replace(' ', '_') + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33525449-e5fa-4d08-a1f0-8334afe3e952",
   "metadata": {},
   "source": [
    "For annotations, try to predict with our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0d5104d4-a249-47fb-b916-4c4612f32523",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-16 21:46:42.149166: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/jillnaiman/anaconda3/envs/Paper1/lib/python3.7/site-packages/tensorflow/python/util/deprecation.py:605: calling map_fn_v2 (from tensorflow.python.ops.map_fn) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use fn_output_signature instead\n"
     ]
    }
   ],
   "source": [
    "# get model to run on pages\n",
    "# read in anchors\n",
    "# saveFileAnchors = weightsFileDir + 'anchors.pickle'\n",
    "# with open(saveFileAnchors, 'rb') as f:\n",
    "#     anchors = pickle.load(f) \n",
    "#     anchors = anchors.astype('float32')\n",
    "\n",
    "    \n",
    "feature_dir = config.save_binary_dir + binary_dirs\n",
    "# how many features\n",
    "with open(feature_dir +'feature_list.pickle', 'rb') as ff:\n",
    "    feature_list = pickle.load(ff)[0]\n",
    "n_features = len(feature_list)\n",
    "\n",
    "# labels file for originally trained\n",
    "LABELS=pd.read_csv(feature_dir + 'LABELS.csv',names=['labels'])['labels'].values.astype('str')\n",
    "\n",
    "# build the model\n",
    "weightsFileDownload = weightsFileDir + weightsFile\n",
    "anchorsFile = weightsFileDir + 'anchors.pickle'  # should this be changed....\n",
    "\n",
    "model = build_predict(weightsFileDownload, anchorsFile, \n",
    "                    feature_dir,LABELS,version=config.version, \n",
    "                      debug=False,n_features=n_features)\n",
    "model.load_weights(weightsFileDownload)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9a1807bf-3f24-4426-8982-2329a0ea3457",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-16 21:50:38.591058: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(storeTmps+'annotations/'):\n",
    "    os.makedirs(storeTmps+'annotations/')\n",
    "# delete and remake\n",
    "shutil.rmtree(storeTmps+'annotations/')\n",
    "os.makedirs(storeTmps+'annotations/')\n",
    "\n",
    "# save tmp binaries for this\n",
    "if not os.path.exists(storeTmps+'binaries/'):\n",
    "    os.makedirs(storeTmps+'binaries/')\n",
    "# delete and remake\n",
    "shutil.rmtree(storeTmps+'binaries/')\n",
    "os.makedirs(storeTmps+'binaries/')\n",
    "\n",
    "# just for counting\n",
    "maxboxes = 50 # I think this is actually a place holder...\n",
    "\n",
    "for i in range(len(dfout)):\n",
    "    dfsingle = df.iloc[i]\n",
    "    # if we've made it this far, let's generate features\n",
    "    img_name, font = generate_single_feature(dfsingle, LABELS, maxboxes, \n",
    "                                           feature_list = feature_list, \n",
    "                                           binary_dir = storeTmps+'binaries/',\n",
    "                                                images_jpeg_dir = images_jpeg_dir,\n",
    "                                                astype='npz', \n",
    "                                                 npzcompressed=True) \n",
    "    # predict squares in 2 ways\n",
    "    # 1. MEGA YOLO\n",
    "    image_np = np.load(img_name)['arr_0']\n",
    "    image_np = image_np.astype(np.float32) / 255.0 \n",
    "\n",
    "    boxes, scores, labels = model.predict(image_np[np.newaxis, ...])\n",
    "    boxes1, scores1, labels1 = np.squeeze(boxes, 0), np.squeeze(scores, 0), np.squeeze(labels, 0)\n",
    "    \n",
    "    # # post process the thing\n",
    "    # # get OCR results and parse them, open image for image processing\n",
    "    # image = \n",
    "    # backtorgb,image_np,rotatedImage,rotatedAngleOCR,bbox_hocr,\\\n",
    "    #   bboxes_words,bbsq,cbsq, rotation,bbox_par = get_ocr_results(imgs_name, dfMakeSense,df,\n",
    "    #                                                              image_np=image.numpy())\n",
    "\n",
    "    import sys; sys.exit()\n",
    "    \n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "383ac2be-9145-45d1-8272-b42836877baa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[-1., -1., -1., -1.],\n",
       "        [-1., -1., -1., -1.],\n",
       "        [-1., -1., -1., -1.],\n",
       "        [-1., -1., -1., -1.],\n",
       "        [-1., -1., -1., -1.],\n",
       "        [-1., -1., -1., -1.],\n",
       "        [-1., -1., -1., -1.],\n",
       "        [-1., -1., -1., -1.],\n",
       "        [-1., -1., -1., -1.],\n",
       "        [-1., -1., -1., -1.],\n",
       "        [-1., -1., -1., -1.],\n",
       "        [-1., -1., -1., -1.],\n",
       "        [-1., -1., -1., -1.],\n",
       "        [-1., -1., -1., -1.],\n",
       "        [-1., -1., -1., -1.],\n",
       "        [-1., -1., -1., -1.],\n",
       "        [-1., -1., -1., -1.],\n",
       "        [-1., -1., -1., -1.],\n",
       "        [-1., -1., -1., -1.],\n",
       "        [-1., -1., -1., -1.],\n",
       "        [-1., -1., -1., -1.],\n",
       "        [-1., -1., -1., -1.],\n",
       "        [-1., -1., -1., -1.],\n",
       "        [-1., -1., -1., -1.],\n",
       "        [-1., -1., -1., -1.],\n",
       "        [-1., -1., -1., -1.],\n",
       "        [-1., -1., -1., -1.],\n",
       "        [-1., -1., -1., -1.],\n",
       "        [-1., -1., -1., -1.],\n",
       "        [-1., -1., -1., -1.],\n",
       "        [-1., -1., -1., -1.],\n",
       "        [-1., -1., -1., -1.],\n",
       "        [-1., -1., -1., -1.],\n",
       "        [-1., -1., -1., -1.],\n",
       "        [-1., -1., -1., -1.],\n",
       "        [-1., -1., -1., -1.],\n",
       "        [-1., -1., -1., -1.],\n",
       "        [-1., -1., -1., -1.],\n",
       "        [-1., -1., -1., -1.],\n",
       "        [-1., -1., -1., -1.],\n",
       "        [-1., -1., -1., -1.],\n",
       "        [-1., -1., -1., -1.],\n",
       "        [-1., -1., -1., -1.],\n",
       "        [-1., -1., -1., -1.],\n",
       "        [-1., -1., -1., -1.],\n",
       "        [-1., -1., -1., -1.],\n",
       "        [-1., -1., -1., -1.],\n",
       "        [-1., -1., -1., -1.],\n",
       "        [-1., -1., -1., -1.],\n",
       "        [-1., -1., -1., -1.],\n",
       "        [-1., -1., -1., -1.],\n",
       "        [-1., -1., -1., -1.],\n",
       "        [-1., -1., -1., -1.],\n",
       "        [-1., -1., -1., -1.],\n",
       "        [-1., -1., -1., -1.],\n",
       "        [-1., -1., -1., -1.],\n",
       "        [-1., -1., -1., -1.],\n",
       "        [-1., -1., -1., -1.],\n",
       "        [-1., -1., -1., -1.],\n",
       "        [-1., -1., -1., -1.],\n",
       "        [-1., -1., -1., -1.],\n",
       "        [-1., -1., -1., -1.],\n",
       "        [-1., -1., -1., -1.],\n",
       "        [-1., -1., -1., -1.],\n",
       "        [-1., -1., -1., -1.],\n",
       "        [-1., -1., -1., -1.],\n",
       "        [-1., -1., -1., -1.],\n",
       "        [-1., -1., -1., -1.],\n",
       "        [-1., -1., -1., -1.],\n",
       "        [-1., -1., -1., -1.],\n",
       "        [-1., -1., -1., -1.],\n",
       "        [-1., -1., -1., -1.],\n",
       "        [-1., -1., -1., -1.],\n",
       "        [-1., -1., -1., -1.],\n",
       "        [-1., -1., -1., -1.],\n",
       "        [-1., -1., -1., -1.],\n",
       "        [-1., -1., -1., -1.],\n",
       "        [-1., -1., -1., -1.],\n",
       "        [-1., -1., -1., -1.],\n",
       "        [-1., -1., -1., -1.],\n",
       "        [-1., -1., -1., -1.],\n",
       "        [-1., -1., -1., -1.],\n",
       "        [-1., -1., -1., -1.],\n",
       "        [-1., -1., -1., -1.],\n",
       "        [-1., -1., -1., -1.],\n",
       "        [-1., -1., -1., -1.],\n",
       "        [-1., -1., -1., -1.],\n",
       "        [-1., -1., -1., -1.],\n",
       "        [-1., -1., -1., -1.],\n",
       "        [-1., -1., -1., -1.],\n",
       "        [-1., -1., -1., -1.],\n",
       "        [-1., -1., -1., -1.],\n",
       "        [-1., -1., -1., -1.],\n",
       "        [-1., -1., -1., -1.],\n",
       "        [-1., -1., -1., -1.],\n",
       "        [-1., -1., -1., -1.],\n",
       "        [-1., -1., -1., -1.],\n",
       "        [-1., -1., -1., -1.],\n",
       "        [-1., -1., -1., -1.],\n",
       "        [-1., -1., -1., -1.],\n",
       "        [-1., -1., -1., -1.],\n",
       "        [-1., -1., -1., -1.],\n",
       "        [-1., -1., -1., -1.],\n",
       "        [-1., -1., -1., -1.],\n",
       "        [-1., -1., -1., -1.],\n",
       "        [-1., -1., -1., -1.],\n",
       "        [-1., -1., -1., -1.],\n",
       "        [-1., -1., -1., -1.],\n",
       "        [-1., -1., -1., -1.],\n",
       "        [-1., -1., -1., -1.],\n",
       "        [-1., -1., -1., -1.],\n",
       "        [-1., -1., -1., -1.],\n",
       "        [-1., -1., -1., -1.],\n",
       "        [-1., -1., -1., -1.],\n",
       "        [-1., -1., -1., -1.],\n",
       "        [-1., -1., -1., -1.],\n",
       "        [-1., -1., -1., -1.],\n",
       "        [-1., -1., -1., -1.],\n",
       "        [-1., -1., -1., -1.],\n",
       "        [-1., -1., -1., -1.],\n",
       "        [-1., -1., -1., -1.],\n",
       "        [-1., -1., -1., -1.],\n",
       "        [-1., -1., -1., -1.],\n",
       "        [-1., -1., -1., -1.],\n",
       "        [-1., -1., -1., -1.],\n",
       "        [-1., -1., -1., -1.],\n",
       "        [-1., -1., -1., -1.],\n",
       "        [-1., -1., -1., -1.],\n",
       "        [-1., -1., -1., -1.],\n",
       "        [-1., -1., -1., -1.],\n",
       "        [-1., -1., -1., -1.],\n",
       "        [-1., -1., -1., -1.],\n",
       "        [-1., -1., -1., -1.],\n",
       "        [-1., -1., -1., -1.],\n",
       "        [-1., -1., -1., -1.],\n",
       "        [-1., -1., -1., -1.],\n",
       "        [-1., -1., -1., -1.],\n",
       "        [-1., -1., -1., -1.],\n",
       "        [-1., -1., -1., -1.],\n",
       "        [-1., -1., -1., -1.],\n",
       "        [-1., -1., -1., -1.],\n",
       "        [-1., -1., -1., -1.],\n",
       "        [-1., -1., -1., -1.],\n",
       "        [-1., -1., -1., -1.],\n",
       "        [-1., -1., -1., -1.],\n",
       "        [-1., -1., -1., -1.],\n",
       "        [-1., -1., -1., -1.],\n",
       "        [-1., -1., -1., -1.],\n",
       "        [-1., -1., -1., -1.],\n",
       "        [-1., -1., -1., -1.]]], dtype=float32)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "253f9c42-0259-4719-8698-5b984a30b047",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e8dfe6-dd11-406e-a7f9-028dc2a89a67",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93556a1d-3101-4778-894f-1cc1b4c88708",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f57405-f798-4e9b-8c9a-e80e64890d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if (use_model_ann) and (not redo_makesense) and bad_ann_file is None:\n",
    "    # should already be in main binaries folder\n",
    "    for w,imSize in zip(ws,imgSizes):\n",
    "        fn = w.split('/')[-1]\n",
    "        fn = fn[:fn.rfind('.')+1]\n",
    "        if fn[-1] == '.': fn = fn[:-1]\n",
    "        fn = binariesDir + '../' + fn + '.npz'\n",
    "\n",
    "        image_np = np.load(fn)['arr_0']\n",
    "        image_np = image_np.astype(np.float32) / 255.0\n",
    "        \n",
    "        boxes, scores, labelsout = model.predict(image_np[np.newaxis, ...])\n",
    "        boxes, scores, labelsout = np.squeeze(boxes, 0), np.squeeze(scores, 0), np.squeeze(labelsout, 0)\n",
    "        #print(labelsout)\n",
    "        # resize boxes\n",
    "        xfrac = imSize[1]*1.0/IMAGE_W; yfrac = imSize[0]*1.0/IMAGE_H\n",
    "        if os.path.exists(fn): \n",
    "            with open(storeTmps+'annotations/'+ fn.split('/')[-1].split('.npz')[0] + '.txt','w') as fsave:\n",
    "                for bb,ll in zip(boxes,labelsout):\n",
    "                    xmin = bb[0]*xfrac; ymin = bb[1]*yfrac; xmax = bb[2]*xfrac; ymax = bb[3]*yfrac\n",
    "                    x = xmin/imSize[1]; y = ymin/imSize[0]; \n",
    "                    w = (xmax-xmin)/imSize[1]; h = (ymax-ymin)/imSize[0]\n",
    "                    # I think we want centers?\n",
    "                    x = x+w*0.5; y = y+0.5*h\n",
    "                    #lab = labels_annotation[int(bb[4])-1]\n",
    "                    if ll > -1:\n",
    "                        #lab = LABELS[int(ll)-1]\n",
    "                        lab = LABELS[int(ll)]\n",
    "                        #print(ll,lab)\n",
    "                        if lab == 'multi-figure': lab = 'figure' # rename all to figure for now\n",
    "                        try: \n",
    "                            #l = labels.index(lab)\n",
    "                            l = labels.index(lab)\n",
    "                            #print(l)\n",
    "                        except:\n",
    "                            l = -1\n",
    "                        if l > -1: # found in these labels\n",
    "                            fsave.write(str(l) + ' ' + str(max([x,0])) + ' ' + str(max([y,0])) + ' ' + str(min([w,1])) + ' ' + str(min([h,1])) + '\\n')\n",
    "                        #import sys; sys.exit()\n",
    "        else:\n",
    "            print('something has gone wrong')\n",
    "            import sys; sys.exit()\n",
    "\n",
    "        #import sys; sys.exit()\n",
    "        \n",
    "    #write label file with ALL labels\n",
    "    with open(storeTmps + 'annotations/' + labelFile,'w') as f:\n",
    "        for l in labels:\n",
    "            f.write(l.replace(' ', '_') + '\\n')\n",
    "\n",
    "\n",
    "print('all done!  go classify!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8efa4385-7928-4c17-ba6b-c2b2da422abc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92142ca3-68ea-48e3-852b-ba328160e37d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c417575-8e20-4fb3-b6c7-351d6782de4b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adea3e9b-67ef-4fcc-939f-072a2d2536f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9abcd02-15e8-4ae0-aa23-aa100441ab42",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb68429-1cc3-41e8-974c-00c75e0a3f1b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87162055-4a5a-4839-9625-2fffeb554d51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1af0aa1-0157-457d-941c-fbe48b34a18b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ca05c866-185f-4d6b-a20e-c65a82a82dd6",
   "metadata": {},
   "source": [
    "# ------------ IGNORE BELOW -----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7e0ef784-3ff8-48e3-84a0-5a288240e747",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add another\n",
    "#ocrFiles.append('/Users/jillnaiman/Dropbox/wwt_image_extraction/FigureLocalization/OCR_processing/full_ocr_newPDFs_take2.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9f501730-6ae3-4b30-b8c2-49c4c53d1c74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/Users/jillnaiman/Dropbox/wwt_image_extraction/FigureLocalization/BenchMarks/OCR_processing_pmcnoncom/full_ocr_newPDFs_TIFF_take1.pickle']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ocrFiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3ef10c59-ccae-42d5-8872-3191cbc9ca2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "wsout = []\n",
    "for cp in ocrFiles:\n",
    "    with open(cp, 'rb') as f:\n",
    "        wsout1, full_run_squares, full_run_ocr, full_run_rotations, \\\n",
    "             full_run_pdf, full_run_hocr, color_bars,\\\n",
    "              centers_in, centers_out = pickle.load(f) \n",
    "    wsout.extend(wsout1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9b6268d6-d177-49a5-9ef7-e9d04ae6c1d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/Users/jillnaiman/Dropbox/wwt_image_extraction/FigureLocalization/BenchMarks/Pages_pmcnoncom/RandomSingleFromPDFIndexed/ofx163.1581.PMC5630905_p1.jpeg',\n",
       " '/Users/jillnaiman/Dropbox/wwt_image_extraction/FigureLocalization/BenchMarks/Pages_pmcnoncom/RandomSingleFromPDFIndexed/10-1055-s-0039-1677738.PMC6382497_p0.jpeg',\n",
       " '/Users/jillnaiman/Dropbox/wwt_image_extraction/FigureLocalization/BenchMarks/Pages_pmcnoncom/RandomSingleFromPDFIndexed/SAJC-7-249.PMC6190391_p0.jpeg']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# take out dones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f14ad5-dd73-4348-8723-7718e2a385fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50158c4e-93df-48c8-af5e-eeef2bd95bf8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd3a3ed-163a-4270-bac8-4f84f2713545",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe8c133-7589-42dc-a8e5-8ee59be4db22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb198189-50a3-4cff-8c33-4d4281f4d950",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4d3c8235",
   "metadata": {},
   "outputs": [],
   "source": [
    "# do we want to use a model to generate possible annotations for us?\n",
    "use_model_ann = True # redo, redo_makesense = False must be toggled!\n",
    "# finally, do we want to use a combo of OTHER pages AND scans from the downloaded PDFs?\n",
    "\n",
    "# reannotate some bad annoations? -- assume you wanna do this all in one go...\n",
    "bad_ann_file = None # if none, this is skipped\n",
    "#bad_ann_file = '/Users/jillnaiman/Dropbox/wwt_image_extraction/FigureLocalization/Annotations/MakeSenseAnnotations_old/more_bad_ann_redux.csv'\n",
    "\n",
    "# look for any particular tag?\n",
    "look_for = None #'table' # set to None for nothing\n",
    "\n",
    "donesDir = '/Users/jillnaiman/Dropbox/wwt_image_extraction/FigureLocalization/Annotations/MakeSenseAnnotations/' # this is redudent I think...\n",
    "\n",
    "# want to re-do a whole batch of annotations from a makesense annotations directory?\n",
    "redo_makesense = False\n",
    "redo_makesense_folder = '/Users/jillnaiman/Dropbox/wwt_image_extraction/FigureLocalization/Annotations/MakeSenseAnnotations_orig/'\n",
    "# what about dones from this?\n",
    "# new place\n",
    "redo_makesense_folder_new = '/Users/jillnaiman/Dropbox/wwt_image_extraction/FigureLocalization/Annotations/MakeSenseAnnotations/'\n",
    "\n",
    "###########################################################################\n",
    "\n",
    "\n",
    "# where are scans stored?\n",
    "images_pulled_dir_orig = '/Users/jillnaiman/Dropbox/wwt_image_extraction/FigureLocalization/Pages/RandomSingleFromPDFIndexed/' ## what about Dropbox though????\n",
    "images_pulled_dir = '/Users/jillnaiman/Dropbox/wwt_image_extraction/FigureLocalization/Pages/RandomSingleFromPDFIndexed/' # unskewed, first run\n",
    "\n",
    "# if we want to copy annotations\n",
    "annotation_dir = '/Users/jillnaiman/tmpMegaYolo/binaries/yolo_512x512_cap_math_ann/'\n",
    "# what are the YOLO labels?\n",
    "labels_annotation = ['figure', 'figure caption', 'table', 'table caption', 'math formula', 'colorbar', 'sub fig caption', 'multi-figure']\n",
    "# what format are the annotations stored in?\n",
    "IMAGE_H, IMAGE_W = 512, 512\n",
    "\n",
    "# NOTE: with labels -- only add to END of list for new ones!!!\n",
    "# where to store the temp files? -- note, this erases what was there!\n",
    "storeTmps = '/Users/jillnaiman/Downloads/tmpMakeSense/'\n",
    "# where to store labels?\n",
    "labelFile = 'labels.txt' # in the storeTmps file\n",
    "\n",
    "# what labels for annotations?\n",
    "labels = ['figure', 'figure caption', 'table', 'math formula', 'sub fig caption', 'colorbar', 'NotSure', 'no label'] # no label is for nothing on the page\n",
    "# used in the already trained model model?\n",
    "LABELS = ['figure', 'figure caption', 'table']\n",
    "\n",
    "\n",
    "# how many to do at a time?\n",
    "nRandom = 100\n",
    "\n",
    "# invert colors for easier labeling?  Generally false -- hard to see some box boundaries\n",
    "invert_colors = False\n",
    "\n",
    "# copy over annotations?\n",
    "copy_annotations = True\n",
    "\n",
    "# other important directories\n",
    "weightsFile = '20210914/training_1_model_orig_training_setsl0.0059608044.h5' # model weights: figure/table, fig/table captions\n",
    "anchorsFile = '20210914/anchors.pickle'\n",
    "binariesDir = '/Users/jillnaiman/tmpMegaYolo/binaries/binaries/'\n",
    "origAnnDir = '/Users/jillnaiman/tmpMegaYolo/binaries/yolo_512x512_cap_ann/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "08f01bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# update dirs\n",
    "weightsFile = '/Users/jillnaiman/Dropbox/wwt_image_extraction/FigureLocalization/mega_yolo/saved_weights/'+weightsFile\n",
    "anchorsFile = '/Users/jillnaiman/Dropbox/wwt_image_extraction/FigureLocalization/mega_yolo/saved_weights/'+anchorsFile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d15d70d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OCR results\n",
    "figCapMain = '/Users/jillnaiman/Dropbox/wwt_image_extraction/FigureLocalization/OCR_processing/'\n",
    "ocrFilesList = [figCapMain+'full_ocr_results_and_squares_REINDEXED_noOCRskew.pickle',\n",
    "            figCapMain+'full_ocr_newPDFs_take*.pickle'\n",
    "           ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b9f7b72b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# libraries\n",
    "import os\n",
    "import shutil\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "from PIL import Image, ImageOps\n",
    "import pickle\n",
    "#import tensorflow as tf\n",
    "import cv2 as cv\n",
    "\n",
    "from sys import path\n",
    "path.append('../../')\n",
    "from utils import stored_classifications, take_out_dones\n",
    "\n",
    "import xml.etree.ElementTree as ET\n",
    "from lxml import etree\n",
    "\n",
    "# more mega-yolo specific libraries\n",
    "path.append('./')\n",
    "from mega_yolo_utils import parse_annotation, build_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "16ace3d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write label file\n",
    "with open(storeTmps + labelFile,'w') as f:\n",
    "    for l in labels:\n",
    "        f.write(l.replace(' ', '_') + '\\n')\n",
    "        \n",
    "if bad_ann_file is not None: redo_makesense = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bab78e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "#update all OCRfiles\n",
    "ocrFiles = []\n",
    "for f in ocrFilesList:\n",
    "    if '*' not in f:\n",
    "        ocrFiles.append(f)\n",
    "    else:\n",
    "        fs = glob(f)\n",
    "        for ff in fs:\n",
    "            ocrFiles.append(ff)\n",
    "#ocrFiles = ocrFiles2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aab6ab2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ocrFiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "99afe32f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/anaconda3/envs/wwtProject3.7/lib/python3.7/site-packages/tensorflow/python/util/deprecation.py:605: calling map_fn_v2 (from tensorflow.python.ops.map_fn) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use fn_output_signature instead\n"
     ]
    }
   ],
   "source": [
    "# build the model if we wanna use it!\n",
    "if use_model_ann and bad_ann_file is None and not redo_makesense:\n",
    "    model = build_predict(weightsFile, anchorsFile, \n",
    "                          binariesDir,LABELS,version='l', debug=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d68f26b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf.keras.utils.plot_model(model, \"yolo_v5.png\", show_shapes=True, \n",
    "#                                  show_layer_names=True, expand_nested=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "14f257e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if len(pdfLinks.split('classified_')) > 0:\n",
    "#     ffiles = glob(pdfLinks + '*')\n",
    "#     ffiles.sort()\n",
    "#     pdfLinks = ffiles[-1]\n",
    "# # get PDF links from file\n",
    "# with open(pdfLinks, 'rb') as f:\n",
    "#     wsLinks1, dlinks,pages = pickle.load(f)  \n",
    "# # format for later\n",
    "# wsLinks = []\n",
    "# for w in wsLinks1:\n",
    "#     wsLinks.append(w[:w.rfind('.')].split('/')[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "16b6eeba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#len(ws), len(paragraphs), len(squares), len(html)\n",
    "#ws[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2a468009",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop and grab hocr\n",
    "ws = []; paragraphs = []; squares = []; html = []; #pdfwords = []\n",
    "for cp in ocrFiles:\n",
    "    # do a little test save here - locations of squares and figure caption boxes\n",
    "#     try: # mixed numbers of things\n",
    "#         with open(cp, 'rb') as f:\n",
    "#             wsout, full_run_squares, full_run_ocr, full_run_rotations, \\\n",
    "#                          full_run_lineNums, full_run_confidences, full_run_paragraphs, \\\n",
    "#                          full_run_links, full_run_gifLinkStorage, full_run_PDFlinkStorage, \\\n",
    "#                          full_run_pageNumStorage, full_run_downloadLinkStorage,\\\n",
    "#                          full_run_htmlText = pickle.load(f)\n",
    "#    except:\n",
    "    with open(cp, 'rb') as f:\n",
    "        #if os.path.getsize(target) > 0\n",
    "        wsout, full_run_squares, full_run_ocr, full_run_rotations, \\\n",
    "                     full_run_lineNums, full_run_confidences, full_run_paragraphs, \\\n",
    "                     full_run_links, full_run_gifLinkStorage, full_run_PDFlinkStorage, \\\n",
    "                     full_run_pageNumStorage, full_run_downloadLinkStorage,\\\n",
    "                     full_run_htmlText,_,_,_,_ = pickle.load(f)\n",
    "\n",
    "        # splits\n",
    "        for i,w in enumerate(wsout):\n",
    "            wsout[i] = w.split('/')[-1]\n",
    "\n",
    "        ws.extend(wsout); paragraphs.extend(full_run_paragraphs); squares.extend(full_run_squares);\n",
    "        html.extend(full_run_htmlText); #pdfwords.extend(full_run_pdfwords)\n",
    "    #if 'ff74bba9-22fc-4b10-81b1-4f0d706934a6_p15.jpeg' in wsout: print(cp)\n",
    "\n",
    "df = pd.DataFrame({'ws':ws, 'paragraphs':paragraphs, 'squares':squares, \n",
    "                   'hocr':html})#, 'pdfwords':pdfwords})\n",
    "df = df.drop_duplicates(subset='ws')\n",
    "df = df.set_index('ws')\n",
    "dfsave = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d040eaab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paragraphs</th>\n",
       "      <th>squares</th>\n",
       "      <th>hocr</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ws</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1993AJ____105__250G_p8.jpeg</th>\n",
       "      <td>[(88, 391, 21, 116), (88, 132, 21, 91), (253, ...</td>\n",
       "      <td>[[[192, 380], [2207, 380], [2207, 2917], [192,...</td>\n",
       "      <td>&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;\\n&lt;!DOCT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1974ApJ___191__111S_p8.jpeg</th>\n",
       "      <td>[(88, 133, 21, 74), (88, 254, 22, 53), (2008, ...</td>\n",
       "      <td>[[[485, 280], [2082, 280], [2082, 2596], [485,...</td>\n",
       "      <td>&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;\\n&lt;!DOCT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1986AJ_____91__290B_p3.jpeg</th>\n",
       "      <td>[(256, 84, 222, 32), (256, 84, 222, 32), (88, ...</td>\n",
       "      <td>[[[363, 520], [2255, 520], [2255, 2459], [363,...</td>\n",
       "      <td>&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;\\n&lt;!DOCT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1986ApJ___301__346H_p5.jpeg</th>\n",
       "      <td>[(88, 132, 21, 75), (266, 221, 195, 34), (266,...</td>\n",
       "      <td>[[[731, 330], [2383, 330], [2383, 1920], [731,...</td>\n",
       "      <td>&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;\\n&lt;!DOCT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1968ApJ___154L__99K_p2.jpeg</th>\n",
       "      <td>[(88, 132, 21, 54), (1958, 216, 227, 35), (195...</td>\n",
       "      <td>[[[315, 304], [2198, 304], [2198, 2705], [315,...</td>\n",
       "      <td>&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;\\n&lt;!DOCT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1991ApJ___367L__59R_p2.jpeg</th>\n",
       "      <td>[(88, 131, 21, 56), (228, 250, 189, 33), (228,...</td>\n",
       "      <td>[]</td>\n",
       "      <td>&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;\\n&lt;!DOCT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1993ApJ___417__502H_p9.jpeg</th>\n",
       "      <td>[(88, 116, 21, 75), (88, 238, 21, 53), (146, 2...</td>\n",
       "      <td>[]</td>\n",
       "      <td>&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;\\n&lt;!DOCT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1938ApJ____88____1B_p0.jpeg</th>\n",
       "      <td>[(88, 132, 21, 35), (88, 371, 27, 136), (414, ...</td>\n",
       "      <td>[[[408, 1578], [1844, 1578], [1844, 1990], [40...</td>\n",
       "      <td>&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;\\n&lt;!DOCT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1964ApJ___140_1467Z_p9.jpeg</th>\n",
       "      <td>[(727, 381, 29, 18), (912, 392, 11, 7), (976, ...</td>\n",
       "      <td>[[[571, 380], [1497, 380], [1497, 2534], [571,...</td>\n",
       "      <td>&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;\\n&lt;!DOCT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1981ApJS___47__229E_p9.jpeg</th>\n",
       "      <td>[(206, 247, 168, 31), (206, 247, 168, 31), (37...</td>\n",
       "      <td>[[[1266, 319], [2268, 319], [2268, 1108], [126...</td>\n",
       "      <td>&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;\\n&lt;!DOCT...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13259 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                    paragraphs  \\\n",
       "ws                                                                               \n",
       "1993AJ____105__250G_p8.jpeg  [(88, 391, 21, 116), (88, 132, 21, 91), (253, ...   \n",
       "1974ApJ___191__111S_p8.jpeg  [(88, 133, 21, 74), (88, 254, 22, 53), (2008, ...   \n",
       "1986AJ_____91__290B_p3.jpeg  [(256, 84, 222, 32), (256, 84, 222, 32), (88, ...   \n",
       "1986ApJ___301__346H_p5.jpeg  [(88, 132, 21, 75), (266, 221, 195, 34), (266,...   \n",
       "1968ApJ___154L__99K_p2.jpeg  [(88, 132, 21, 54), (1958, 216, 227, 35), (195...   \n",
       "...                                                                        ...   \n",
       "1991ApJ___367L__59R_p2.jpeg  [(88, 131, 21, 56), (228, 250, 189, 33), (228,...   \n",
       "1993ApJ___417__502H_p9.jpeg  [(88, 116, 21, 75), (88, 238, 21, 53), (146, 2...   \n",
       "1938ApJ____88____1B_p0.jpeg  [(88, 132, 21, 35), (88, 371, 27, 136), (414, ...   \n",
       "1964ApJ___140_1467Z_p9.jpeg  [(727, 381, 29, 18), (912, 392, 11, 7), (976, ...   \n",
       "1981ApJS___47__229E_p9.jpeg  [(206, 247, 168, 31), (206, 247, 168, 31), (37...   \n",
       "\n",
       "                                                                       squares  \\\n",
       "ws                                                                               \n",
       "1993AJ____105__250G_p8.jpeg  [[[192, 380], [2207, 380], [2207, 2917], [192,...   \n",
       "1974ApJ___191__111S_p8.jpeg  [[[485, 280], [2082, 280], [2082, 2596], [485,...   \n",
       "1986AJ_____91__290B_p3.jpeg  [[[363, 520], [2255, 520], [2255, 2459], [363,...   \n",
       "1986ApJ___301__346H_p5.jpeg  [[[731, 330], [2383, 330], [2383, 1920], [731,...   \n",
       "1968ApJ___154L__99K_p2.jpeg  [[[315, 304], [2198, 304], [2198, 2705], [315,...   \n",
       "...                                                                        ...   \n",
       "1991ApJ___367L__59R_p2.jpeg                                                 []   \n",
       "1993ApJ___417__502H_p9.jpeg                                                 []   \n",
       "1938ApJ____88____1B_p0.jpeg  [[[408, 1578], [1844, 1578], [1844, 1990], [40...   \n",
       "1964ApJ___140_1467Z_p9.jpeg  [[[571, 380], [1497, 380], [1497, 2534], [571,...   \n",
       "1981ApJS___47__229E_p9.jpeg  [[[1266, 319], [2268, 319], [2268, 1108], [126...   \n",
       "\n",
       "                                                                          hocr  \n",
       "ws                                                                              \n",
       "1993AJ____105__250G_p8.jpeg  <?xml version=\"1.0\" encoding=\"UTF-8\"?>\\n<!DOCT...  \n",
       "1974ApJ___191__111S_p8.jpeg  <?xml version=\"1.0\" encoding=\"UTF-8\"?>\\n<!DOCT...  \n",
       "1986AJ_____91__290B_p3.jpeg  <?xml version=\"1.0\" encoding=\"UTF-8\"?>\\n<!DOCT...  \n",
       "1986ApJ___301__346H_p5.jpeg  <?xml version=\"1.0\" encoding=\"UTF-8\"?>\\n<!DOCT...  \n",
       "1968ApJ___154L__99K_p2.jpeg  <?xml version=\"1.0\" encoding=\"UTF-8\"?>\\n<!DOCT...  \n",
       "...                                                                        ...  \n",
       "1991ApJ___367L__59R_p2.jpeg  <?xml version=\"1.0\" encoding=\"UTF-8\"?>\\n<!DOCT...  \n",
       "1993ApJ___417__502H_p9.jpeg  <?xml version=\"1.0\" encoding=\"UTF-8\"?>\\n<!DOCT...  \n",
       "1938ApJ____88____1B_p0.jpeg  <?xml version=\"1.0\" encoding=\"UTF-8\"?>\\n<!DOCT...  \n",
       "1964ApJ___140_1467Z_p9.jpeg  <?xml version=\"1.0\" encoding=\"UTF-8\"?>\\n<!DOCT...  \n",
       "1981ApJS___47__229E_p9.jpeg  <?xml version=\"1.0\" encoding=\"UTF-8\"?>\\n<!DOCT...  \n",
       "\n",
       "[13259 rows x 3 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfsave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "881f7a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# grab dones\n",
    "if redo_makesense:\n",
    "    #donesFileMS = 'dones_makesense.csv' # temp storage file\n",
    "    if os.path.exists(redo_makesense_folder):\n",
    "        # generate donesfile from list\n",
    "        lfiles = glob(redo_makesense_folder+'labels*csv')\n",
    "        filenames = []; labelss = []; xmins = []; ymins=[]; xmaxs=[]; ymaxs=[]; widths=[]\n",
    "        fnameSave = []; heights = []\n",
    "        for il,l in enumerate(lfiles):\n",
    "            d = pd.read_csv(l, names = ['label','xmin','ymin','xmax','ymax', 'fname','x','y'])\n",
    "            #d = d.drop_duplicates(subset='fname')\n",
    "            for idd, dd in enumerate(d['fname'].values):\n",
    "                fn = dd[:dd.rfind('.')+1]\n",
    "                if fn[-1] == '.': fn = fn[:-1]\n",
    "                filenames.append(fn)\n",
    "                fnameSave.append(dd)\n",
    "            if il == 0:\n",
    "                dfTmp = d.copy()\n",
    "            else:\n",
    "                dfTmp = dfTmp.append(d)\n",
    "        # get unique\n",
    "        fnameSave,uind = np.unique(fnameSave,return_index=True)\n",
    "        # loop and grab\n",
    "        for idd, dd in enumerate(fnameSave):\n",
    "            mask = dfTmp['fname'] == dd\n",
    "            labelss.append(dfTmp.loc[mask]['label'].values); xmins.append(dfTmp.loc[mask]['xmin'].values)\n",
    "            ymins.append(dfTmp.loc[mask]['ymin'].values); \n",
    "            # widths and heights!!\n",
    "            xmaxs.append(dfTmp.loc[mask]['xmax'].values+dfTmp.loc[mask]['xmin'].values); \n",
    "            ymaxs.append(dfTmp.loc[mask]['ymax'].values+dfTmp.loc[mask]['ymin'].values)\n",
    "            widths.append(dfTmp.loc[mask]['x'].values[0]); heights.append(dfTmp.loc[mask]['y'].values[0])\n",
    "        dones = pd.DataFrame({'filename':np.array(filenames)[uind], 'labels':labelss, 'xmin':xmins, \n",
    "                              'ymin':ymins, 'xmax':xmaxs,'ymax':ymaxs, 'width':widths, 'height':heights})\n",
    "elif bad_ann_file is not None:\n",
    "    f = pd.read_csv(bad_ann_file, delimiter = '(')\n",
    "    ws = []\n",
    "    for ff in f.index.values:\n",
    "        ws.append(ff.split('.png')[0])\n",
    "    dones = pd.DataFrame({'filename':ws})\n",
    "    \n",
    "    if os.path.exists(redo_makesense_folder_new):\n",
    "        # generate donesfile from list\n",
    "        lfiles = glob(redo_makesense_folder_new+'labels*csv')\n",
    "        filenames = []; labelss = []; xmins = []; ymins=[]; xmaxs=[]; ymaxs=[]; widths=[]\n",
    "        fnameSave = []; heights = []\n",
    "        for il,l in enumerate(lfiles):\n",
    "            d = pd.read_csv(l, names = ['label','xmin','ymin','xmax','ymax', 'fname','x','y'])\n",
    "            #d = d.drop_duplicates(subset='fname')\n",
    "            for idd, dd in enumerate(d['fname'].values):\n",
    "                fn = dd[:dd.rfind('.')+1]\n",
    "                if fn[-1] == '.': fn = fn[:-1]\n",
    "                filenames.append(fn)\n",
    "                fnameSave.append(dd)\n",
    "            if il == 0:\n",
    "                dfTmp = d.copy()\n",
    "            else:\n",
    "                dfTmp = dfTmp.append(d)\n",
    "        # get unique\n",
    "        fnameSave,uind = np.unique(fnameSave,return_index=True)\n",
    "        # loop and grab\n",
    "        for idd, dd in enumerate(fnameSave):\n",
    "            mask = dfTmp['fname'] == dd\n",
    "            labelss.append(dfTmp.loc[mask]['label'].values); xmins.append(dfTmp.loc[mask]['xmin'].values)\n",
    "            ymins.append(dfTmp.loc[mask]['ymin'].values); \n",
    "            # widths and heights!!\n",
    "            xmaxs.append(dfTmp.loc[mask]['xmax'].values+dfTmp.loc[mask]['xmin'].values); \n",
    "            ymaxs.append(dfTmp.loc[mask]['ymax'].values+dfTmp.loc[mask]['ymin'].values)\n",
    "            widths.append(dfTmp.loc[mask]['x'].values[0]); heights.append(dfTmp.loc[mask]['y'].values[0])\n",
    "        donesMS = pd.DataFrame({'filename':np.array(filenames)[uind], 'labels':labelss, 'xmin':xmins, \n",
    "                              'ymin':ymins, 'xmax':xmaxs,'ymax':ymaxs, 'width':widths, 'height':heights})\n",
    "\n",
    "else: # let's grab a bunch!  Only from the OCR-processed files    \n",
    "    ws = []\n",
    "    # loop and grab\n",
    "    for cp in ocrFiles:\n",
    "        # do a little test save here - locations of squares and figure caption boxes\n",
    "        with open(cp, 'rb') as f:\n",
    "            wsout, full_run_squares, full_run_ocr, full_run_rotations, \\\n",
    "                         full_run_lineNums, full_run_confidences, full_run_paragraphs, \\\n",
    "                        full_run_pdfwords, full_run_pdftextBoxes, \\\n",
    "                         full_run_links, full_run_gifLinkStorage, full_run_PDFlinkStorage, \\\n",
    "                         full_run_pageNumStorage, full_run_downloadLinkStorage, full_run_PDFlayouts,\\\n",
    "                        full_run_htmlTextUS, full_run_htmlText = pickle.load(f)\n",
    "\n",
    "            # splits\n",
    "            for i,w in enumerate(wsout):\n",
    "                wsout[i] = w.split('/')[-1].split('.jpeg')[0]\n",
    "\n",
    "            ws.extend(wsout)\n",
    "\n",
    "    dones = pd.DataFrame({'filename':ws})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6c4c268a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#len(fnameSave)\n",
    "#dones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "de67f6fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1993AJ____105__250G_p8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1974ApJ___191__111S_p8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1986AJ_____91__290B_p3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1986ApJ___301__346H_p5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1968ApJ___154L__99K_p2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13254</th>\n",
       "      <td>1991ApJ___367L__59R_p2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13255</th>\n",
       "      <td>1993ApJ___417__502H_p9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13256</th>\n",
       "      <td>1938ApJ____88____1B_p0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13257</th>\n",
       "      <td>1964ApJ___140_1467Z_p9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13258</th>\n",
       "      <td>1981ApJS___47__229E_p9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13259 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     filename\n",
       "0      1993AJ____105__250G_p8\n",
       "1      1974ApJ___191__111S_p8\n",
       "2      1986AJ_____91__290B_p3\n",
       "3      1986ApJ___301__346H_p5\n",
       "4      1968ApJ___154L__99K_p2\n",
       "...                       ...\n",
       "13254  1991ApJ___367L__59R_p2\n",
       "13255  1993ApJ___417__502H_p9\n",
       "13256  1938ApJ____88____1B_p0\n",
       "13257  1964ApJ___140_1467Z_p9\n",
       "13258  1981ApJS___47__229E_p9\n",
       "\n",
       "[13259 rows x 1 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# take out duplicates\n",
    "dones = dones.drop_duplicates('filename')\n",
    "dones = dones.reset_index(drop=True)\n",
    "dones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "03284e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dones.iloc[0]\n",
    "#donesMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "106b7e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "wsOrig = []\n",
    "for f in dones['filename'].values:\n",
    "    # check for file\n",
    "    if os.path.isfile(images_pulled_dir+f+'.jpeg'):\n",
    "        ff = images_pulled_dir+f+'.jpeg'\n",
    "    elif os.path.isfile(images_pulled_dir+f+'.jpg'):\n",
    "        ff = images_pulled_dir+f+'.jpg'\n",
    "    else:\n",
    "        # find correct hocr index\n",
    "        ff = glob(images_pulled_dir+f + '*')\n",
    "        #if len(ff) == 0: # could be that these aren't re-processed yet\n",
    "        #    #print('have issue!')\n",
    "        #    #import sys; sys.exit()\n",
    "        #else:\n",
    "        if len(ff) > 0:\n",
    "            ff = ff[0]\n",
    "    if len(ff) > 0: # take this out for once everything is re-processed\n",
    "        wsOrig.append(ff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "209a1f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "#wsOrig\n",
    "#labels_annotation\n",
    "#redo_makesense_folder_new\n",
    "# x = np.array(['sad','sad','bad'])\n",
    "# np.unique(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c60c08ae",
   "metadata": {},
   "source": [
    "## Start here for re-running generally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "cc03c78c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#redo_makesense\n",
    "#donesDir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "9fd4b874",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "left to classify: 7669 , already classified =  5590\n",
      "   Total current labels for figure = 5019\n",
      "   Total current labels for figure caption = 4854\n",
      "   Total current labels for table = 682\n",
      "   Total current labels for math formula = 1041\n",
      "   Total current labels for sub fig caption = 1412\n",
      "   Total current labels for colorbar = 531\n",
      "   Total current labels for NotSure = 7\n",
      "   Total current labels for no label = 555\n"
     ]
    }
   ],
   "source": [
    "# take out dones\n",
    "if not redo_makesense:\n",
    "    donesFileMS = 'dones_makesense.csv' # temp storage file\n",
    "    if os.path.exists(donesDir):\n",
    "        # generate donesfile from list\n",
    "        lfiles = glob(donesDir+'labels*csv')\n",
    "        filenames = []; labelsTotal = []\n",
    "        for l in lfiles:\n",
    "            d = pd.read_csv(l, names = ['label','xmin','ymin','xmax','ymax', 'fname','x','y'])\n",
    "            #d = d.drop_duplicates(subset='fname')\n",
    "            for dd,ll in zip(d['fname'].values,d['label'].values):\n",
    "                fn = dd[:dd.rfind('.')+1]\n",
    "                if fn[-1] == '.': fn = fn[:-1]\n",
    "                filenames.append(fn)\n",
    "                labelsTotal.append(ll)\n",
    "            d = pd.DataFrame({'filename':filenames})\n",
    "            d = d.drop_duplicates(subset='filename')\n",
    "            # write donesfile\n",
    "            #d.to_csv(storeTmps+donesFileMS,index=False)\n",
    "        filenames = np.unique(filenames)\n",
    "        wsFull2, _, mask = take_out_dones(wsOrig, filenames, anotherFile = dones['filename'].values) # do we need another file here??\n",
    "        ws = wsFull2\n",
    "        # done\n",
    "        lendone = len(wsOrig)-len(ws)\n",
    "        print('left to classify:', len(ws), ', already classified = ', lendone)\n",
    "        # print classes\n",
    "        for ll in labels:\n",
    "            numl = np.count_nonzero(np.array(labelsTotal) == ll.replace(' ','_'))\n",
    "            print('   Total current labels for', ll, '=', numl)\n",
    "    else:\n",
    "        ws = wsOrig\n",
    "else:\n",
    "    #print('hi')\n",
    "    ws = wsOrig\n",
    "    donesFileMS = 'dones_makesense.csv' # temp storage file\n",
    "    if os.path.exists(redo_makesense_folder_new):\n",
    "        # generate donesfile from list\n",
    "        lfiles = glob(redo_makesense_folder_new+'labels*csv')\n",
    "        filenames = []; labelsTotal = []\n",
    "        for l in lfiles:\n",
    "            d = pd.read_csv(l, names = ['label','xmin','ymin','xmax','ymax', 'fname','x','y'])\n",
    "            #d = d.drop_duplicates(subset='fname')\n",
    "            for dd in d['fname'].values:\n",
    "                fn = dd[:dd.rfind('.')+1]\n",
    "                if fn[-1] == '.': fn = fn[:-1]\n",
    "                filenames.append(fn)\n",
    "                labelsTotal.append(d['label'])\n",
    "            d = pd.DataFrame({'filename':filenames})\n",
    "            d = d.drop_duplicates(subset='filename')\n",
    "            # write donesfile\n",
    "            #filenames = d['fn'].values\n",
    "            d.to_csv(storeTmps+donesFileMS,index=False)\n",
    "        #wsFull2, _, mask = take_out_dones(wsOrig, storeTmps+donesFileMS, anotherFile = wsOrig) # do we need another file here??\n",
    "        wsFull2, _, mask = take_out_dones(wsOrig, filenames, anotherFile = wsOrig) # do we need another file here??\n",
    "        # delete the storeTmps+donesFileMS for starting over!\n",
    "        ws = wsFull2\n",
    "        # done\n",
    "        lendone = len(wsOrig)-len(ws)\n",
    "        print('pages left to classify:', len(ws), ', already classified = ', lendone)\n",
    "    else:\n",
    "        ws = wsOrig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "854bac0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "on icount 0 of 7669 len(ws)= 0\n",
      "on icount 100 of 7669 len(ws)= 0\n",
      "on icount 200 of 7669 len(ws)= 15\n",
      "on icount 300 of 7669 len(ws)= 62\n",
      "on icount 400 of 7669 len(ws)= 99\n"
     ]
    }
   ],
   "source": [
    "##### HERE IS WHERE WE COULD LOOK FOR TABLES SPECIFICALLY ###\n",
    "#len(wsOrig), len(ws), len(wsFull2)\n",
    "icountStart = 0 # change if you want to look further down\n",
    "boxesSave = []; scoresSave = []; labelsSave = []\n",
    "if look_for is not None:\n",
    "    ws = []\n",
    "    icount = icountStart\n",
    "    while len(ws) < nRandom and icount<len(wsFull2):\n",
    "        if icount%100 == 0: print('on icount',icount,'of', len(wsFull2), 'len(ws)=',len(ws))\n",
    "        w = binariesDir + '../'+  wsFull2[icount].split('/')[-1].split('.jpeg')[0]+'.npz'\n",
    "        image_np = np.load(w)['arr_0']\n",
    "        image_np = image_np.astype(np.float32) / 255.0\n",
    "        boxes, scores, labelsout = model.predict(image_np[np.newaxis, ...])\n",
    "        boxes, scores, labelsout = np.squeeze(boxes, 0), np.squeeze(scores, 0), np.squeeze(labelsout, 0)\n",
    "        # any tag of a table\n",
    "        ii = labels.index(look_for)\n",
    "        if ii in labelsout: ws.append(wsFull2[icount]) # found it!\n",
    "            \n",
    "        icount += 1\n",
    "    \n",
    "# while len(tables)<X ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "7aa16e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if look_for is None:\n",
    "    if nRandom < len(ws):\n",
    "        # grab randomly\n",
    "        ind = np.random.choice(range(len(ws)),nRandom,replace=False)\n",
    "        ws = np.array(ws)[ind]\n",
    "    else:\n",
    "        ws = np.array(ws)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "0343f3aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#len(ws)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "3989e544",
   "metadata": {},
   "outputs": [],
   "source": [
    "# put in the random directory\n",
    "\n",
    "# start on empty\n",
    "if not os.path.exists(storeTmps+'images/'):\n",
    "    os.makedirs(storeTmps+'images/')\n",
    "# delete and remake\n",
    "shutil.rmtree(storeTmps+'images/')\n",
    "os.makedirs(storeTmps+'images/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "6f3bb86f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# move files\n",
    "imgSizes = []; imgSizesOrig = []\n",
    "for iw,w in enumerate(ws):\n",
    "    #print(w)\n",
    "    if not invert_colors:\n",
    "        shutil.copyfile(w, storeTmps+'images/'+ w.split('/')[-1])\n",
    "        imgSizes.append(Image.open(w).convert('RGB').size)\n",
    "    else: # invert B/W\n",
    "        im = Image.open(w).convert('RGB')\n",
    "        imgSizes.append(im.size)\n",
    "        \n",
    "        if redo_makesense:\n",
    "            # get orig sizes\n",
    "            #ff = w.split('/')[-1].split('_p')[0]\n",
    "            ff = w.split('/')[-1]\n",
    "            # check for file\n",
    "            if os.path.isfile(images_pulled_dir_orig+ff):\n",
    "                indh = ff\n",
    "            elif os.path.isfile(images_pulled_dir_orig+ff):\n",
    "                indh = ff\n",
    "            else:\n",
    "                # find correct hocr index\n",
    "                ff = glob(images_pulled_dir_orig+ff.split('.')[0] + '*')\n",
    "                if len(ff) == 0 or len(ff)>1:\n",
    "                    print('have issue!')\n",
    "                    import sys; sys.exit()\n",
    "                else:\n",
    "                    indh = ff[0].split('/')[-1]     \n",
    "            with Image.open(images_pulled_dir_orig+indh).convert('L') as f:\n",
    "                imgSizesOrig.append(f.size)\n",
    "        else:\n",
    "            imgSizesOrig.append(im.size)\n",
    "        \n",
    "        im_invert = ImageOps.invert(im)\n",
    "        imgdata = np.array(im_invert)\n",
    "        # overplot paragraphs\n",
    "        plotPar = True\n",
    "        try:\n",
    "            dp = dfsave.loc[w.split('/')[-1]]\n",
    "        except:\n",
    "            plotPar = False\n",
    "            #import sys; sys.exit()\n",
    "            print('no info for:', w)\n",
    "        #dp = dfsave.loc[w.split('/')[-1]]\n",
    "        if plotPar:\n",
    "            hocr = dp['hocr']\n",
    "            # also, get paragraphs\n",
    "            # paragraphs from OCR\n",
    "            bbox_par = []\n",
    "            nameSpace = ''\n",
    "            for l in hocr.split('\\n'):\n",
    "                if 'xmlns' in l:\n",
    "                    nameSpace = l.split('xmlns=\"')[1].split('\"')[0]\n",
    "                    break\n",
    "            ns = {'tei': nameSpace}\n",
    "            tree = etree.fromstring(hocr.encode())\n",
    "            # get paragraphs\n",
    "            lines = tree.xpath(\"//tei:p[@class='ocr_par']/@title\", namespaces=ns)\n",
    "            langs = tree.xpath(\"//tei:p[@class='ocr_par']/@lang\", namespaces=ns)\n",
    "            for l,la in zip(lines,langs):\n",
    "                x = l.split(' ')\n",
    "                b = np.array(x[1:]).astype('int')\n",
    "                area = (b[3]-b[1])*(b[2]-b[0])\n",
    "                bbox_par.append((b,area,la))\n",
    "\n",
    "            fn = w.split('/')[-1]\n",
    "            fn2 = fn[:fn.rfind('.')+1]\n",
    "            if fn2[-1] == '.': fn2 = fn2[:-1]\n",
    "            #fn = w.split('/')[-1].split('_p')[0]\n",
    "            d = dones.loc[dones['filename'] == fn2]\n",
    "            widthOrig = imgSizesOrig[iw][0]*1.0; heightOrig = imgSizesOrig[iw][1]*1.0\n",
    "            width = imgSizes[iw][0]*1.0; height = imgSizes[iw][1]*1.0\n",
    "            # create fig caption bounding boxs\n",
    "            boxesOut = []; labelsOut = []; bb = []\n",
    "            if len(d) > 0 and not use_model_ann: # otherwise skip\n",
    "                for lab,xmin,ymin,xmax,ymax in zip(d['labels'].values[0], d['xmin'].values[0], \n",
    "                                                   d['ymin'].values[0],d['xmax'].values[0],d['ymax'].values[0]):\n",
    "                    b = [xmin/widthOrig*width,ymin/heightOrig*height,xmax/widthOrig*width,ymax/heightOrig*height]\n",
    "                    if 'caption' in lab.lower():\n",
    "                        iouMax = -10; \n",
    "                        indIou = []\n",
    "                        for ibb, bp in enumerate(bbox_par): # these are also xmin,ymin,xmax,ymax -- found w/OCR, original page size\n",
    "                            bb,aa,ll = bp\n",
    "                            x1min = b[0]; y1min = b[1]; x1max = b[2]; y1max = b[3]\n",
    "                            x2min, y2min, x2max, y2max = bb\n",
    "                            isOverlapping = (x1min <= x2max and x2min <= x1max and y1min <= y2max and y2min <= y1max)\n",
    "                            if isOverlapping:\n",
    "                            #if True:\n",
    "                                coords = np.array(np.round(bb), dtype=np.int32)\n",
    "                                c1 = (coords[0],coords[1]); c2 = (coords[2], coords[3])\n",
    "                                cv.rectangle(imgdata, c1, c2, (0, 255, 255), 4)  \n",
    "            elif use_model_ann and len(d) == 0:\n",
    "                print('no d!')\n",
    "                print(fn)\n",
    "\n",
    "        im_invert = Image.fromarray(imgdata)\n",
    "        im_invert.save(storeTmps+'images/'+ w.split('/')[-1], quality=100) \n",
    "        im.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "8940d806",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ff\n",
    "#d\n",
    "#copy_annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "282a5e9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all done!  go classify!\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(storeTmps+'annotations/'):\n",
    "    os.makedirs(storeTmps+'annotations/')\n",
    "# delete and remake\n",
    "shutil.rmtree(storeTmps+'annotations/')\n",
    "os.makedirs(storeTmps+'annotations/')\n",
    "\n",
    "if (copy_annotations) and bad_ann_file is None:\n",
    "    # copy\n",
    "    for iw,w in enumerate(ws):\n",
    "        fn = w.split('/')[-1]\n",
    "        fn = fn[:fn.rfind('.')+1]\n",
    "        if fn[-1] == '.': fn = fn[:-1]\n",
    "        fn = annotation_dir + fn + '.xml'\n",
    "        # open and parse -- ASSUMES LABELS ARE AS LISTED\n",
    "        #labels_annotation\n",
    "        if os.path.exists(fn): \n",
    "            img_name, bbox = parse_annotation([fn],labels_annotation) \n",
    "            # bbox is in xmin, ymin, xmax,ymax\n",
    "            xfrac = imgSizes[iw][1]*1.0/IMAGE_W; yfrac = imgSizes[iw][0]*1.0/IMAGE_H\n",
    "            with open(storeTmps+'annotations/'+ fn.split('/')[-1].split('.xml')[0] + '.txt','w') as fsave:\n",
    "                if len(bbox) > 0:\n",
    "                    for bb in bbox[0]:\n",
    "                        xmin = bb[0]*xfrac; ymin = bb[1]*yfrac; xmax = bb[2]*xfrac; ymax = bb[3]*yfrac\n",
    "                        #x = int(round(xmin/imgSizes[iw][1])); y = int(round(ymin/imgSizes[iw][0])); \n",
    "                        #w = int(round((xmax-xmin)/imgSizes[iw][1])); h = int(round((ymax-ymin)/imgSizes[iw][0]))\n",
    "                        x = xmin/imgSizes[iw][1]; y = ymin/imgSizes[iw][0]; \n",
    "                        w = (xmax-xmin)/imgSizes[iw][1]; h = (ymax-ymin)/imgSizes[iw][0]\n",
    "                        # I think we want centers?\n",
    "                        x = x+w*0.5; y = y+0.5*h\n",
    "                        lab = labels_annotation[int(bb[4])-1]\n",
    "                        if lab == 'multi-figure': lab = 'figure' # rename all to figure for now\n",
    "                        try: \n",
    "                            l = labels.index(lab)\n",
    "                        except:\n",
    "                            l = -1\n",
    "                        if l > -1: # found in these labels\n",
    "                            fsave.write(str(l) + ' ' + str(x) + ' ' + str(y) + ' ' + str(w) + ' ' + str(h) + '\\n')\n",
    "            \n",
    "    #write label file\n",
    "    with open(storeTmps + 'annotations/' + labelFile,'w') as f:\n",
    "        for l in labels:\n",
    "            f.write(l.replace(' ', '_') + '\\n')\n",
    "\n",
    "elif bad_ann_file is not None:\n",
    "    for iw,w in enumerate(ws):\n",
    "        fn = w.split('/')[-1]\n",
    "        fn = fn[:fn.rfind('.')+1]\n",
    "        if fn[-1] == '.': fn = fn[:-1]\n",
    "        fn = annotation_dir + fn + '.xml'\n",
    "        # open and parse -- ASSUMES LABELS ARE AS LISTED\n",
    "        #labels_annotation\n",
    "        if os.path.exists(fn): \n",
    "            img_name, bbox = parse_annotation([fn],labels_annotation) \n",
    "            # bbox is in xmin, ymin, xmax,ymax\n",
    "            xfrac = imgSizes[iw][1]*1.0/IMAGE_W; yfrac = imgSizes[iw][0]*1.0/IMAGE_H\n",
    "            with open(storeTmps+'annotations/'+ fn.split('/')[-1].split('.xml')[0] + '.txt','w') as fsave:\n",
    "                if len(bbox) > 0:\n",
    "                    for bb in bbox[0]:\n",
    "                        xmin = bb[0]*xfrac; ymin = bb[1]*yfrac; xmax = bb[2]*xfrac; ymax = bb[3]*yfrac\n",
    "                        #x = int(round(xmin/imgSizes[iw][1])); y = int(round(ymin/imgSizes[iw][0])); \n",
    "                        #w = int(round((xmax-xmin)/imgSizes[iw][1])); h = int(round((ymax-ymin)/imgSizes[iw][0]))\n",
    "                        x = xmin/imgSizes[iw][1]; y = ymin/imgSizes[iw][0]; \n",
    "                        w = (xmax-xmin)/imgSizes[iw][1]; h = (ymax-ymin)/imgSizes[iw][0]\n",
    "                        # I think we want centers?\n",
    "                        x = x+w*0.5; y = y+0.5*h\n",
    "                        lab = labels_annotation[int(bb[4])-1]\n",
    "                        if lab == 'multi-figure': lab = 'figure' # rename all to figure for now\n",
    "                        try: \n",
    "                            l = labels.index(lab)\n",
    "                        except:\n",
    "                            l = -1\n",
    "                        if l > -1: # found in these labels\n",
    "                            fsave.write(str(l) + ' ' + str(x) + ' ' + str(y) + ' ' + str(w) + ' ' + str(h) + '\\n')\n",
    "                # other ones\n",
    "                dMS = donesMS.loc[donesMS['filename']==fn.split('/')[-1].split('.xml')[0]]\n",
    "                ww,hh = dMS['width'].values,dMS['height'].values\n",
    "                for ls,xmins,xmaxs,ymins,ymaxs in zip(dMS['labels'].values, \n",
    "                                                            dMS['xmin'].values,\n",
    "                                                            dMS['xmax'].values,dMS['ymin'].values,\n",
    "                                                            dMS['ymax'].values):\n",
    "                    for l,xmin,xmax,ymin,ymax in zip(ls,xmins,xmaxs,ymins,ymaxs):\n",
    "                        if 'figure' != l and 'figure_caption' != l and 'table' != l:\n",
    "                            w = xmax-xmin; h = ymax-ymin\n",
    "                            fsave.write(str(labels.index(l.replace('_',' '))) + ' ' + \\\n",
    "                                        str((xmin+0.5*w)/ww[0]) + ' ' + str((ymin+0.5*h)/hh[0]) + ' ' + str(w/ww[0]) + ' ' + str(h/hh[0]) + '\\n')\n",
    "                    \n",
    "                #import sys; sys.exit()\n",
    "            \n",
    "    #write label file\n",
    "    with open(storeTmps + 'annotations/' + labelFile,'w') as f:\n",
    "        for l in labels:\n",
    "            f.write(l.replace(' ', '_') + '\\n')\n",
    "\n",
    "elif redo_makesense: # grab old annotations\n",
    "    for iw, ww in enumerate(ws):\n",
    "        if '_p' in ww:\n",
    "            fn = ww.split('/')[-1]\n",
    "            fn2 = fn[:fn.rfind('.')+1]\n",
    "            if fn2[-1] == '.': fn2 = fn2[:-1]\n",
    "            #fn = ww.split('/')[-1].split('_p')[0]\n",
    "\n",
    "            d = dones.loc[dones['filename'] == fn2]\n",
    "            w = d['width'].values[0]*1.0; h = d['height'].values[0]*1.0\n",
    "            #w = imgSizesOrig[iw][0]*1.0; h = imgSizesOrig[iw][1]*1.0\n",
    "            \n",
    "            # check ratios -- scanned pages can be subsets of full PDFs\n",
    "            rDPI = imgSizes[iw][0]/imgSizes[iw][1]; rImg = imgSizesOrig[iw][0]/imgSizesOrig[iw][1]\n",
    "            if rDPI == rImg:\n",
    "                xc = 0\n",
    "            else: # we have a border that has been cut and need to re-size\n",
    "                dX = rDPI*imgSizesOrig[iw][1] # dX1/dY1 * dY2\n",
    "                xc = (dX-imgSizesOrig[iw][0])*0.5\n",
    "            \n",
    "            #if xc != 0: import sys; sys.exit()\n",
    "            # for translation\n",
    "            fracy = imgSizes[iw][1]*1.0/imgSizesOrig[iw][1]\n",
    "            with open(storeTmps+'annotations/'+ fn2 + '.txt','w') as fsave:\n",
    "                for lab,xmin,ymin,xmax,ymax in zip(d['labels'].values[0], d['xmin'].values[0], \n",
    "                                                   d['ymin'].values[0],d['xmax'].values[0],d['ymax'].values[0]):\n",
    "                #for lab,(xmin,ymin,xmax,ymax) in zip(labelsOut,boxesOut):\n",
    "                    try: \n",
    "                        l = labels.index(lab.replace('_',' '))\n",
    "                    except:\n",
    "                        l = -1\n",
    "                    if l > -1: # found in these labels\n",
    "                        #x = xmin + 0.5*(xmax-xmin); y = ymin + 0.5*(ymax-ymin)\n",
    "                        #fsave.write(str(l) + ' ' + str(x/w) + ' ' + str(y/h) + \\\n",
    "                        #            ' ' + str((xmax-xmin)/w) + ' ' + str((ymax-ymin)/h) + '\\n')\n",
    "                        if abs(fracy-1.0) < 0.05:\n",
    "                            x1 = (xmin+xc)/fracy; y1 = ymin*fracy; x2 = (xmax-xc)*fracy; y2 = ymax*fracy\n",
    "                            xc1 = x1 + 0.5*(x2-x1); yc1 = y1 + 0.5*(y2-y1)\n",
    "                            if x1>0 and (x2-x1)> 0:\n",
    "                                fsave.write(str(l) + ' ' + str(xc1/w) + ' ' + str(min([yc1/h,1.0])) + \\\n",
    "                                        ' ' + str(min([(x2-x1)/w,1.0-x1/w])) + ' ' + str(min([(y2-y1)/h,1.0-y1/h])) + '\\n')\n",
    "                        else:\n",
    "                            xc1 = xmin + 0.5*(xmax-xmin); yc1 = ymin + 0.5*(ymax-ymin)\n",
    "                            fsave.write(str(l) + ' ' + str(xc1/w) + ' ' + str(min([yc1/h,1.0])) + \\\n",
    "                                        ' ' + str(min([(xmax-xmin)/w,1.0])) + ' ' + str(min([(ymax-ymin)/h,1.0])) + '\\n')\n",
    "                            \n",
    "\n",
    "        else:\n",
    "            print('nope for', ww)\n",
    "            \n",
    "    #write label file\n",
    "    with open(storeTmps + 'annotations/' + labelFile,'w') as f:\n",
    "        for l in labels:\n",
    "            f.write(l.replace(' ', '_') + '\\n')    \n",
    "    \n",
    "            \n",
    "if (use_model_ann) and (not redo_makesense) and bad_ann_file is None:\n",
    "    # should already be in main binaries folder\n",
    "    for w,imSize in zip(ws,imgSizes):\n",
    "        fn = w.split('/')[-1]\n",
    "        fn = fn[:fn.rfind('.')+1]\n",
    "        if fn[-1] == '.': fn = fn[:-1]\n",
    "        fn = binariesDir + '../' + fn + '.npz'\n",
    "\n",
    "        image_np = np.load(fn)['arr_0']\n",
    "        image_np = image_np.astype(np.float32) / 255.0\n",
    "        \n",
    "        boxes, scores, labelsout = model.predict(image_np[np.newaxis, ...])\n",
    "        boxes, scores, labelsout = np.squeeze(boxes, 0), np.squeeze(scores, 0), np.squeeze(labelsout, 0)\n",
    "        #print(labelsout)\n",
    "        # resize boxes\n",
    "        xfrac = imSize[1]*1.0/IMAGE_W; yfrac = imSize[0]*1.0/IMAGE_H\n",
    "        if os.path.exists(fn): \n",
    "            with open(storeTmps+'annotations/'+ fn.split('/')[-1].split('.npz')[0] + '.txt','w') as fsave:\n",
    "                for bb,ll in zip(boxes,labelsout):\n",
    "                    xmin = bb[0]*xfrac; ymin = bb[1]*yfrac; xmax = bb[2]*xfrac; ymax = bb[3]*yfrac\n",
    "                    x = xmin/imSize[1]; y = ymin/imSize[0]; \n",
    "                    w = (xmax-xmin)/imSize[1]; h = (ymax-ymin)/imSize[0]\n",
    "                    # I think we want centers?\n",
    "                    x = x+w*0.5; y = y+0.5*h\n",
    "                    #lab = labels_annotation[int(bb[4])-1]\n",
    "                    if ll > -1:\n",
    "                        #lab = LABELS[int(ll)-1]\n",
    "                        lab = LABELS[int(ll)]\n",
    "                        #print(ll,lab)\n",
    "                        if lab == 'multi-figure': lab = 'figure' # rename all to figure for now\n",
    "                        try: \n",
    "                            #l = labels.index(lab)\n",
    "                            l = labels.index(lab)\n",
    "                            #print(l)\n",
    "                        except:\n",
    "                            l = -1\n",
    "                        if l > -1: # found in these labels\n",
    "                            fsave.write(str(l) + ' ' + str(max([x,0])) + ' ' + str(max([y,0])) + ' ' + str(min([w,1])) + ' ' + str(min([h,1])) + '\\n')\n",
    "                        #import sys; sys.exit()\n",
    "        else:\n",
    "            print('something has gone wrong')\n",
    "            import sys; sys.exit()\n",
    "\n",
    "        #import sys; sys.exit()\n",
    "        \n",
    "    #write label file with ALL labels\n",
    "    with open(storeTmps + 'annotations/' + labelFile,'w') as f:\n",
    "        for l in labels:\n",
    "            f.write(l.replace(' ', '_') + '\\n')\n",
    "\n",
    "\n",
    "print('all done!  go classify!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f845c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33171c1f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a71c2ab6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d512491e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5498388d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
