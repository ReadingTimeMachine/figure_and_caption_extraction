{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"mega_yolo_train_tfrecords.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.12"}},"cells":[{"cell_type":"markdown","metadata":{"id":"iC4Y4O7bneag"},"source":["# Mega Yolo -- train the model"]},{"cell_type":"markdown","metadata":{"id":"Kado6I35Dzr1"},"source":["## Some toggles for if you want to re-start from weights"]},{"cell_type":"code","metadata":{"id":"0bgHP_GnD2SN","executionInfo":{"status":"ok","timestamp":1638890386604,"user_tz":360,"elapsed":209,"user":{"displayName":"Jill Naiman","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13328008118965652761"}}},"source":["# Do you want to re-run from an already generated train/valid/test split?\n","#  -- this is useful for feature testing and/or re-starting from weights\n","re_run_from_splits = True\n","\n","# if restarting, how many previous log files do we want to look at?\n","#####nRecent = 7 # for the 1st restart, this will be 1, for the 2nd, 2, etc\n","\n","# set to true if you are not re-running from the same dataset\n","regenAnchors = False\n"," \n","# use a saved weights file? Set to None if not and training will start anew\n","#saved_weights_file = 'weights/savedWeights/training_1_model_l0.017813377.h5'\n","saved_weights_file = None\n","\n","#fileStorage = 'binaries/' # binaries is where things are -- MAIN   \n","#extraName = '' # append to training weights name\n","\n","\n","# fileStorage = 'binaries_model1_tfrecordz/'\n","# extraName = 'model1_tfrec'\n","\n","# fileStorage = 'binaries_model10_tfrecordz/'\n","# extraName = 'model10_tfrec'\n","\n","fileStorage = 'binaries_model2_tfrecordz/'\n","extraName = 'model2_tfrec'"],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"id":"dLbPjO38y8NV","executionInfo":{"status":"ok","timestamp":1638890387433,"user_tz":360,"elapsed":509,"user":{"displayName":"Jill Naiman","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13328008118965652761"}}},"source":["# toggle for if on google collab or not\n","import os\n","thisDir = os.getcwd()\n","onGoogle = False\n","if 'content' in thisDir: onGoogle = True"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XsUy8h5AnlMU","executionInfo":{"status":"ok","timestamp":1638890389038,"user_tz":360,"elapsed":1607,"user":{"displayName":"Jill Naiman","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13328008118965652761"}},"outputId":"d30f335a-9a17-4896-f0c4-7f6b1963aff3"},"source":["\n","# Mount Google Drive\n","if onGoogle: # probably on google\n","    from google.colab import drive\n","    drive.mount('/content/gdrive/', force_remount=True)"],"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive/\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gnmYH7-3neak","executionInfo":{"status":"ok","timestamp":1638890389039,"user_tz":360,"elapsed":14,"user":{"displayName":"Jill Naiman","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13328008118965652761"}},"outputId":"ef5831fa-c8ec-4778-ed7d-ae792a13bb1d"},"source":["if onGoogle: # probably on google\n","    # find config\n","    # from pathlib import Path\n","    # for path in Path('./').rglob('config.py'):\n","    #     if path.name == 'config.py':\n","    #         continue\n","    #print(path)\n","    if not os.path.exists(\"/content/gdrive/My Drive/Colab Notebooks/scienceDigitization/\"):\n","        print(\"ERROR: path does not exist\")\n","    os.chdir(\"/content/gdrive/My Drive/Colab Notebooks/scienceDigitization/\")\n","    import config\n","\n","    print('On google')\n","    \n","    classDirMain = './'\n","    figCapMain = './'\n","    \n","    yoloWeightDir = classDirMain + 'classifications/'\n","\n","    weightsDir = classDirMain + 'classifications/'\n","    logsDir = classDirMain + 'classifications/'\n","\n","    classDirMain = './classifications/'\n","    classDirMainHOME = fileStorage \n","    splitsDir = './classifications/'\n","    logsDir = classDirMain\n","    chksDir = classDirMain\n","    saveFile = classDirMain + 'weights/testList.csv'\n","\n","    gpu_info = !nvidia-smi\n","    gpu_info = '\\n'.join(gpu_info)\n","    if gpu_info.find('failed') >= 0:\n","        print('Not connected to a GPU')\n","    else:\n","        print(gpu_info)\n","else:\n","    print('on laptop')\n","    import config\n","    classDirMain = config.save_binary_dir #+ fileStorage\n","    #figCapMain = '/Users/jillnaiman/Dropbox/wwt_image_extraction/ClassifyingImages/'\n","    #from sys import path; path.append('/Users/jillnaiman/scienceDigitization/')\n","    # where are raw images?\n","    images_pulled_dir = config.images_jpeg_dir #'/Users/jillnaiman/Dropbox/wwt_image_extraction/FigureLocalization/Pages/RandomSingleFromPDFIndexed/' ## what about Dropbox though????\n","    yoloWeightDir = config.save_weights_dir\n","    #weightsDir = '/Users/jillnaiman/Dropbox/wwt_image_extraction/FigureLocalization/mega_yolo/saved_weights/' # weights/\n","    logsDir = config.save_weights_dir #'/Users/jillnaiman/Dropbox/wwt_image_extraction/FigureLocalization/mega_yolo/' # weights/\n","    classDirMainHOME = fileStorage \n","    # note -- we are generally not running locally, so this is really tmp storage\n","    splitsDir = config.tmp_storage_dir #'/Users/jillnaiman/tmpModels/mega_yolo/'\n","    weightsDir = splitsDir\n","    logsDir = splitsDir\n","    chksDir = splitsDir\n","    saveFile = config.tmp_storage_dir + 'testList.csv'\n","    # make if not there\n","    if not os.path.exists(weightsDir+'weights/'):\n","        os.makedirs(weightsDir+'weights/')\n"],"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["On google\n","Tue Dec  7 15:19:48 2021       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 495.44       Driver Version: 460.32.03    CUDA Version: 11.2     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   31C    P0    22W / 300W |      0MiB / 16160MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"]}]},{"cell_type":"code","metadata":{"id":"H67BF_sLneah","executionInfo":{"status":"ok","timestamp":1638890389039,"user_tz":360,"elapsed":9,"user":{"displayName":"Jill Naiman","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13328008118965652761"}}},"source":["# # some parameters for different architectures of YOLO\n","batch_size = 10 #32, might be possible, 10 was what we had before\n","buffer_size = 50\n","num_epochs = 150 #150 #300"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"i6twEUYmGrFa","executionInfo":{"status":"ok","timestamp":1638890389039,"user_tz":360,"elapsed":9,"user":{"displayName":"Jill Naiman","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13328008118965652761"}},"outputId":"b0f862c1-9e4f-49d3-da26-3573b49fa4a0"},"source":["weightsDir"],"execution_count":6,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'./classifications/'"]},"metadata":{},"execution_count":6}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9GGT2mW9nean","executionInfo":{"status":"ok","timestamp":1638890389040,"user_tz":360,"elapsed":9,"user":{"displayName":"Jill Naiman","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13328008118965652761"}},"outputId":"575d9dc8-d8c4-4e6c-c36e-eb05e5d7d568"},"source":["# where annotations and features files\n","classDir_main_to = classDirMain + config.ann_name + str(int(config.IMAGE_H)) + 'x' + str(int(config.IMAGE_W))  + '_ann/'\n","\n","classDir_main_to_imgs = classDirMain + fileStorage.split('/')[-2] + '/'\n","classDir_main_to, classDir_main_to_imgs"],"execution_count":7,"outputs":[{"output_type":"execute_result","data":{"text/plain":["('./classifications/yolo_512x512_ann/',\n"," './classifications/binaries_model2_tfrecordz/')"]},"metadata":{},"execution_count":7}]},{"cell_type":"code","metadata":{"id":"vwhXD8HOneap","executionInfo":{"status":"ok","timestamp":1638890391349,"user_tz":360,"elapsed":2316,"user":{"displayName":"Jill Naiman","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13328008118965652761"}}},"source":["import os\n","import glob\n","import re\n","import numpy as np\n","import cv2 as cv\n","import matplotlib.pyplot as plt\n","import matplotlib.patches as patches\n","import xml.etree.cElementTree as ET\n","\n","# make more better?\n","#from numba import jit\n","from time import perf_counter\n","import sys\n","\n","# for v5\n","import math\n","import tensorflow as tf\n","from tensorflow.keras import backend\n","from tensorflow.keras import layers"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TpGXEVAfnear","executionInfo":{"status":"ok","timestamp":1638890391634,"user_tz":360,"elapsed":288,"user":{"displayName":"Jill Naiman","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13328008118965652761"}},"outputId":"63b966e1-298d-408a-887c-b86114986de8"},"source":["import tensorflow as tf\n","print('Tensorflow version : {}'.format(tf.__version__))\n","print('GPU : {}'.format(tf.config.list_physical_devices('GPU')))\n","from tensorflow import keras\n","import tensorflow.keras.backend as K\n","import imgaug as ia\n","from imgaug import augmenters as iaa\n","\n","# my imports\n","import pickle\n","import pandas as pd\n","from PIL import Image\n","import json\n","from scipy import stats\n","import shutil\n","\n","# for restart\n","from tensorflow.python.summary.summary_iterator import summary_iterator\n","import struct\n","from datetime import date as DATE\n","\n","from tensorflow.keras.utils import Progbar\n","import time \n","\n","\n","# get parse\n","from mega_yolo_utils import build_model, train_test_valid_split, \\\n","    process_box, process_layer, box_iou, compute_nms, iou, num_cluster, generator, \\\n","    get_n_features\n","from general_utils import parse_annotation"],"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Tensorflow version : 2.7.0\n","GPU : [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"]}]},{"cell_type":"markdown","metadata":{"id":"HUJLjHVTneav"},"source":["## First, data setup\n","\n","In data pre-processing (`generate_features_only.py`) TF records files are created."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"P_3sP0JDwiGR","executionInfo":{"status":"ok","timestamp":1638890391634,"user_tz":360,"elapsed":5,"user":{"displayName":"Jill Naiman","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13328008118965652761"}},"outputId":"dd639c5d-46d7-41cc-d252-110a4d27da9e"},"source":["train_list = glob.glob(classDir_main_to_imgs + 'train_*tfrecords')\n","valid_list = glob.glob(classDir_main_to_imgs + 'valid_*tfrecords')\n","#test_list = glob.glob(classDir_main_to_imgs + 'test_*tfrecords')\n","train_list, valid_list"],"execution_count":10,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(['./classifications/binaries_model2_tfrecordz/train_3.tfrecords',\n","  './classifications/binaries_model2_tfrecordz/train_2.tfrecords',\n","  './classifications/binaries_model2_tfrecordz/train_1.tfrecords',\n","  './classifications/binaries_model2_tfrecordz/train_0.tfrecords'],\n"," ['./classifications/binaries_model2_tfrecordz/valid_0.tfrecords'])"]},"metadata":{},"execution_count":10}]},{"cell_type":"code","metadata":{"id":"KpB7Jxx-wiGR","executionInfo":{"status":"ok","timestamp":1638890392427,"user_tz":360,"elapsed":356,"user":{"displayName":"Jill Naiman","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13328008118965652761"}}},"source":["# get raw data\n","#train_raw_data = tf.data.TFRecordDataset(train_list)\n","#valid_raw_data = tf.data.TFRecordDataset(valid_list)\n","\n","# for compressed\n","train_raw_data = tf.data.TFRecordDataset(filenames=train_list, \n","                                         compression_type='GZIP', \n","                                         buffer_size=None, \n","                                        num_parallel_reads=tf.data.AUTOTUNE)\n","valid_raw_data = tf.data.TFRecordDataset(filenames=valid_list, \n","                                         compression_type='GZIP', \n","                                         buffer_size=None,\n","                                        num_parallel_reads=tf.data.AUTOTUNE)"],"execution_count":11,"outputs":[]},{"cell_type":"code","metadata":{"id":"q-Eal3vJwiGS","executionInfo":{"status":"ok","timestamp":1638890392428,"user_tz":360,"elapsed":9,"user":{"displayName":"Jill Naiman","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13328008118965652761"}}},"source":["# Create a dictionary describing the features.\n","image_feature_description = {\n","    'nbox': tf.io.FixedLenFeature([], tf.float32),\n","    'nfeatures': tf.io.FixedLenFeature([], tf.float32),\n","    'boxes': tf.io.FixedLenFeature([], tf.string),\n","    'image_raw': tf.io.FixedLenFeature([], tf.string),\n","    'image_name': tf.io.FixedLenFeature([], tf.string),\n","}"],"execution_count":12,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"T-qo_r-cwiGS"},"source":["If we don't have access to anchors file and train/test/valid files -- read from splits.  Either way, get the labels from a file."]},{"cell_type":"code","metadata":{"id":"Qf5-lzc_wiGS","executionInfo":{"status":"ok","timestamp":1638890392428,"user_tz":360,"elapsed":8,"user":{"displayName":"Jill Naiman","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13328008118965652761"}}},"source":["LABELS = np.loadtxt(classDir_main_to_imgs + 'LABELS.csv', \n","                    dtype=str, delimiter=',')\n","CLASS = len(LABELS)\n","labels = np.arange(0,len(LABELS))"],"execution_count":13,"outputs":[]},{"cell_type":"code","metadata":{"id":"lkCJXh4rwiGS","executionInfo":{"status":"ok","timestamp":1638890392428,"user_tz":360,"elapsed":8,"user":{"displayName":"Jill Naiman","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13328008118965652761"}}},"source":["# if we have anchors already\n","def _parse_just_boxes(example_proto):\n","    image_features = tf.io.parse_single_example(example_proto, image_feature_description)\n","    # parse the data\n","    nboxes = image_features['nbox']\n","    nfeatures = image_features['nfeatures']\n","    boxes = tf.io.decode_raw(image_features['boxes'],tf.float32)\n","    boxes = tf.reshape(boxes,[nboxes,5])  \n","    return boxes"],"execution_count":14,"outputs":[]},{"cell_type":"code","metadata":{"id":"0X6FSg0hwiGT","executionInfo":{"status":"ok","timestamp":1638890392429,"user_tz":360,"elapsed":8,"user":{"displayName":"Jill Naiman","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13328008118965652761"}}},"source":["if re_run_from_splits: regenAnchors = False\n","\n","if regenAnchors:\n","#if True:\n","    boxes = train_raw_data.map(lambda example_proto:_parse_just_boxes(example_proto),num_parallel_reads=tf.data.AUTOTUNE)\n","    saved_boxes = []\n","    for ib,b in enumerate(boxes):\n","        if ib%500 == 0: print('on', ib, 'of ? (probably 5000ish for full)')\n","        saved_boxes.append(b.numpy())\n","    # valid\n","    boxes = valid_raw_data.map(lambda example_proto:_parse_just_boxes(example_proto),num_parallel_reads=tf.data.AUTOTUNE)\n","    for ib,b in enumerate(boxes):\n","        if ib%500 == 0: print('on', ib, 'of ? (probably 5000*0.15ish for valid)')\n","        saved_boxes.append(b.numpy())"],"execution_count":15,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"W48q6rc2wiGT","executionInfo":{"status":"ok","timestamp":1638890392429,"user_tz":360,"elapsed":8,"user":{"displayName":"Jill Naiman","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13328008118965652761"}},"outputId":"0d8cd69f-83ca-4fa1-81cb-8869e20054d3"},"source":["# assume location of saved anchors:\n","saveFileAnchors = classDirMain + 'weights/anchors.pickle'\n","# hack for local debugging\n","if '/Users/jillnaiman' in thisDir:\n","    saveFileAnchors = splitsDir + 'anchors.pickle'\n","\n","if regenAnchors:\n","#if True:\n","    boxes = []\n","    for bb in saved_boxes:\n","        if len(bb) > 0:\n","            for b in bb:\n","                boxes.append([b[2]-b[0], b[3]-b[1]])\n","    boxes = np.array(boxes)\n","    \n","    anchors = generator(boxes,k=num_cluster)\n","    print('NEW ANCHORS:')\n","    \n","    # save!\n","    with open(saveFileAnchors, 'wb') as ff:\n","        pickle.dump(anchors, ff)\n","else:\n","    print('from saved:')\n","    with open(saveFileAnchors, 'rb') as f:\n","        anchors = pickle.load(f)    \n","    \n","print(anchors)"],"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["from saved:\n","[[195.  21.]\n"," [203.   7.]\n"," [ 51.   5.]\n"," [313. 199.]\n"," [359. 391.]\n"," [435.  16.]\n"," [399. 307.]\n"," [204. 114.]\n"," [ 15. 355.]]\n"]}]},{"cell_type":"markdown","metadata":{"id":"7gIZwALawiGT"},"source":["Take a look at what is in each TFRecord file:"]},{"cell_type":"code","metadata":{"id":"yrTlU-v0wiGT","executionInfo":{"status":"ok","timestamp":1638890392429,"user_tz":360,"elapsed":6,"user":{"displayName":"Jill Naiman","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13328008118965652761"}}},"source":["# if we have anchors already\n","def _parse_to_view(example_proto,anchors,CLASS):\n","    image_features = tf.io.parse_single_example(example_proto, image_feature_description)\n","    # parse the data\n","    nboxes = image_features['nbox']\n","    nfeatures = image_features['nfeatures']\n","    boxes = tf.io.decode_raw(image_features['boxes'],tf.float32)\n","    boxes = tf.reshape(boxes,[nboxes,5])\n","    images_raw = image_features['image_raw']\n","    image = tf.io.decode_raw(images_raw,tf.float32)\n","    image = tf.reshape(image,[config.IMAGE_H,config.IMAGE_W,nfeatures]) \n","    return image,nboxes,nfeatures,boxes"],"execution_count":17,"outputs":[]},{"cell_type":"code","metadata":{"id":"2D7aGPgCwiGU","executionInfo":{"status":"ok","timestamp":1638890392430,"user_tz":360,"elapsed":7,"user":{"displayName":"Jill Naiman","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13328008118965652761"}}},"source":["view_dataset = valid_raw_data.map(lambda example_proto:_parse_to_view(example_proto,\n","                                                                      anchors,CLASS),\n","                                  num_parallel_calls=tf.data.AUTOTUNE)\n","# view_dataset = train_raw_data.interleave(lambda example_proto:\n","#     train_raw_data.map(_parse_to_view(example_proto,anchors,CLASS), num_parallel_calls=tf.data.AUTOTUNE),\n","#     cycle_length=4, block_length=16)\n","\n","#view_dataset = train_raw_data.interleave(lambda x: train_raw_data.map(lambda example_proto:_parse_to_view(example_proto, anchors,CLASS),num_parallel_calls=tf.data.AUTOTUNE))\n","\n","# dataset = tf.data.Dataset.from_tensor_slices(filenames)\n","# def parse_fn(filename):\n","#   return tf.data.Dataset.range(10)\n","# dataset = dataset.interleave(lambda x:\n","#     tf.data.TextLineDataset(x).map(parse_fn, num_parallel_calls=1),\n","#     cycle_length=4, block_length=16)"],"execution_count":18,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"D9eoUfW1wiGU","outputId":"d77efaa9-280a-47f8-8fff-66197da160e9"},"source":["# check one out\n","nx = 2\n","fig,ax = plt.subplots(nx,1, figsize=(20,10))\n","ix=0\n","for image,nboxes,nfeatures,boxes in view_dataset.take(nx):\n","    imgPlot = image.numpy()\n","    ax[ix].imshow(imgPlot[:,:,0],cmap='gray')\n","    print('nboxes=',nboxes)\n","    print('nfeatures=',nfeatures)\n","    print('boxes=',boxes)\n","    ix+=1"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["nboxes= tf.Tensor(2.0, shape=(), dtype=float32)\n","nfeatures= tf.Tensor(2.0, shape=(), dtype=float32)\n","boxes= tf.Tensor(\n","[[ 94.  46. 361. 487.   1.]\n"," [367.  49. 387. 485.   2.]], shape=(2, 5), dtype=float32)\n","nboxes= tf.Tensor(7.0, shape=(), dtype=float32)\n","nfeatures= tf.Tensor(2.0, shape=(), dtype=float32)\n","boxes= tf.Tensor(\n","[[ 94. 311. 416. 317.   2.]\n"," [169. 136. 344. 305.   1.]\n"," [159.  61. 432.  81.   3.]\n"," [159. 112. 430. 132.   3.]\n"," [127. 344. 428. 364.   3.]\n"," [161. 367. 234. 388.   3.]\n"," [170. 391. 332. 410.   3.]], shape=(7, 5), dtype=float32)\n"]}]},{"cell_type":"code","metadata":{"id":"RR-Tkf9SVTSy"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"K-cv6ak0wiGU"},"source":["# # debug\n","# import mega_yolo_utils\n","# from importlib import reload\n","# reload(mega_yolo_utils)\n","# from mega_yolo_utils import process_box"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Rn3QwhL3wiGU"},"source":["# if we have anchors already\n","def _parse_image_function(example_proto,anchors,CLASS):\n","    image_features = tf.io.parse_single_example(example_proto, image_feature_description)\n","    # parse the data\n","    nboxes = image_features['nbox']\n","    nfeatures = image_features['nfeatures']\n","    boxes = tf.io.decode_raw(image_features['boxes'],tf.float32)\n","    boxes = tf.reshape(boxes,[nboxes,5])\n","    images_raw = image_features['image_raw']\n","    image = tf.io.decode_raw(images_raw,tf.float32)\n","    image = tf.reshape(image,[config.IMAGE_H,config.IMAGE_W,nfeatures])\n","    # process boxes -- wrap in a tf.py_function\n","    y1,y2,y3 = tf.py_function(process_box,\n","                              (boxes[:,:4], boxes[:,4],anchors,CLASS),\n","                              (tf.float32,tf.float32,tf.float32))   \n","    return image, y1,y2,y3"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uffVLLDewiGU"},"source":["train_dataset = train_raw_data.map(lambda example_proto:_parse_image_function(example_proto,\n","                                                                             anchors,CLASS),\n","                                   num_parallel_calls=tf.data.AUTOTUNE)\n","# train_dataset = train_raw_data.interleave(lambda x: train_raw_data.map(lambda example_proto:_parse_image_function(example_proto, \n","#                                                                                                                   anchors,CLASS),\n","#                                                                        num_parallel_calls=tf.data.AUTOTUNE))\n","\n","valid_dataset = valid_raw_data.map(lambda example_proto:_parse_image_function(example_proto,\n","                                                                              anchors,CLASS), \n","                                   num_parallel_calls=tf.data.AUTOTUNE)\n","# valid_dataset = valid_raw_data.interleave(lambda x: valid_raw_data.map(lambda example_proto:_parse_image_function(example_proto, \n","#                                                                                                                   anchors,CLASS),\n","#                                                                        num_parallel_calls=tf.data.AUTOTUNE))\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VXKIh_SowiGV"},"source":["# ***** ADD TRAINING AUGMENTATION HERE JUST FOR TRAINING SET ********"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"w4aAl41uwiGV"},"source":["train_dataset = train_dataset.shuffle(buffer_size)\n","train_dataset = train_dataset.repeat()\n","train_dataset = train_dataset.batch(batch_size)\n","train_dataset = train_dataset.prefetch(tf.data.AUTOTUNE)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dChKSxmGwiGV"},"source":["valid_dataset = valid_dataset.shuffle(buffer_size)\n","valid_dataset = valid_dataset.repeat()\n","valid_dataset = valid_dataset.batch(batch_size)\n","valid_dataset = valid_dataset.prefetch(tf.data.AUTOTUNE)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KAg-HL8UwiGV"},"source":["# check one out, but only if you wanna\n","for image,y1,y2,y3 in train_dataset.take(1):\n","    imgPlot = image.numpy()\n","    print(imgPlot.shape)\n","    plt.imshow(imgPlot[0,:,:,0],cmap='gray')\n","    #print(y1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EoCivx-Jneav"},"source":["# Grab annotations to calculate steps... sometimes this can take a bit...\n","annotations = glob.glob(classDir_main_to + '*')\n","len(annotations)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0R42fLapLPI-"},"source":["labels, LABELS"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RRyluqn6vvaW"},"source":["Steps per epoch -- training:"]},{"cell_type":"code","metadata":{"id":"CLo56cF3wiGW"},"source":["# debug\n","# from importlib import reload\n","# reload(config)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yZqiCyEIwiGW"},"source":["# guestimate based on splits\n","len_train = len(annotations)*config.train_per\n","len_valid = len(annotations)*config.valid_per"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SdEYPW-tvvaW"},"source":["#steps = len(X_train) // batch_size\n","#print(len(X_train)//batch_size)\n","# can do larger with augmentation\n","\n","#aug_fac = 2 # 2 or 3\n","\n","aug_fac = 1 # set to 1 if no data aug\n","\n","steps_training = int((len_train//batch_size)*aug_fac)\n","# factor of 2 from: https://stackoverflow.com/questions/49922252/choosing-number-of-steps-per-epoch\n","\n","steps_val = int((len_valid//batch_size)*aug_fac)\n","\n","print('Steps per epoch = training:', steps_training, ', validation:', steps_val)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7gQi6kS8nea8"},"source":["# 1. Define YOLO model\n","\n","For v5, see: https://github.com/jahongir7174/YOLOv5-tf"]},{"cell_type":"markdown","metadata":{"id":"zoBEY7n67jy1"},"source":["For creating the model -- how many features are we using:"]},{"cell_type":"code","metadata":{"id":"opMDc4u5wiGX"},"source":["# if we have anchors already\n","def _parse_nfeatures(example_proto):\n","    image_features = tf.io.parse_single_example(example_proto, image_feature_description)\n","    # parse the data\n","    nfeatures = image_features['nfeatures']\n","    return nfeatures"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YB3FajGqwiGX"},"source":["nfeatures_data = train_raw_data.map(lambda example_proto:_parse_nfeatures(example_proto))\n","n_features = -1\n","for f in nfeatures_data.take(1):\n","    n_features = int(f.numpy())\n","n_features"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yXdkSQK9Emky"},"source":["Build YOLOv5:"]},{"cell_type":"code","metadata":{"id":"KRQsM_X7XC-V"},"source":["tf.keras.backend.clear_session()\n","version = 'l' # large version\n","model = build_model(n_features, anchors, version, len(LABELS))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0Vnf5mjn7jy2","scrolled":true},"source":["model.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6tXmyitL7jy2"},"source":["# plot if you wanna\n","#tf.keras.utils.plot_model(model_v5, \"yolo_v5.png\", show_shapes=True, show_layer_names=True, expand_nested=False)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AS4ZcPQg7jy2"},"source":["For optimizer:"]},{"cell_type":"code","metadata":{"id":"IWLTDSz77jy2"},"source":["LRrate = 0.004\n","#LRrate = 0.002\n","\n","class CosineLR(tf.optimizers.schedules.LearningRateSchedule):\n","    def __init__(self,steps):\n","        super().__init__()\n","        self.lr = LRrate * batch_size / 64\n","        self.warmup_init = LRrate/10.\n","        self.warmup_step = steps\n","        self.decay_steps = tf.cast((num_epochs - 1) * self.warmup_step, tf.float32)\n","\n","    def __call__(self, step):\n","        linear_warmup = tf.cast(step, dtype=tf.float32) / self.warmup_step * (self.lr - self.warmup_init)\n","        cosine_lr = 0.5 * self.lr * (1 + tf.cos(math.pi * tf.cast(step, tf.float32) / self.decay_steps))\n","        return tf.where(step < self.warmup_step, self.warmup_init + linear_warmup, cosine_lr)\n","\n","    def get_config(self):\n","        pass"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JpreDZDZvvaW"},"source":["optimizer = tf.keras.optimizers.Adam(CosineLR(steps_training), 0.937)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_l3BLK6XDRms"},"source":["# load weights if you wanna\n","if saved_weights_file is not None:\n","    weightsFiles = glob.glob(weightsDir + 'weights/' + '*h5')\n","    # OR\n","    if saved_weights_file is not None:\n","      weightsFiles = [classDirMain+saved_weights_file]\n","    model.load_weights(weightsFiles[0])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3MAgw3DHD_nA"},"source":["#print(weightsFiles)\n","optimizer.learning_rate.lr"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RKk7HxgC7jy3"},"source":["# For processing data:"]},{"cell_type":"markdown","metadata":{"id":"uvgdY6ly7jy4"},"source":["Including Augmentation like a boss!"]},{"cell_type":"code","metadata":{"id":"nm5833D_7jy5"},"source":["######aug_train_dataset = augmentation_generator(train_dataset, anchors, CLASS)\n","aug_train_dataset = train_dataset"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"b5doQu9r7jy5"},"source":["For calculating the loss:"]},{"cell_type":"code","metadata":{"id":"oWeoMEdE7jy5"},"source":["class ComputeLoss(object):\n","    def __init__(self):\n","        super().__init__()\n","\n","    @staticmethod\n","    def compute_loss(y_pred, y_true, anchors):\n","        grid_size = tf.shape(y_pred)[1:3]\n","        ratio = tf.cast(tf.constant([config.IMAGE_W, config.IMAGE_H]) / grid_size, tf.float32)\n","        batch_size = tf.cast(tf.shape(y_pred)[0], tf.float32)\n","\n","        x_y_offset, pred_boxes, pred_conf, pred_prob = process_layer(y_pred, anchors,CLASS)\n","\n","        object_mask = y_true[..., 4:5]\n","\n","        def cond(idx, _):\n","            return tf.less(idx, tf.cast(batch_size, tf.int32))\n","\n","        def body(idx, mask):\n","            valid_true_boxes = tf.boolean_mask(y_true[idx, ..., 0:4],\n","                                               tf.cast(object_mask[idx, ..., 0], 'bool'))\n","            iou = box_iou(pred_boxes[idx], valid_true_boxes)\n","            return idx + 1, mask.write(idx, tf.cast(tf.reduce_max(iou, axis=-1) < 0.2, tf.float32))\n","\n","        ignore_mask = tf.TensorArray(dtype=tf.float32, size=0, dynamic_size=True)\n","        \n","        #print('here1.1')\n","        #print(cond, body, ignore_mask)\n","\n","        _, ignore_mask = tf.while_loop(cond=cond, body=body, loop_vars=[0, ignore_mask])\n","        \n","        #print('here1.2')\n","        ignore_mask = ignore_mask.stack()\n","        ignore_mask = tf.expand_dims(ignore_mask, -1)\n","\n","        true_xy = y_true[..., 0:2] / ratio[::-1] - x_y_offset\n","        pred_xy = pred_boxes[..., 0:2] / ratio[::-1] - x_y_offset\n","\n","        true_tw_th = y_true[..., 2:4] / anchors\n","        pred_tw_th = pred_boxes[..., 2:4] / anchors\n","        true_tw_th = tf.where(tf.equal(true_tw_th, 0), tf.ones_like(true_tw_th), true_tw_th)\n","        pred_tw_th = tf.where(tf.equal(pred_tw_th, 0), tf.ones_like(pred_tw_th), pred_tw_th)\n","        true_tw_th = tf.math.log(tf.clip_by_value(true_tw_th, 1e-9, 1e+9))\n","        pred_tw_th = tf.math.log(tf.clip_by_value(pred_tw_th, 1e-9, 1e+9))\n","\n","        box_loss_scale = y_true[..., 2:3] * y_true[..., 3:4]\n","        # note -- assumes IMAGE_H == IMAGE_W\n","        box_loss_scale = 2. - box_loss_scale / tf.cast(config.IMAGE_H ** 2, tf.float32)\n","\n","        xy_loss = tf.reduce_sum(tf.square(true_xy - pred_xy) * object_mask * box_loss_scale)\n","        wh_loss = tf.reduce_sum(tf.square(true_tw_th - pred_tw_th) * object_mask * box_loss_scale)\n","\n","        conf_pos_mask = object_mask\n","        conf_neg_mask = (1 - object_mask) * ignore_mask\n","        conf_loss_pos = conf_pos_mask * tf.nn.sigmoid_cross_entropy_with_logits(labels=object_mask, logits=pred_conf)\n","        conf_loss_neg = conf_neg_mask * tf.nn.sigmoid_cross_entropy_with_logits(labels=object_mask, logits=pred_conf)\n","        # try this\n","        #conf_loss_pos = conf_pos_mask * -tf.reduce_sum(object_mask*tf.math.log(tf.clip_by_value(pred_conf,1e-10,1.0)))\n","        #conf_loss_neg = conf_neg_mask * -tf.reduce_sum(object_mask*tf.math.log(tf.clip_by_value(pred_conf,1e-10,1.0)))\n","\n","\n","        conf_loss = tf.reduce_sum((conf_loss_pos + conf_loss_neg))\n","\n","        true_conf = y_true[..., 5:]\n","\n","        class_loss = object_mask * tf.nn.sigmoid_cross_entropy_with_logits(true_conf, pred_prob)\n","        #class_loss = object_mask * -tf.reduce_sum(true_conf*tf.math.log(tf.clip_by_value(pred_conf,1e-10,1.0)))\n","        #class_loss = object_mask * tf.losses.categorical_crossentropy(true_conf, pred_prob)\n","        #tf.losses.sparse_softmax_cross_entropy(y, logits)\n","        class_loss = tf.reduce_sum(class_loss) # sum across all -- 1 number for loss\n","\n","        if np.isnan(xy_loss):\n","          print('xy_loss is NaN')\n","        if np.isnan(wh_loss):\n","          print('wh_loss is NaN')\n","        if np.isnan(conf_loss):\n","          print('conf_loss is NaN')#, conf_loss_pos, conf_loss_neg)\n","        if np.isnan(class_loss):\n","          print('class_loss is NaN')\n","\n","        if np.isnan(xy_loss + wh_loss + conf_loss + class_loss):\n","          print('--- object mask ---')\n","          print(object_mask.numpy().shape, pred_conf.numpy().shape, true_conf.numpy().shape)\n","          print(object_mask.numpy().max(), object_mask.numpy().min())\n","          print(object_mask)\n","          print(' ')\n","          print('--------')\n","        #else:\n","        #  print(object_mask.numpy().max(), object_mask.numpy().min())\n","\n","        return xy_loss + wh_loss + conf_loss + class_loss\n","\n","    def __call__(self, y_pred, y_true):\n","        loss = 0.\n","        anchor_group = [anchors[6:9], anchors[3:6], anchors[0:3]]\n","\n","        for i in range(len(y_pred)):\n","            loss += self.compute_loss(y_pred[i], y_true[i], anchor_group[i])\n","        return loss"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XZZsr5247jy6"},"source":["loss_object = ComputeLoss()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZbiMja467jy6"},"source":["def compute_loss(y_true, y_pred):\n","    total_loss = loss_object(y_pred, y_true)\n","    return tf.reduce_sum(total_loss) / batch_size"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fbZopKn57jy6"},"source":["def train_step(image, y_true):\n","    with tf.GradientTape() as tape:\n","        y_pred = model(image, training=True)\n","        #print(y_pred[0].shape, y_true[0].shape)\n","        loss = compute_loss(y_true, y_pred)\n","    if not np.isnan(loss):\n","        variables = model.trainable_variables\n","        gradients = tape.gradient(loss, variables)\n","        #if np.isnan(loss):\n","        #  print('nan')\n","        #  print\n","        optimizer.apply_gradients(zip(gradients, variables))\n","    else: # this will stop if we have non-convergence \n","        print('is NaN -- probably want to lower your learning rate!!!!')\n","        import sys; sys.exit()\n","    return loss"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"l1ftaeHUvvab"},"source":["# save weights\n","def save_best_weights(model, name, val_loss_avg):\n","    # delete existing weights file\n","    files = glob.glob(os.path.join(weightsDir + 'weights/', name + '*'))\n","    for file in files:\n","        os.remove(file)\n","    # create new weights file\n","    name = name + '_model_' +version + str(val_loss_avg) + '.h5'\n","    path_name = os.path.join(weightsDir +'weights/', name)\n","    model.save_weights(path_name)\n","    \n","    \n","# log (tensorboard)\n","def log_loss(loss, val_loss, step):\n","    tf.summary.scalar('loss', loss, step)\n","    tf.summary.scalar('val_loss', val_loss, step)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"omK8Zjfc7jy6"},"source":["# training\n","def train(epochs, model, train_dataset, val_dataset, steps_per_epoch_train, \n","          steps_per_epoch_val, optimizer, train_name = 'train'):\n","    '''\n","    Train YOLO model for n epochs.\n","    Eval loss on training and validation dataset.\n","    Log training loss and validation loss for tensorboard.\n","    Save best weights during training (according to validation loss).\n","\n","    Parameters\n","    ----------\n","    - epochs : integer, number of epochs to train the model.\n","    - model : YOLO model.\n","    - train_dataset : YOLO ground truth and image generator from training dataset.\n","    - val_dataset : YOLO ground truth and image generator from validation dataset.\n","    - steps_per_epoch_train : integer, number of batch to complete one epoch for train_dataset.\n","    - steps_per_epoch_val : integer, number of batch to complete one epoch for val_dataset.\n","    - train_name : string, training name used to log loss and save weights.\n","    \n","    Notes :\n","    - train_dataset and val_dataset generate YOLO ground truth tensors : detector_mask,\n","      matching_true_boxes, class_one_hot, true_boxes_grid. Shape of these tensors (batch size, tensor shape).\n","    - steps per epoch = number of images in dataset // batch size of dataset\n","    \n","    Returns\n","    -------\n","    - loss history : [train_loss_history, val_loss_history] : list of average loss for each epoch.\n","    '''\n","    num_epochs1 = epochs\n","    #steps_per_epoch_train = steps_per_epoch_train #why?\n","    #steps_per_epoch_val = steps_per_epoch_val # why?\n","    train_loss_history = []\n","    val_loss_history = []\n","    best_val_loss = 1e6\n","    \n","    # optimizer\n","    #optimizer = tf.keras.optimizers.Adam(learning_rate=1e-5, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n","    \n","    # log (tensorboard)\n","    summary_writer = tf.summary.create_file_writer(os.path.join(logsDir+'logs/', train_name), \n","                                                   flush_millis=20000)\n","    summary_writer.set_as_default()\n","    \n","    metrics_names = ['loss','val_loss','dt(min)'] \n","\n","    start_time = time.time()\n","    print('START: ', time.ctime(start_time-6*60*60), 'CT')\n","    # training\n","    #iepoch = 0\n","    dt = []\n","    for epoch in range(num_epochs1):\n","        t1c = time.time()\n","        t1 = time.perf_counter()\n","        pb_i = Progbar(steps_per_epoch_train+steps_per_epoch_val, stateful_metrics=metrics_names) # progress bar!\n","\n","        epoch_loss = []\n","        epoch_val_loss = []\n","        epoch_val_sub_loss = []\n","        print('Epoch {} :'.format(epoch))\n","        for image,y_true_1,y_true_2, y_true_3 in train_dataset.take(steps_per_epoch_train): ## THIS MIGHT BE INEFFICENT\n","            y_true = (y_true_1, y_true_2, y_true_3)\n","            loss = train_step(image, y_true)\n","            epoch_loss.append(loss)\n","\n","            t2 = time.perf_counter()\n","            values=[('loss',loss), ('val_loss',np.nan), ('dt(min)',(t2-t1)/60.)]\n","            pb_i.add(1, values=values)\n","\n","            #print('-', end='',flush=True)\n","\n","\n","        #print(' | ', end='')\n","        #tloss = loss #\n","        # val\n","        # for batch_idx in range(steps_per_epoch_val): \n","        #     image, y_true_1, y_true_2, y_true_3 = next(val_dataset)\n","        for image,y_true_1,y_true_2, y_true_3 in valid_dataset.take(steps_per_epoch_val): ## THIS MIGHT BE INEFFICENT\n","            y_true = (y_true_1, y_true_2, y_true_3)\n","            loss = train_step(image, y_true)\n","            epoch_val_loss.append(loss)\n","            t2 = time.perf_counter()\n","            values=[('loss',epoch_loss[-1]), ('val_loss',loss), ('dt(min)',(t2-t1)/60.)]\n","            pb_i.add(1, values=values)\n","\n","            #print('-', end='',flush=True)\n","\n","        loss_avg = np.mean(np.array(epoch_loss))\n","        val_loss_avg = np.mean(np.array(epoch_val_loss))\n","        train_loss_history.append(loss_avg)\n","        val_loss_history.append(val_loss_avg)\n","        \n","        # log\n","        log_loss(loss_avg, val_loss_avg, epoch)\n","        \n","        # save\n","        if val_loss_avg < best_val_loss:\n","            save_best_weights(model, train_name, val_loss_avg)\n","            best_val_loss = val_loss_avg\n","            #tf.saved_model.save(model, chksDir + 'checkpoints/'+today)        \n","        end_time = time.time()\n","        dt.append(end_time-t1c)\n","        if len(dt) > 0:\n","            eta = time.ctime(start_time+np.mean(dt)*(num_epochs1)-6*60*60) # offset from UTC to central\n","        else:\n","            eta = 'NaN'\n","        eta += ' CT'\n","        with np.errstate(divide='ignore',invalid='ignore'): # ignore stuff\n","            print(' loss = {:.4f}, val_loss = {:.4f}, ETA = {}'.format(loss_avg, val_loss_avg,eta))\n","        #iepoch+=1\n","        \n","    return [train_loss_history, val_loss_history]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UMxaRVJBWf7-"},"source":["results = train(num_epochs, model, aug_train_dataset, valid_dataset, \n","              steps_training, steps_val, optimizer,'training_1'+extraName)\n","\n","# # debug\n","# results = train(num_epochs, model, aug_train_dataset, valid_dataset, \n","#                1, 1, optimizer,'training_1'+extraName)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oqGPwg0R7jy6"},"source":["Plot diagnostics:"]},{"cell_type":"code","metadata":{"id":"68HC1xUF7jy6"},"source":["fig, ax = plt.subplots(figsize=(30,14))\n","ax.plot(results[0], label='Training Loss')\n","ax.plot(results[1], label='Validation Loss')\n","plt.legend()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8eIrQ2HH5C8Z"},"source":["fig, ax = plt.subplots(figsize=(30,14))\n","ax.plot(results[0], label='Training Loss')\n","ax.plot(results[1], label='Validation Loss')\n","ax.set_ylim(0.0,0.5)\n","minx = np.argmin(results[1])\n","ax.plot([minx,minx],[np.min(results[1]),np.max(results[1])])\n","ax.set_title('min val = '+str(np.min(results[1])))\n","plt.legend()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wXSKvTs95Pp6"},"source":["fig, ax = plt.subplots(figsize=(30,14))\n","ax.plot(results[0], label='Training Loss')\n","ax.plot(results[1], label='Validation Loss')\n","ax.set_ylim(1e-3,0.1)\n","ax.set_xlim(50,150)\n","ax.set_yscale('log')\n","plt.legend()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ihigRX4m5QaB"},"source":[""],"execution_count":null,"outputs":[]}]}