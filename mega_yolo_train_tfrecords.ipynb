{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iC4Y4O7bneag"
   },
   "source": [
    "# Mega Yolo -- train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kado6I35Dzr1"
   },
   "source": [
    "## Some toggles for if you want to re-start from weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "executionInfo": {
     "elapsed": 379,
     "status": "ok",
     "timestamp": 1638452980366,
     "user": {
      "displayName": "Jill Naiman",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "13328008118965652761"
     },
     "user_tz": 360
    },
    "id": "0bgHP_GnD2SN"
   },
   "outputs": [],
   "source": [
    "# Do you want to re-run from an already generated train/valid/test split?\n",
    "#  -- this is useful for feature testing and/or re-starting from weights\n",
    "re_run_from_splits = True\n",
    "\n",
    "# if restarting, how many previous log files do we want to look at?\n",
    "nRecent = 7 # for the 1st restart, this will be 1, for the 2nd, 2, etc\n",
    "\n",
    "# set to true if you are not re-running from the same dataset\n",
    "regenAnchors = False\n",
    " \n",
    "# use a saved weights file? Set to None if not and training will start anew\n",
    "#saved_weights_file = 'weights/savedWeights/training_1_model_l0.017813377.h5'\n",
    "saved_weights_file = None\n",
    "\n",
    "#fileStorage = 'binaries/' # binaries is where things are -- MAIN   \n",
    "#extraName = '' # append to training weights name\n",
    "\n",
    "# for feature collections\n",
    "#fileStorage = 'binaries_model1/' # binaries is where things are -- MAIN   \n",
    "#extraName = 'model1' # never use 8, this is our usual model?\n",
    "\n",
    "#fileStorage = 'binaries_model1_inverted/' # binaries is where things are -- MAIN   \n",
    "#extraName = 'model1_inverted'\n",
    "\n",
    "#fileStorage = 'binaries_model1_inverted_palletized/'\n",
    "#extraName = 'model1_inverted_palletized'\n",
    "\n",
    "# fileStorage = 'binaries_model2/'\n",
    "# extraName = 'model2'\n",
    "\n",
    "#fileStorage = 'binaries_model3/'\n",
    "#extraName = 'model3'\n",
    "\n",
    "#fileStorage = 'binaries_model4/'\n",
    "#extraName = 'model4'\n",
    "\n",
    "#fileStorage = 'binaries_model5/'\n",
    "#extraName = 'model5'\n",
    "\n",
    "#fileStorage = 'binaries_model5_maxTag125/'\n",
    "#extraName = 'model5_maxTag125'\n",
    "\n",
    "# fileStorage = 'binaries_model6/'\n",
    "# extraName = 'model6'\n",
    "\n",
    "# fileStorage = 'binaries_model8/'\n",
    "# extraName = 'model8'\n",
    "\n",
    "fileStorage = 'binaries_model8_tfrecordz/'\n",
    "extraName = 'model8_tfrec'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "executionInfo": {
     "elapsed": 18,
     "status": "ok",
     "timestamp": 1638452980368,
     "user": {
      "displayName": "Jill Naiman",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "13328008118965652761"
     },
     "user_tz": 360
    },
    "id": "dLbPjO38y8NV"
   },
   "outputs": [],
   "source": [
    "# toggle for if on google collab or not\n",
    "import os\n",
    "thisDir = os.getcwd()\n",
    "onGoogle = False\n",
    "if 'content' in thisDir: onGoogle = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 27939,
     "status": "ok",
     "timestamp": 1638453008289,
     "user": {
      "displayName": "Jill Naiman",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "13328008118965652761"
     },
     "user_tz": 360
    },
    "id": "XsUy8h5AnlMU",
    "outputId": "bdb3a427-d219-4752-b923-1c5c33fdda17"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Mount Google Drive\n",
    "if onGoogle: # probably on google\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/gdrive/', force_remount=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2935,
     "status": "ok",
     "timestamp": 1638453011220,
     "user": {
      "displayName": "Jill Naiman",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "13328008118965652761"
     },
     "user_tz": 360
    },
    "id": "gnmYH7-3neak",
    "outputId": "d8f5f748-88bb-49f1-9621-528affb73ea9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "on laptop\n"
     ]
    }
   ],
   "source": [
    "if onGoogle: # probably on google\n",
    "    # find config\n",
    "    # from pathlib import Path\n",
    "    # for path in Path('./').rglob('config.py'):\n",
    "    #     if path.name == 'config.py':\n",
    "    #         continue\n",
    "    #print(path)\n",
    "    if not os.path.exists(\"/content/gdrive/My Drive/Colab Notebooks/scienceDigitization/\"):\n",
    "        print(\"ERROR: path does not exist\")\n",
    "    os.chdir(\"/content/gdrive/My Drive/Colab Notebooks/scienceDigitization/\")\n",
    "    import config\n",
    "\n",
    "    print('On google')\n",
    "    \n",
    "    classDirMain = './'\n",
    "    figCapMain = './'\n",
    "    \n",
    "    yoloWeightDir = classDirMain + 'classifications/'\n",
    "\n",
    "    weightsDir = classDirMain + 'classifications/'\n",
    "    logsDir = classDirMain + 'classifications/'\n",
    "\n",
    "    classDirMain = './classifications/'\n",
    "    classDirMainHOME = fileStorage \n",
    "    splitsDir = './classifications/'\n",
    "    logsDir = classDirMain\n",
    "    chksDir = classDirMain\n",
    "    saveFile = classDirMain + 'weights/testList.csv'\n",
    "else:\n",
    "    print('on laptop')\n",
    "    import config\n",
    "    classDirMain = config.save_binary_dir #+ fileStorage\n",
    "    #figCapMain = '/Users/jillnaiman/Dropbox/wwt_image_extraction/ClassifyingImages/'\n",
    "    #from sys import path; path.append('/Users/jillnaiman/scienceDigitization/')\n",
    "    # where are raw images?\n",
    "    images_pulled_dir = config.images_jpeg_dir #'/Users/jillnaiman/Dropbox/wwt_image_extraction/FigureLocalization/Pages/RandomSingleFromPDFIndexed/' ## what about Dropbox though????\n",
    "    yoloWeightDir = config.save_weights_dir\n",
    "    #weightsDir = '/Users/jillnaiman/Dropbox/wwt_image_extraction/FigureLocalization/mega_yolo/saved_weights/' # weights/\n",
    "    logsDir = config.save_weights_dir #'/Users/jillnaiman/Dropbox/wwt_image_extraction/FigureLocalization/mega_yolo/' # weights/\n",
    "    classDirMainHOME = fileStorage \n",
    "    # note -- we are generally not running locally, so this is really tmp storage\n",
    "    splitsDir = config.tmp_storage_dir #'/Users/jillnaiman/tmpModels/mega_yolo/'\n",
    "    weightsDir = splitsDir\n",
    "    logsDir = splitsDir\n",
    "    chksDir = splitsDir\n",
    "    saveFile = config.tmp_storage_dir + 'testList.csv'\n",
    "    # make if not there\n",
    "    if not os.path.exists(weightsDir+'weights/'):\n",
    "        os.makedirs(weightsDir+'weights/')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "executionInfo": {
     "elapsed": 21,
     "status": "ok",
     "timestamp": 1638453011221,
     "user": {
      "displayName": "Jill Naiman",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "13328008118965652761"
     },
     "user_tz": 360
    },
    "id": "H67BF_sLneah"
   },
   "outputs": [],
   "source": [
    "# # some parameters for different architectures of YOLO\n",
    "batch_size = 10#32, might be possible\n",
    "buffer_size = 50\n",
    "num_epochs = 150 #150 #300\n",
    "\n",
    "# #IMAGE_H, IMAGE_W = 512, 512\n",
    "# image_size = config.IMAGE_H # assume width=height\n",
    "\n",
    "# TRAIN_BATCH_SIZE = batch_size #10\n",
    "# VAL_BATCH_SIZE   = batch_size #10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 20,
     "status": "ok",
     "timestamp": 1638453011221,
     "user": {
      "displayName": "Jill Naiman",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "13328008118965652761"
     },
     "user_tz": 360
    },
    "id": "i6twEUYmGrFa",
    "outputId": "4e9f9c12-3084-4101-c8e8-df1c5d71c718"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/jillnaiman/Downloads/tmp/'"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weightsDir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 19,
     "status": "ok",
     "timestamp": 1638453011221,
     "user": {
      "displayName": "Jill Naiman",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "13328008118965652761"
     },
     "user_tz": 360
    },
    "id": "9GGT2mW9nean",
    "outputId": "d992623e-6dc0-4050-f840-8196b8c297fc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/Users/jillnaiman/MegaYolo/yolo_512x512_ann/',\n",
       " '/Users/jillnaiman/MegaYolo/binaries_model8_tfrecordz/')"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# where annotations and features files\n",
    "#classDir_main_to = classDirMain + 'yolo_512x512_cap_ann/'\n",
    "#classDir_main_to_imgs = classDirMain + 'binaries/'#+ 'yolo_512x512/'\n",
    "classDir_main_to = classDirMain + config.ann_name + str(int(config.IMAGE_H)) + 'x' + str(int(config.IMAGE_W))  + '_ann/'\n",
    "\n",
    "classDir_main_to_imgs = classDirMain + fileStorage.split('/')[-2] + '/'\n",
    "classDir_main_to, classDir_main_to_imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "executionInfo": {
     "elapsed": 16,
     "status": "ok",
     "timestamp": 1638453011222,
     "user": {
      "displayName": "Jill Naiman",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "13328008118965652761"
     },
     "user_tz": 360
    },
    "id": "IVGN-eMPe0-c"
   },
   "outputs": [],
   "source": [
    "#!conda install numba --yes\n",
    "#logsDir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "executionInfo": {
     "elapsed": 2341,
     "status": "ok",
     "timestamp": 1638453013547,
     "user": {
      "displayName": "Jill Naiman",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "13328008118965652761"
     },
     "user_tz": 360
    },
    "id": "vwhXD8HOneap"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import re\n",
    "import numpy as np\n",
    "import cv2 as cv\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import xml.etree.cElementTree as ET\n",
    "\n",
    "# make more better?\n",
    "#from numba import jit\n",
    "from time import perf_counter\n",
    "import sys\n",
    "\n",
    "# for v5\n",
    "import math\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1779,
     "status": "ok",
     "timestamp": 1638453015322,
     "user": {
      "displayName": "Jill Naiman",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "13328008118965652761"
     },
     "user_tz": 360
    },
    "id": "TpGXEVAfnear",
    "outputId": "1d71eaa5-40ff-4c42-ede3-410568a7a371"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow version : 2.4.1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print('Tensorflow version : {}'.format(tf.__version__))\n",
    "#print('GPU : {}'.format(tf.config.list_physical_devices('GPU')))\n",
    "from tensorflow import keras\n",
    "import tensorflow.keras.backend as K\n",
    "#from tensorflow.keras.layers import Concatenate, concatenate, Dropout, \\\n",
    "#   LeakyReLU, Reshape, Activation, Conv2D, Input, MaxPooling2D, \\\n",
    "#   BatchNormalization, Flatten, Dense, Lambda\n",
    "import imgaug as ia\n",
    "from imgaug import augmenters as iaa\n",
    "\n",
    "# my imports\n",
    "import pickle\n",
    "#from classification_utils import make_get_csv\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import json\n",
    "#from classification_utils import train_test_valid_split\n",
    "from scipy import stats\n",
    "import shutil\n",
    "\n",
    "# for restart\n",
    "from tensorflow.python.summary.summary_iterator import summary_iterator\n",
    "import struct\n",
    "from datetime import date as DATE\n",
    "\n",
    "# get parse\n",
    "from mega_yolo_utils import build_model, train_test_valid_split, \\\n",
    "    process_box, process_layer, box_iou, compute_nms, iou, num_cluster, generator, \\\n",
    "    get_n_features\n",
    "from general_utils import parse_annotation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HUJLjHVTneav"
   },
   "source": [
    "## First, data setup\n",
    "\n",
    "In data pre-processing (`generate_features_only.py`) TF records files are created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/Users/jillnaiman/MegaYolo/binaries_model8_tfrecordz/train_3.tfrecords',\n",
       " '/Users/jillnaiman/MegaYolo/binaries_model8_tfrecordz/train_8.tfrecords',\n",
       " '/Users/jillnaiman/MegaYolo/binaries_model8_tfrecordz/train_5.tfrecords',\n",
       " '/Users/jillnaiman/MegaYolo/binaries_model8_tfrecordz/train_2.tfrecords',\n",
       " '/Users/jillnaiman/MegaYolo/binaries_model8_tfrecordz/train_9.tfrecords']"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_list = glob.glob(classDir_main_to_imgs + 'train_*tfrecords')\n",
    "valid_list = glob.glob(classDir_main_to_imgs + 'valid_*tfrecords')\n",
    "#test_list = glob.glob(classDir_main_to_imgs + 'test_*tfrecords')\n",
    "train_list[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get raw data\n",
    "#train_raw_data = tf.data.TFRecordDataset(train_list)\n",
    "#valid_raw_data = tf.data.TFRecordDataset(valid_list)\n",
    "\n",
    "# for compressed\n",
    "train_raw_data = tf.data.TFRecordDataset(filenames=train_list, \n",
    "                                         compression_type='GZIP', \n",
    "                                         buffer_size=None, \n",
    "                                        num_parallel_reads=tf.data.AUTOTUNE)\n",
    "valid_raw_data = tf.data.TFRecordDataset(filenames=valid_list, \n",
    "                                         compression_type='GZIP', \n",
    "                                         buffer_size=None,\n",
    "                                        num_parallel_reads=tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TakeDataset shapes: (), types: tf.string>"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_raw_data.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary describing the features.\n",
    "image_feature_description = {\n",
    "    'nbox': tf.io.FixedLenFeature([], tf.float32),\n",
    "    'nfeatures': tf.io.FixedLenFeature([], tf.float32),\n",
    "    'boxes': tf.io.FixedLenFeature([], tf.string),\n",
    "    'image_raw': tf.io.FixedLenFeature([], tf.string),\n",
    "    'image_name': tf.io.FixedLenFeature([], tf.string),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we don't have access to anchors file and train/test/valid files -- read from splits.  Either way, get the labels from a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABELS = np.loadtxt(classDir_main_to_imgs + 'LABELS.csv', \n",
    "                    dtype=str, delimiter=',')\n",
    "CLASS = len(LABELS)\n",
    "labels = np.arange(0,len(LABELS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if we have anchors already\n",
    "def _parse_just_boxes(example_proto):\n",
    "    image_features = tf.io.parse_single_example(example_proto, image_feature_description)\n",
    "    # parse the data\n",
    "    nboxes = image_features['nbox']\n",
    "    nfeatures = image_features['nfeatures']\n",
    "    boxes = tf.io.decode_raw(image_features['boxes'],tf.float32)\n",
    "    boxes = tf.reshape(boxes,[nboxes,5])  \n",
    "    return boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "if re_run_from_splits: regenAnchors = False\n",
    "\n",
    "if regenAnchors:\n",
    "#if True:\n",
    "    boxes = train_raw_data.map(lambda example_proto:_parse_just_boxes(example_proto))\n",
    "    saved_boxes = []\n",
    "    for ib,b in enumerate(boxes):\n",
    "        if ib%500 == 0: print('on', ib, 'of ? (probably 5000ish for full)')\n",
    "        saved_boxes.append(b.numpy())\n",
    "    # valid\n",
    "    boxes = valid_raw_data.map(lambda example_proto:_parse_just_boxes(example_proto))\n",
    "    for ib,b in enumerate(boxes):\n",
    "        if ib%500 == 0: print('on', ib, 'of ? (probably 5000*0.15ish for valid)')\n",
    "        saved_boxes.append(b.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from saved:\n",
      "[[203. 118.]\n",
      " [ 19. 355.]\n",
      " [377. 373.]\n",
      " [ 24.   4.]\n",
      " [182.   9.]\n",
      " [199.  22.]\n",
      " [  6.  45.]\n",
      " [434.  16.]\n",
      " [334. 216.]]\n"
     ]
    }
   ],
   "source": [
    "# assume location of saved anchors:\n",
    "saveFileAnchors = classDirMain + 'weights/anchors.pickle'\n",
    "# hack for local debugging\n",
    "if '/Users/jillnaiman' in thisDir:\n",
    "    saveFileAnchors = splitsDir + 'anchors.pickle'\n",
    "\n",
    "if regenAnchors:\n",
    "#if True:\n",
    "    boxes = []\n",
    "    for bb in saved_boxes:\n",
    "        if len(bb) > 0:\n",
    "            for b in bb:\n",
    "                boxes.append([b[2]-b[0], b[3]-b[1]])\n",
    "    boxes = np.array(boxes)\n",
    "    \n",
    "    anchors = generator(boxes,k=num_cluster)\n",
    "    print('NEW ANCHORS:')\n",
    "    \n",
    "    # save!\n",
    "    with open(saveFileAnchors, 'wb') as ff:\n",
    "        pickle.dump(anchors, ff)\n",
    "else:\n",
    "    print('from saved:')\n",
    "    with open(saveFileAnchors, 'rb') as f:\n",
    "        anchors = pickle.load(f)    \n",
    "    \n",
    "print(anchors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at what is in each TFRecord file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if we have anchors already\n",
    "def _parse_to_view(example_proto,anchors,CLASS):\n",
    "    image_features = tf.io.parse_single_example(example_proto, image_feature_description)\n",
    "    # parse the data\n",
    "    nboxes = image_features['nbox']\n",
    "    nfeatures = image_features['nfeatures']\n",
    "    boxes = tf.io.decode_raw(image_features['boxes'],tf.float32)\n",
    "    boxes = tf.reshape(boxes,[nboxes,5])\n",
    "    images_raw = image_features['image_raw']\n",
    "    image = tf.io.decode_raw(images_raw,tf.float32)\n",
    "    image = tf.reshape(image,[config.IMAGE_H,config.IMAGE_W,nfeatures]) \n",
    "    return image,nboxes,nfeatures,boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "view_dataset = train_raw_data.map(lambda example_proto:_parse_to_view(example_proto,\n",
    "                                                                      anchors,CLASS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nboxes= tf.Tensor(2.0, shape=(), dtype=float32)\n",
      "nfeatures= tf.Tensor(12.0, shape=(), dtype=float32)\n",
      "boxes= tf.Tensor(\n",
      "[[197. 413. 308. 418.   2.]\n",
      " [ 76.  51. 426. 407.   1.]], shape=(2, 5), dtype=float32)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# check one out\n",
    "for image,nboxes,nfeatures,boxes in view_dataset.take(1):\n",
    "    imgPlot = image.numpy()\n",
    "    plt.imshow(imgPlot[:,:,0],cmap='gray')\n",
    "    print('nboxes=',nboxes)\n",
    "    print('nfeatures=',nfeatures)\n",
    "    print('boxes=',boxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "# debug\n",
    "import mega_yolo_utils\n",
    "from importlib import reload\n",
    "reload(mega_yolo_utils)\n",
    "from mega_yolo_utils import process_box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if we have anchors already\n",
    "def _parse_image_function(example_proto,anchors,CLASS):\n",
    "    image_features = tf.io.parse_single_example(example_proto, image_feature_description)\n",
    "    # parse the data\n",
    "    nboxes = image_features['nbox']\n",
    "    nfeatures = image_features['nfeatures']\n",
    "    boxes = tf.io.decode_raw(image_features['boxes'],tf.float32)\n",
    "    boxes = tf.reshape(boxes,[nboxes,5])\n",
    "    images_raw = image_features['image_raw']\n",
    "    image = tf.io.decode_raw(images_raw,tf.float32)\n",
    "    image = tf.reshape(image,[config.IMAGE_H,config.IMAGE_W,nfeatures])\n",
    "    # process boxes -- wrap in a tf.py_function\n",
    "    y1,y2,y3 = tf.py_function(process_box,\n",
    "                              (boxes[:,:4], boxes[:,4],anchors,CLASS),\n",
    "                              (tf.float32,tf.float32,tf.float32))   \n",
    "    return image, y1,y2,y3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = train_raw_data.map(lambda example_proto:_parse_image_function(example_proto,\n",
    "                                                                              anchors,CLASS))\n",
    "valid_dataset = valid_raw_data.map(lambda example_proto:_parse_image_function(example_proto,\n",
    "                                                                              anchors,CLASS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ***** ADD TRAINING AUGMENTATION HERE JUST FOR TRAINING SET ********"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = train_dataset.shuffle(buffer_size)\n",
    "train_dataset = train_dataset.repeat()\n",
    "train_dataset = train_dataset.batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_dataset = valid_dataset.shuffle(buffer_size)\n",
    "valid_dataset = valid_dataset.repeat()\n",
    "valid_dataset = valid_dataset.batch(batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Invalid shape (10, 512, 12) for image data",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/t5/9xgccmv92hnfvjwd62mk8zqh0000gn/T/ipykernel_49094/109680278.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;31m# image = tf.reshape(image,[config.IMAGE_H,config.IMAGE_W,nfeatures])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mimgPlot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimgPlot\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcmap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'gray'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0;31m#print(y1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/Paper1/lib/python3.7/site-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36mimshow\u001b[0;34m(X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, filternorm, filterrad, resample, url, data, **kwargs)\u001b[0m\n\u001b[1;32m   2907\u001b[0m         \u001b[0mfilternorm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfilternorm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilterrad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfilterrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresample\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresample\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2908\u001b[0m         \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"data\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m}\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2909\u001b[0;31m         **kwargs)\n\u001b[0m\u001b[1;32m   2910\u001b[0m     \u001b[0msci\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__ret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2911\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m__ret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/Paper1/lib/python3.7/site-packages/matplotlib/__init__.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(ax, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1359\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1360\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1361\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msanitize_sequence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1362\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1363\u001b[0m         \u001b[0mbound\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_sig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/Paper1/lib/python3.7/site-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36mimshow\u001b[0;34m(self, X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, filternorm, filterrad, resample, url, **kwargs)\u001b[0m\n\u001b[1;32m   5607\u001b[0m                               resample=resample, **kwargs)\n\u001b[1;32m   5608\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5609\u001b[0;31m         \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5610\u001b[0m         \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_alpha\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5611\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_clip_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/Paper1/lib/python3.7/site-packages/matplotlib/image.py\u001b[0m in \u001b[0;36mset_data\u001b[0;34m(self, A)\u001b[0m\n\u001b[1;32m    708\u001b[0m                 or self._A.ndim == 3 and self._A.shape[-1] in [3, 4]):\n\u001b[1;32m    709\u001b[0m             raise TypeError(\"Invalid shape {} for image data\"\n\u001b[0;32m--> 710\u001b[0;31m                             .format(self._A.shape))\n\u001b[0m\u001b[1;32m    711\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    712\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_A\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Invalid shape (10, 512, 12) for image data"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# check one out\n",
    "for image,y1,y2,y3 in train_dataset.take(1):\n",
    "    #nboxes = image_features['nbox']\n",
    "    #nfeatures = image_features['nfeatures']\n",
    "    #boxes = tf.io.decode_raw(image_features['boxes'],tf.float32)\n",
    "    #boxes = tf.reshape(boxes,[nboxes,5])\n",
    "    # images_raw = image_features['image_raw']\n",
    "    # image = tf.io.decode_raw(images_raw,tf.float32)\n",
    "    # image = tf.reshape(image,[config.IMAGE_H,config.IMAGE_W,nfeatures])\n",
    "    imgPlot = image.numpy()\n",
    "    plt.imshow(imgPlot[0,:,0],cmap='gray')\n",
    "    #print(y1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "#***HERE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 54410,
     "status": "ok",
     "timestamp": 1638453069726,
     "user": {
      "displayName": "Jill Naiman",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "13328008118965652761"
     },
     "user_tz": 360
    },
    "id": "EoCivx-Jneav",
    "outputId": "62107197-3fe1-4018-da1b-0020ae21c597"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5515"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Grab annotations to calculate steps\n",
    "annotations = glob.glob(classDir_main_to + '*')\n",
    "len(annotations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tFWzDaooBb6A"
   },
   "source": [
    "Parse annotations -- **NOTE: this can take a while!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1638453069727,
     "user": {
      "displayName": "Jill Naiman",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "13328008118965652761"
     },
     "user_tz": 360
    },
    "id": "jRvZyeZiGrFe"
   },
   "outputs": [],
   "source": [
    "#tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2506111,
     "status": "ok",
     "timestamp": 1638455575835,
     "user": {
      "displayName": "Jill Naiman",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "13328008118965652761"
     },
     "user_tz": 360
    },
    "id": "N34O8BwTneay",
    "outputId": "34b01934-f30d-4986-d455-0a1695687c1d"
   },
   "outputs": [],
   "source": [
    "# ##### what about anchors -- do we want to regenerate? Generally keep this as True...\n",
    "# #####regenAnchors = True\n",
    "# # ... unless we are re-running from a previous split\n",
    "# if re_run_from_splits: regenAnchors = False\n",
    "# #if regenAnchorsAnyway: regenAnchors = True\n",
    "\n",
    "# if regenAnchors:\n",
    "#     bboxes = []\n",
    "    \n",
    "\n",
    "# # this parsing does some loading on collab that I'm not 100% sure about, but seems necessary\n",
    "# #   to load into memory, so keep it and figure it out later\n",
    "# def load_parse_data_split(X_full):\n",
    "#     Y_full_str = np.array([]) # have to loop and give best guesses for the pages that have multiple images/classes in them\n",
    "#     slabels = np.array([])\n",
    "#     for ii, X in enumerate(X_full):\n",
    "#         if ii%200 == 0: print('on ', ii, ' of ', len(X_full))\n",
    "#         tree = ET.parse(X)\n",
    "#         tags = np.array([])\n",
    "#         for elem in tree.iter(): \n",
    "#             if 'object' in elem.tag or 'part' in elem.tag:                  \n",
    "#                 box = np.zeros((5))\n",
    "#                 for attr in list(elem):\n",
    "#                     if 'name' in attr.tag:\n",
    "#                         if attr.text is not None:\n",
    "#                             tags = np.append(tags,attr.text)\n",
    "#                             slabels = np.append(slabels,attr.text)\n",
    "#                         #print(tags, slabels)\n",
    "#                     if regenAnchors:\n",
    "#                         if 'bndbox' in attr.tag and 'bndboxOrig' not in attr.tag:\n",
    "#                             for dim in list(attr):\n",
    "#                                 if 'xmin' in dim.tag:\n",
    "#                                     box[0] = int(round(float(dim.text)))\n",
    "#                                 if 'ymin' in dim.tag:\n",
    "#                                     box[1] = int(round(float(dim.text)))\n",
    "#                                 if 'xmax' in dim.tag:\n",
    "#                                     box[2] = int(round(float(dim.text)))\n",
    "#                                 if 'ymax' in dim.tag:\n",
    "#                                     box[3] = int(round(float(dim.text)))\n",
    "#                 if regenAnchors and len(box)>0: bboxes.append(np.asarray(box))\n",
    "#         if len(tags) > 0:\n",
    "#             #print(tags)\n",
    "#             modeClass = stats.mode(tags).mode[0] # most frequent class that pops up on this page\n",
    "#             Y_full_str = np.append(Y_full_str, modeClass) # class in string\n",
    "#         else:\n",
    "#             Y_full_str = np.append(Y_full_str, 'none')\n",
    "#     return Y_full_str,slabels\n",
    "\n",
    "# # NEXT: do a quick test run-through of the data generator for train/test splits\n",
    "# X_full = np.array(annotations)\n",
    "\n",
    "# start_time = perf_counter( )\n",
    "# Y_full_str,slabels = load_parse_data_split(X_full)\n",
    "# stop_time = perf_counter( )\n",
    "# print('    Elapsed wall clock time = %g seconds.' % (stop_time - start_time) )\n",
    "    \n",
    "# # also do regeneration of anchors\n",
    "# if regenAnchors: bboxes = np.array(bboxes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yUWutM847jyy"
   },
   "source": [
    "Get anchors, if that is what you wanna do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 19,
     "status": "ok",
     "timestamp": 1638455575835,
     "user": {
      "displayName": "Jill Naiman",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "13328008118965652761"
     },
     "user_tz": 360
    },
    "id": "pCrtynsPIMXP",
    "outputId": "5507476c-e555-43d9-979c-52beb713773f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/jillnaiman/MegaYolo/'"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classDirMain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1048,
     "status": "ok",
     "timestamp": 1638455576866,
     "user": {
      "displayName": "Jill Naiman",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "13328008118965652761"
     },
     "user_tz": 360
    },
    "id": "Dvtr6msG7jyy",
    "outputId": "06cc7a7e-7d7a-4f16-8d25-604736542e62"
   },
   "outputs": [],
   "source": [
    "# # assume location of saved anchors:\n",
    "# saveFileAnchors = classDirMain + 'weights/anchors.pickle'\n",
    "# # hack for local debugging\n",
    "# if '/Users/jillnaiman' in thisDir:\n",
    "#     saveFileAnchors = splitsDir + 'anchors.pickle'\n",
    "\n",
    "# if regenAnchors:\n",
    "#     boxes = []\n",
    "#     for b in bboxes:\n",
    "#         boxes.append([b[2]-b[0], b[3]-b[1]])\n",
    "#     boxes = np.array(boxes)\n",
    "    \n",
    "#     anchors = generator(boxes,k=num_cluster)\n",
    "#     print('NEW ANCHORS:')\n",
    "    \n",
    "#     # save!\n",
    "#     with open(saveFileAnchors, 'wb') as ff:\n",
    "#         pickle.dump(anchors, ff)\n",
    "# else:\n",
    "#     print('from saved:')\n",
    "#     with open(saveFileAnchors, 'rb') as f:\n",
    "#         anchors = pickle.load(f)    \n",
    "    \n",
    "# print(anchors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1638455576867,
     "user": {
      "displayName": "Jill Naiman",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "13328008118965652761"
     },
     "user_tz": 360
    },
    "id": "Z1G0r3vYnea0",
    "outputId": "c3d63b7f-1c43-4cb8-89b9-53b4dfefd38d"
   },
   "outputs": [],
   "source": [
    "# LABELS = np.unique(slabels).tolist()\n",
    "# CLASS = len(LABELS)\n",
    "# ##if use_only_one_class: CLASS = 1\n",
    "# LABELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1638455576867,
     "user": {
      "displayName": "Jill Naiman",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "13328008118965652761"
     },
     "user_tz": 360
    },
    "id": "yZJXzsfVnea2"
   },
   "outputs": [],
   "source": [
    "# # strings to integers\n",
    "\n",
    "# if not re_run_from_splits:\n",
    "#     Y_full = []\n",
    "#     labels = np.arange(len(LABELS))\n",
    "\n",
    "#     for i in range(len(Y_full_str)):\n",
    "#         try:\n",
    "#             Y_full.append( labels[np.array(LABELS) == Y_full_str[i]][0] +1 ) # 0 means unlabeled data\n",
    "#         except:\n",
    "#             Y_full.append( 0 ) # 0 means unlabeled data\n",
    "\n",
    "#         if len(labels[np.array(LABELS) == Y_full_str[i]]) > 1:\n",
    "#             import sys\n",
    "#             sys.exit()\n",
    "\n",
    "#     Y_full = np.array(Y_full)\n",
    "# else:\n",
    "#     labels = np.arange(0,len(LABELS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1638455576867,
     "user": {
      "displayName": "Jill Naiman",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "13328008118965652761"
     },
     "user_tz": 360
    },
    "id": "0R42fLapLPI-",
    "outputId": "ece84189-cd6c-49dc-84cf-51d1d11721fd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1, 2, 3]),\n",
       " array(['figure', 'figure caption', 'math formula', 'table'], dtype='<U14'))"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels, LABELS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6wKLOv6xBb6D"
   },
   "source": [
    "Create splits one way or the other.  Using the function makes sure the classes are evenly split as there can be un-even classes (for example, there might be way more figures/figure captions than tables).  Note: each instance is tagged as having one class but this is just the most frequent type on that page -- pages can sometimes have multiple types.\n",
    "\n",
    "Or, load from previous file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 438
    },
    "executionInfo": {
     "elapsed": 6176,
     "status": "ok",
     "timestamp": 1638455583040,
     "user": {
      "displayName": "Jill Naiman",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "13328008118965652761"
     },
     "user_tz": 360
    },
    "id": "nIG98zeHnea6",
    "outputId": "6613f570-864f-4339-e669-9ffde7ba26b3"
   },
   "outputs": [],
   "source": [
    "# # splits\n",
    "# train_per = 0.75\n",
    "# valid_per = 0.15\n",
    "# test_per = 0.10\n",
    "# if not re_run_from_splits:\n",
    "#     # note: y_* aren't actually used anywhere\n",
    "#     X_train, y_train, X_valid, y_valid, X_test, y_test = train_test_valid_split(X_full, Y_full, \n",
    "#                                                                   train_size = train_per, valid_size = valid_per, test_size = test_per, \n",
    "#                                                                   textClassification=True, asInts=False)\n",
    "\n",
    "#     print('We have AT LEAST', len(X_train), 'training,', len(X_valid), 'validation,', len(X_test), 'test instances.')\n",
    "    \n",
    "#     # write files for splits\n",
    "#     np.savetxt(splitsDir + 'train.csv', X_train, fmt='%s', delimiter=',')\n",
    "#     np.savetxt(splitsDir + 'test.csv', X_test, fmt='%s', delimiter=',')\n",
    "#     np.savetxt(splitsDir + 'valid.csv', X_valid, fmt='%s', delimiter=',')\n",
    "# else: # read from files\n",
    "#     X_train = np.loadtxt(splitsDir + 'train.csv', dtype=str, delimiter=',')\n",
    "#     X_test = np.loadtxt(splitsDir + 'test.csv', dtype=str, delimiter=',')\n",
    "#     X_valid = np.loadtxt(splitsDir + 'valid.csv', dtype=str, delimiter=',')\n",
    "#     if '/Users/jillnaiman' in thisDir: # probably should update if we have copied from google to local\n",
    "#         Xtmp = []\n",
    "#         for i in range(len(X_train)):\n",
    "#             Xtmp.append(classDir_main_to+X_train[i].split('/')[-1])\n",
    "#         X_train = Xtmp\n",
    "#         Xtmp = []\n",
    "#         for i in range(len(X_valid)):\n",
    "#             Xtmp.append(classDir_main_to+X_valid[i].split('/')[-1])\n",
    "#         X_valid = Xtmp\n",
    "#         Xtmp = []\n",
    "#         for i in range(len(X_test)):\n",
    "#             Xtmp.append(classDir_main_to+X_test[i].split('/')[-1])\n",
    "#         X_test = Xtmp\n",
    "        \n",
    "#     # if rerun, do it!\n",
    "#     logFiles = glob.glob(logsDir+'logs/training_1'+extraName+'/events*')\n",
    "#     logFiles.sort()\n",
    "#     #nRecent = 7\n",
    "#     logFile = logFiles[-nRecent:] # most N recent\n",
    "#     loss = []; val_loss = []; step = []\n",
    "#     for l in logFile:\n",
    "#         for summary in summary_iterator(l):\n",
    "#             #print(summary)\n",
    "#             if summary.step > 0: # only after start\n",
    "#                 ent = summary.summary.value[0]\n",
    "#                 if ent.tag == 'loss':\n",
    "#                     loss.append(struct.unpack('f', ent.tensor.tensor_content)[0])\n",
    "#                 elif ent.tag == 'val_loss':\n",
    "#                     val_loss.append(struct.unpack('f', ent.tensor.tensor_content)[0])\n",
    "#                 else:\n",
    "#                     print('not sure:', ent)\n",
    "#                 step.append(summary.step)\n",
    "#     step = np.unique(step).tolist()\n",
    "\n",
    "#     # plot\n",
    "#     stop = min([len(step),len(loss)])\n",
    "#     if len(step[:stop]) > 0: # only plot if we have something\n",
    "#         fig,ax = plt.subplots(figsize=(10,5))\n",
    "#         ax.plot(step[:stop],loss[:stop],label='Training Loss')\n",
    "#         stop = min([len(step),len(val_loss)])\n",
    "#         ax.plot(step[:stop],val_loss[:stop], label='Validation Loss')\n",
    "#         ax.set_xlabel('Epoch number')\n",
    "#         ax.set_title('Loss for prior run')\n",
    "#         ax.legend()\n",
    "#         plt.show()\n",
    "    \n",
    "#     # also, check for new annotations, add to new lists\n",
    "#     X_all = X_train.copy()\n",
    "#     if type(X_all) is not list: X_all = X_all.tolist()\n",
    "#     X_all.extend(X_valid.copy())\n",
    "#     X_all.extend(X_test.copy())\n",
    "#     if len(X_train)+len(X_valid)+len(X_test) < len(annotations): # we have new annotations from last run!\n",
    "#         # pick out new annotations\n",
    "#         X_new = []; Y_new_str = []\n",
    "#         for a,yfs in zip(annotations,Y_full_str):\n",
    "#             if a not in X_all:\n",
    "#                 X_new.append(a); Y_new_str.append(yfs)\n",
    "\n",
    "#         Y_new = []\n",
    "#         labels = np.arange(len(LABELS))\n",
    "\n",
    "#         for i in range(len(Y_new_str)):\n",
    "#             try:\n",
    "#                 Y_new.append( labels[np.array(LABELS) == Y_new_str[i]][0] +1 ) # 0 means unlabeled data\n",
    "#             except:\n",
    "#                 Y_new.append( 0 ) # 0 means unlabeled data\n",
    "\n",
    "#             if len(labels[np.array(LABELS) == Y_new_str[i]]) > 1:\n",
    "#                 import sys\n",
    "#                 sys.exit()\n",
    "\n",
    "#         Y_new = np.array(Y_new)\n",
    "#         X_new = np.array(X_new)\n",
    "\n",
    "#         # resplit\n",
    "#         # note: y_* aren't actually used anywhere\n",
    "#         X_train_new, y_train_new, X_valid_new, \\\n",
    "#         y_valid_new, X_test_new, y_test_new = train_test_valid_split(X_new, Y_new, \n",
    "#                                                                     train_size = train_per, \n",
    "#                                                                       valid_size = valid_per, test_size = test_per, \n",
    "#                                                                     textClassification=True, asInts=False)\n",
    "\n",
    "#         print('We have', len(X_train_new), 'training,', len(X_valid_new), 'validation,', len(X_test_new), 'test NEW instances.')\n",
    "#         # add\n",
    "#         if type(X_train) is not list: X_train = X_train.tolist()\n",
    "#         if type(X_valid) is not list: X_valid = X_valid.tolist()\n",
    "#         if type(X_test) is not list: X_test = X_test.tolist()\n",
    "#         X_train.extend(X_train_new)\n",
    "#         X_valid.extend(X_valid_new)\n",
    "#         X_test.extend(X_test_new)\n",
    "#         print('We have', len(X_train), 'training,', len(X_valid), 'validation,', len(X_test), 'test TOTAL instances.')\n",
    "#         # move old\n",
    "#         shutil.copyfile(splitsDir + 'train.csv', splitsDir + 'train_old.csv')\n",
    "#         shutil.copyfile(splitsDir + 'valid.csv', splitsDir + 'valid_old.csv')\n",
    "#         shutil.copyfile(splitsDir + 'test.csv', splitsDir + 'test_old.csv')\n",
    "#         # write files for \"filled in\" splits\n",
    "#         np.savetxt(splitsDir + 'train.csv', X_train, fmt='%s', delimiter=',')\n",
    "#         np.savetxt(splitsDir + 'test.csv', X_test, fmt='%s', delimiter=',')\n",
    "#         np.savetxt(splitsDir + 'valid.csv', X_valid, fmt='%s', delimiter=',')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RRyluqn6vvaW"
   },
   "source": [
    "Steps per epoch -- training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'config' from '/Users/jillnaiman/figure_and_caption_extraction/config.py'>"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# debug\n",
    "from importlib import reload\n",
    "reload(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "# guestimate based on splits\n",
    "len_train = len(annotations)*config.train_per\n",
    "len_valid = len(annotations)*config.valid_per"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 19,
     "status": "ok",
     "timestamp": 1638455583041,
     "user": {
      "displayName": "Jill Naiman",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "13328008118965652761"
     },
     "user_tz": 360
    },
    "id": "SdEYPW-tvvaW",
    "outputId": "a3200ccd-6da9-4ef2-9607-6f126abf1459"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps per epoch = training: 413 , validation: 82\n"
     ]
    }
   ],
   "source": [
    "#steps = len(X_train) // batch_size\n",
    "#print(len(X_train)//batch_size)\n",
    "# can do larger with augmentation\n",
    "\n",
    "#aug_fac = 2 # 2 or 3\n",
    "\n",
    "aug_fac = 1 # set to 1 if no data aug\n",
    "\n",
    "steps_training = int((len_train//batch_size)*aug_fac)\n",
    "# factor of 2 from: https://stackoverflow.com/questions/49922252/choosing-number-of-steps-per-epoch\n",
    "\n",
    "steps_val = int((len_valid//batch_size)*aug_fac)\n",
    "\n",
    "print('Steps per epoch = training:', steps_training, ', validation:', steps_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sExlEUTwIYBy"
   },
   "source": [
    "Save also the names of the test instances to use later:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 280,
     "status": "ok",
     "timestamp": 1638455583308,
     "user": {
      "displayName": "Jill Naiman",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "13328008118965652761"
     },
     "user_tz": 360
    },
    "id": "y4LLclWnIQ49",
    "outputId": "acc77e47-d7fa-475c-9c50-901e91374442"
   },
   "outputs": [],
   "source": [
    "# save test list in an extra place... this is a bit redundant since its saved another place too\n",
    "#if not re_run_from_splits:\n",
    "# np.savetxt(saveFile, X_test, delimiter=',',fmt='%s')\n",
    "# print('Hey, saved tests!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7gQi6kS8nea8"
   },
   "source": [
    "# 1. Define YOLO model\n",
    "\n",
    "For v5, see: https://github.com/jahongir7174/YOLOv5-tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zoBEY7n67jy1"
   },
   "source": [
    "For creating the model -- how many features are we using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if we have anchors already\n",
    "def _parse_nfeatures(example_proto):\n",
    "    image_features = tf.io.parse_single_example(example_proto, image_feature_description)\n",
    "    # parse the data\n",
    "    nfeatures = image_features['nfeatures']\n",
    "    return nfeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 5), dtype=float32, numpy=\n",
       "array([[197., 413., 308., 418.,   2.],\n",
       "       [ 76.,  51., 426., 407.,   1.]], dtype=float32)>"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nfeatures_data = train_raw_data.map(lambda example_proto:_parse_just_boxes(example_proto))\n",
    "nfeatures = -1\n",
    "for f in nfeatures_data.take(1):\n",
    "    nfeatures = f\n",
    "nfeatures\n",
    "\n",
    "\n",
    "# if regenAnchors:\n",
    "# #if True:\n",
    "#     boxes = train_raw_data.map(lambda example_proto:_parse_just_boxes(example_proto))\n",
    "#     saved_boxes = []\n",
    "#     for ib,b in enumerate(boxes):\n",
    "#         if ib%500 == 0: print('on', ib, 'of ? (probably 5000ish for full)')\n",
    "#         saved_boxes.append(b.numpy())\n",
    "#     # valid\n",
    "#     boxes = valid_raw_data.map(lambda example_proto:_parse_just_boxes(example_proto))\n",
    "#     for ib,b in enumerate(boxes):\n",
    "#         if ib%500 == 0: print('on', ib, 'of ? (probably 5000*0.15ish for valid)')\n",
    "#         saved_boxes.append(b.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<MapDataset shapes: (None, 5), types: tf.float32>"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nfeatures_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 43600,
     "status": "ok",
     "timestamp": 1638455626905,
     "user": {
      "displayName": "Jill Naiman",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "13328008118965652761"
     },
     "user_tz": 360
    },
    "id": "ymfe7jyL7jy1",
    "outputId": "e90adedb-f1d4-4033-fe4b-ce65beafe6c3"
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot load file containing pickled data when allow_pickle=False",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/t5/9xgccmv92hnfvjwd62mk8zqh0000gn/T/ipykernel_49094/3335330532.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mn_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_n_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassDir_main_to_imgs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mn_features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/figure_and_caption_extraction/mega_yolo_utils.py\u001b[0m in \u001b[0;36mget_n_features\u001b[0;34m(classDir_main_to_imgs)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m         \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'arr_0'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m     \u001b[0mn_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/Paper1/lib/python3.7/site-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[1;32m    442\u001b[0m             \u001b[0;31m# Try a pickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    443\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mallow_pickle\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 444\u001b[0;31m                 raise ValueError(\"Cannot load file containing pickled data \"\n\u001b[0m\u001b[1;32m    445\u001b[0m                                  \"when allow_pickle=False\")\n\u001b[1;32m    446\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot load file containing pickled data when allow_pickle=False"
     ]
    }
   ],
   "source": [
    "n_features = get_n_features(classDir_main_to_imgs)\n",
    "n_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "executionInfo": {
     "elapsed": 4704,
     "status": "ok",
     "timestamp": 1638455631604,
     "user": {
      "displayName": "Jill Naiman",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "13328008118965652761"
     },
     "user_tz": 360
    },
    "id": "KRQsM_X7XC-V"
   },
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "version = 'l' # large version\n",
    "model = build_model(n_features, anchors, version, len(LABELS))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yXdkSQK9Emky"
   },
   "source": [
    "Build YOLOv5:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 29,
     "status": "ok",
     "timestamp": 1638455631605,
     "user": {
      "displayName": "Jill Naiman",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "13328008118965652761"
     },
     "user_tz": 360
    },
    "id": "0Vnf5mjn7jy2",
    "outputId": "67c42020-eff8-492d-be13-1c5f9786528b",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 512, 512, 1  0           []                               \n",
      "                                2)]                                                               \n",
      "                                                                                                  \n",
      " tf.nn.space_to_depth (TFOpLamb  (None, 256, 256, 48  0          ['input_1[0][0]']                \n",
      " da)                            )                                                                 \n",
      "                                                                                                  \n",
      " conv2d (Conv2D)                (None, 256, 256, 64  27648       ['tf.nn.space_to_depth[0][0]']   \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " batch_normalization (BatchNorm  (None, 256, 256, 64  256        ['conv2d[0][0]']                 \n",
      " alization)                     )                                                                 \n",
      "                                                                                                  \n",
      " activation (Activation)        (None, 256, 256, 64  0           ['batch_normalization[0][0]']    \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " zero_padding2d (ZeroPadding2D)  (None, 257, 257, 64  0          ['activation[0][0]']             \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_1 (Conv2D)              (None, 128, 128, 12  73728       ['zero_padding2d[0][0]']         \n",
      "                                8)                                                                \n",
      "                                                                                                  \n",
      " batch_normalization_1 (BatchNo  (None, 128, 128, 12  512        ['conv2d_1[0][0]']               \n",
      " rmalization)                   8)                                                                \n",
      "                                                                                                  \n",
      " activation_1 (Activation)      (None, 128, 128, 12  0           ['batch_normalization_1[0][0]']  \n",
      "                                8)                                                                \n",
      "                                                                                                  \n",
      " conv2d_2 (Conv2D)              (None, 128, 128, 64  8192        ['activation_1[0][0]']           \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " batch_normalization_2 (BatchNo  (None, 128, 128, 64  256        ['conv2d_2[0][0]']               \n",
      " rmalization)                   )                                                                 \n",
      "                                                                                                  \n",
      " activation_2 (Activation)      (None, 128, 128, 64  0           ['batch_normalization_2[0][0]']  \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_3 (Conv2D)              (None, 128, 128, 64  4096        ['activation_2[0][0]']           \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " batch_normalization_3 (BatchNo  (None, 128, 128, 64  256        ['conv2d_3[0][0]']               \n",
      " rmalization)                   )                                                                 \n",
      "                                                                                                  \n",
      " activation_3 (Activation)      (None, 128, 128, 64  0           ['batch_normalization_3[0][0]']  \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_4 (Conv2D)              (None, 128, 128, 64  36864       ['activation_3[0][0]']           \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " batch_normalization_4 (BatchNo  (None, 128, 128, 64  256        ['conv2d_4[0][0]']               \n",
      " rmalization)                   )                                                                 \n",
      "                                                                                                  \n",
      " activation_4 (Activation)      (None, 128, 128, 64  0           ['batch_normalization_4[0][0]']  \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " tf.__operators__.add (TFOpLamb  (None, 128, 128, 64  0          ['activation_2[0][0]',           \n",
      " da)                            )                                 'activation_4[0][0]']           \n",
      "                                                                                                  \n",
      " conv2d_5 (Conv2D)              (None, 128, 128, 64  4096        ['tf.__operators__.add[0][0]']   \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " batch_normalization_5 (BatchNo  (None, 128, 128, 64  256        ['conv2d_5[0][0]']               \n",
      " rmalization)                   )                                                                 \n",
      "                                                                                                  \n",
      " activation_5 (Activation)      (None, 128, 128, 64  0           ['batch_normalization_5[0][0]']  \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_6 (Conv2D)              (None, 128, 128, 64  36864       ['activation_5[0][0]']           \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " batch_normalization_6 (BatchNo  (None, 128, 128, 64  256        ['conv2d_6[0][0]']               \n",
      " rmalization)                   )                                                                 \n",
      "                                                                                                  \n",
      " activation_6 (Activation)      (None, 128, 128, 64  0           ['batch_normalization_6[0][0]']  \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " tf.__operators__.add_1 (TFOpLa  (None, 128, 128, 64  0          ['tf.__operators__.add[0][0]',   \n",
      " mbda)                          )                                 'activation_6[0][0]']           \n",
      "                                                                                                  \n",
      " conv2d_7 (Conv2D)              (None, 128, 128, 64  4096        ['tf.__operators__.add_1[0][0]'] \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " batch_normalization_7 (BatchNo  (None, 128, 128, 64  256        ['conv2d_7[0][0]']               \n",
      " rmalization)                   )                                                                 \n",
      "                                                                                                  \n",
      " activation_7 (Activation)      (None, 128, 128, 64  0           ['batch_normalization_7[0][0]']  \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_8 (Conv2D)              (None, 128, 128, 64  36864       ['activation_7[0][0]']           \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_9 (Conv2D)              (None, 128, 128, 64  8192        ['activation_1[0][0]']           \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " batch_normalization_8 (BatchNo  (None, 128, 128, 64  256        ['conv2d_8[0][0]']               \n",
      " rmalization)                   )                                                                 \n",
      "                                                                                                  \n",
      " batch_normalization_9 (BatchNo  (None, 128, 128, 64  256        ['conv2d_9[0][0]']               \n",
      " rmalization)                   )                                                                 \n",
      "                                                                                                  \n",
      " activation_8 (Activation)      (None, 128, 128, 64  0           ['batch_normalization_8[0][0]']  \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " activation_9 (Activation)      (None, 128, 128, 64  0           ['batch_normalization_9[0][0]']  \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " tf.__operators__.add_2 (TFOpLa  (None, 128, 128, 64  0          ['tf.__operators__.add_1[0][0]', \n",
      " mbda)                          )                                 'activation_8[0][0]']           \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 128, 128, 12  0           ['activation_9[0][0]',           \n",
      "                                8)                                'tf.__operators__.add_2[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_10 (Conv2D)             (None, 128, 128, 12  16384       ['concatenate[0][0]']            \n",
      "                                8)                                                                \n",
      "                                                                                                  \n",
      " batch_normalization_10 (BatchN  (None, 128, 128, 12  512        ['conv2d_10[0][0]']              \n",
      " ormalization)                  8)                                                                \n",
      "                                                                                                  \n",
      " activation_10 (Activation)     (None, 128, 128, 12  0           ['batch_normalization_10[0][0]'] \n",
      "                                8)                                                                \n",
      "                                                                                                  \n",
      " zero_padding2d_1 (ZeroPadding2  (None, 129, 129, 12  0          ['activation_10[0][0]']          \n",
      " D)                             8)                                                                \n",
      "                                                                                                  \n",
      " conv2d_11 (Conv2D)             (None, 64, 64, 256)  294912      ['zero_padding2d_1[0][0]']       \n",
      "                                                                                                  \n",
      " batch_normalization_11 (BatchN  (None, 64, 64, 256)  1024       ['conv2d_11[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_11 (Activation)     (None, 64, 64, 256)  0           ['batch_normalization_11[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_12 (Conv2D)             (None, 64, 64, 128)  32768       ['activation_11[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_12 (BatchN  (None, 64, 64, 128)  512        ['conv2d_12[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_12 (Activation)     (None, 64, 64, 128)  0           ['batch_normalization_12[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_13 (Conv2D)             (None, 64, 64, 128)  16384       ['activation_12[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_13 (BatchN  (None, 64, 64, 128)  512        ['conv2d_13[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_13 (Activation)     (None, 64, 64, 128)  0           ['batch_normalization_13[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_14 (Conv2D)             (None, 64, 64, 128)  147456      ['activation_13[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_14 (BatchN  (None, 64, 64, 128)  512        ['conv2d_14[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_14 (Activation)     (None, 64, 64, 128)  0           ['batch_normalization_14[0][0]'] \n",
      "                                                                                                  \n",
      " tf.__operators__.add_3 (TFOpLa  (None, 64, 64, 128)  0          ['activation_12[0][0]',          \n",
      " mbda)                                                            'activation_14[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_15 (Conv2D)             (None, 64, 64, 128)  16384       ['tf.__operators__.add_3[0][0]'] \n",
      "                                                                                                  \n",
      " batch_normalization_15 (BatchN  (None, 64, 64, 128)  512        ['conv2d_15[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_15 (Activation)     (None, 64, 64, 128)  0           ['batch_normalization_15[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_16 (Conv2D)             (None, 64, 64, 128)  147456      ['activation_15[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_16 (BatchN  (None, 64, 64, 128)  512        ['conv2d_16[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_16 (Activation)     (None, 64, 64, 128)  0           ['batch_normalization_16[0][0]'] \n",
      "                                                                                                  \n",
      " tf.__operators__.add_4 (TFOpLa  (None, 64, 64, 128)  0          ['tf.__operators__.add_3[0][0]', \n",
      " mbda)                                                            'activation_16[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_17 (Conv2D)             (None, 64, 64, 128)  16384       ['tf.__operators__.add_4[0][0]'] \n",
      "                                                                                                  \n",
      " batch_normalization_17 (BatchN  (None, 64, 64, 128)  512        ['conv2d_17[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_17 (Activation)     (None, 64, 64, 128)  0           ['batch_normalization_17[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_18 (Conv2D)             (None, 64, 64, 128)  147456      ['activation_17[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_18 (BatchN  (None, 64, 64, 128)  512        ['conv2d_18[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_18 (Activation)     (None, 64, 64, 128)  0           ['batch_normalization_18[0][0]'] \n",
      "                                                                                                  \n",
      " tf.__operators__.add_5 (TFOpLa  (None, 64, 64, 128)  0          ['tf.__operators__.add_4[0][0]', \n",
      " mbda)                                                            'activation_18[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_19 (Conv2D)             (None, 64, 64, 128)  16384       ['tf.__operators__.add_5[0][0]'] \n",
      "                                                                                                  \n",
      " batch_normalization_19 (BatchN  (None, 64, 64, 128)  512        ['conv2d_19[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_19 (Activation)     (None, 64, 64, 128)  0           ['batch_normalization_19[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_20 (Conv2D)             (None, 64, 64, 128)  147456      ['activation_19[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_20 (BatchN  (None, 64, 64, 128)  512        ['conv2d_20[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_20 (Activation)     (None, 64, 64, 128)  0           ['batch_normalization_20[0][0]'] \n",
      "                                                                                                  \n",
      " tf.__operators__.add_6 (TFOpLa  (None, 64, 64, 128)  0          ['tf.__operators__.add_5[0][0]', \n",
      " mbda)                                                            'activation_20[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_21 (Conv2D)             (None, 64, 64, 128)  16384       ['tf.__operators__.add_6[0][0]'] \n",
      "                                                                                                  \n",
      " batch_normalization_21 (BatchN  (None, 64, 64, 128)  512        ['conv2d_21[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_21 (Activation)     (None, 64, 64, 128)  0           ['batch_normalization_21[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_22 (Conv2D)             (None, 64, 64, 128)  147456      ['activation_21[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_22 (BatchN  (None, 64, 64, 128)  512        ['conv2d_22[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_22 (Activation)     (None, 64, 64, 128)  0           ['batch_normalization_22[0][0]'] \n",
      "                                                                                                  \n",
      " tf.__operators__.add_7 (TFOpLa  (None, 64, 64, 128)  0          ['tf.__operators__.add_6[0][0]', \n",
      " mbda)                                                            'activation_22[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_23 (Conv2D)             (None, 64, 64, 128)  16384       ['tf.__operators__.add_7[0][0]'] \n",
      "                                                                                                  \n",
      " batch_normalization_23 (BatchN  (None, 64, 64, 128)  512        ['conv2d_23[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_23 (Activation)     (None, 64, 64, 128)  0           ['batch_normalization_23[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_24 (Conv2D)             (None, 64, 64, 128)  147456      ['activation_23[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_24 (BatchN  (None, 64, 64, 128)  512        ['conv2d_24[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_24 (Activation)     (None, 64, 64, 128)  0           ['batch_normalization_24[0][0]'] \n",
      "                                                                                                  \n",
      " tf.__operators__.add_8 (TFOpLa  (None, 64, 64, 128)  0          ['tf.__operators__.add_7[0][0]', \n",
      " mbda)                                                            'activation_24[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_25 (Conv2D)             (None, 64, 64, 128)  16384       ['tf.__operators__.add_8[0][0]'] \n",
      "                                                                                                  \n",
      " batch_normalization_25 (BatchN  (None, 64, 64, 128)  512        ['conv2d_25[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_25 (Activation)     (None, 64, 64, 128)  0           ['batch_normalization_25[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_26 (Conv2D)             (None, 64, 64, 128)  147456      ['activation_25[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_26 (BatchN  (None, 64, 64, 128)  512        ['conv2d_26[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_26 (Activation)     (None, 64, 64, 128)  0           ['batch_normalization_26[0][0]'] \n",
      "                                                                                                  \n",
      " tf.__operators__.add_9 (TFOpLa  (None, 64, 64, 128)  0          ['tf.__operators__.add_8[0][0]', \n",
      " mbda)                                                            'activation_26[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_27 (Conv2D)             (None, 64, 64, 128)  16384       ['tf.__operators__.add_9[0][0]'] \n",
      "                                                                                                  \n",
      " batch_normalization_27 (BatchN  (None, 64, 64, 128)  512        ['conv2d_27[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_27 (Activation)     (None, 64, 64, 128)  0           ['batch_normalization_27[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_28 (Conv2D)             (None, 64, 64, 128)  147456      ['activation_27[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_28 (BatchN  (None, 64, 64, 128)  512        ['conv2d_28[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_28 (Activation)     (None, 64, 64, 128)  0           ['batch_normalization_28[0][0]'] \n",
      "                                                                                                  \n",
      " tf.__operators__.add_10 (TFOpL  (None, 64, 64, 128)  0          ['tf.__operators__.add_9[0][0]', \n",
      " ambda)                                                           'activation_28[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_29 (Conv2D)             (None, 64, 64, 128)  16384       ['tf.__operators__.add_10[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_29 (BatchN  (None, 64, 64, 128)  512        ['conv2d_29[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_29 (Activation)     (None, 64, 64, 128)  0           ['batch_normalization_29[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_30 (Conv2D)             (None, 64, 64, 128)  147456      ['activation_29[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_31 (Conv2D)             (None, 64, 64, 128)  32768       ['activation_11[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_30 (BatchN  (None, 64, 64, 128)  512        ['conv2d_30[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_31 (BatchN  (None, 64, 64, 128)  512        ['conv2d_31[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_30 (Activation)     (None, 64, 64, 128)  0           ['batch_normalization_30[0][0]'] \n",
      "                                                                                                  \n",
      " activation_31 (Activation)     (None, 64, 64, 128)  0           ['batch_normalization_31[0][0]'] \n",
      "                                                                                                  \n",
      " tf.__operators__.add_11 (TFOpL  (None, 64, 64, 128)  0          ['tf.__operators__.add_10[0][0]',\n",
      " ambda)                                                           'activation_30[0][0]']          \n",
      "                                                                                                  \n",
      " concatenate_1 (Concatenate)    (None, 64, 64, 256)  0           ['activation_31[0][0]',          \n",
      "                                                                  'tf.__operators__.add_11[0][0]']\n",
      "                                                                                                  \n",
      " conv2d_32 (Conv2D)             (None, 64, 64, 256)  65536       ['concatenate_1[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_32 (BatchN  (None, 64, 64, 256)  1024       ['conv2d_32[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_32 (Activation)     (None, 64, 64, 256)  0           ['batch_normalization_32[0][0]'] \n",
      "                                                                                                  \n",
      " zero_padding2d_2 (ZeroPadding2  (None, 65, 65, 256)  0          ['activation_32[0][0]']          \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " conv2d_33 (Conv2D)             (None, 32, 32, 512)  1179648     ['zero_padding2d_2[0][0]']       \n",
      "                                                                                                  \n",
      " batch_normalization_33 (BatchN  (None, 32, 32, 512)  2048       ['conv2d_33[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_33 (Activation)     (None, 32, 32, 512)  0           ['batch_normalization_33[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_34 (Conv2D)             (None, 32, 32, 256)  131072      ['activation_33[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_34 (BatchN  (None, 32, 32, 256)  1024       ['conv2d_34[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_34 (Activation)     (None, 32, 32, 256)  0           ['batch_normalization_34[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_35 (Conv2D)             (None, 32, 32, 256)  65536       ['activation_34[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_35 (BatchN  (None, 32, 32, 256)  1024       ['conv2d_35[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_35 (Activation)     (None, 32, 32, 256)  0           ['batch_normalization_35[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_36 (Conv2D)             (None, 32, 32, 256)  589824      ['activation_35[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_36 (BatchN  (None, 32, 32, 256)  1024       ['conv2d_36[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_36 (Activation)     (None, 32, 32, 256)  0           ['batch_normalization_36[0][0]'] \n",
      "                                                                                                  \n",
      " tf.__operators__.add_12 (TFOpL  (None, 32, 32, 256)  0          ['activation_34[0][0]',          \n",
      " ambda)                                                           'activation_36[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_37 (Conv2D)             (None, 32, 32, 256)  65536       ['tf.__operators__.add_12[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_37 (BatchN  (None, 32, 32, 256)  1024       ['conv2d_37[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_37 (Activation)     (None, 32, 32, 256)  0           ['batch_normalization_37[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_38 (Conv2D)             (None, 32, 32, 256)  589824      ['activation_37[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_38 (BatchN  (None, 32, 32, 256)  1024       ['conv2d_38[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_38 (Activation)     (None, 32, 32, 256)  0           ['batch_normalization_38[0][0]'] \n",
      "                                                                                                  \n",
      " tf.__operators__.add_13 (TFOpL  (None, 32, 32, 256)  0          ['tf.__operators__.add_12[0][0]',\n",
      " ambda)                                                           'activation_38[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_39 (Conv2D)             (None, 32, 32, 256)  65536       ['tf.__operators__.add_13[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_39 (BatchN  (None, 32, 32, 256)  1024       ['conv2d_39[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_39 (Activation)     (None, 32, 32, 256)  0           ['batch_normalization_39[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_40 (Conv2D)             (None, 32, 32, 256)  589824      ['activation_39[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_40 (BatchN  (None, 32, 32, 256)  1024       ['conv2d_40[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_40 (Activation)     (None, 32, 32, 256)  0           ['batch_normalization_40[0][0]'] \n",
      "                                                                                                  \n",
      " tf.__operators__.add_14 (TFOpL  (None, 32, 32, 256)  0          ['tf.__operators__.add_13[0][0]',\n",
      " ambda)                                                           'activation_40[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_41 (Conv2D)             (None, 32, 32, 256)  65536       ['tf.__operators__.add_14[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_41 (BatchN  (None, 32, 32, 256)  1024       ['conv2d_41[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_41 (Activation)     (None, 32, 32, 256)  0           ['batch_normalization_41[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_42 (Conv2D)             (None, 32, 32, 256)  589824      ['activation_41[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_42 (BatchN  (None, 32, 32, 256)  1024       ['conv2d_42[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_42 (Activation)     (None, 32, 32, 256)  0           ['batch_normalization_42[0][0]'] \n",
      "                                                                                                  \n",
      " tf.__operators__.add_15 (TFOpL  (None, 32, 32, 256)  0          ['tf.__operators__.add_14[0][0]',\n",
      " ambda)                                                           'activation_42[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_43 (Conv2D)             (None, 32, 32, 256)  65536       ['tf.__operators__.add_15[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_43 (BatchN  (None, 32, 32, 256)  1024       ['conv2d_43[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_43 (Activation)     (None, 32, 32, 256)  0           ['batch_normalization_43[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_44 (Conv2D)             (None, 32, 32, 256)  589824      ['activation_43[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_44 (BatchN  (None, 32, 32, 256)  1024       ['conv2d_44[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_44 (Activation)     (None, 32, 32, 256)  0           ['batch_normalization_44[0][0]'] \n",
      "                                                                                                  \n",
      " tf.__operators__.add_16 (TFOpL  (None, 32, 32, 256)  0          ['tf.__operators__.add_15[0][0]',\n",
      " ambda)                                                           'activation_44[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_45 (Conv2D)             (None, 32, 32, 256)  65536       ['tf.__operators__.add_16[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_45 (BatchN  (None, 32, 32, 256)  1024       ['conv2d_45[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_45 (Activation)     (None, 32, 32, 256)  0           ['batch_normalization_45[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_46 (Conv2D)             (None, 32, 32, 256)  589824      ['activation_45[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_46 (BatchN  (None, 32, 32, 256)  1024       ['conv2d_46[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_46 (Activation)     (None, 32, 32, 256)  0           ['batch_normalization_46[0][0]'] \n",
      "                                                                                                  \n",
      " tf.__operators__.add_17 (TFOpL  (None, 32, 32, 256)  0          ['tf.__operators__.add_16[0][0]',\n",
      " ambda)                                                           'activation_46[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_47 (Conv2D)             (None, 32, 32, 256)  65536       ['tf.__operators__.add_17[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_47 (BatchN  (None, 32, 32, 256)  1024       ['conv2d_47[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_47 (Activation)     (None, 32, 32, 256)  0           ['batch_normalization_47[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_48 (Conv2D)             (None, 32, 32, 256)  589824      ['activation_47[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_48 (BatchN  (None, 32, 32, 256)  1024       ['conv2d_48[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_48 (Activation)     (None, 32, 32, 256)  0           ['batch_normalization_48[0][0]'] \n",
      "                                                                                                  \n",
      " tf.__operators__.add_18 (TFOpL  (None, 32, 32, 256)  0          ['tf.__operators__.add_17[0][0]',\n",
      " ambda)                                                           'activation_48[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_49 (Conv2D)             (None, 32, 32, 256)  65536       ['tf.__operators__.add_18[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_49 (BatchN  (None, 32, 32, 256)  1024       ['conv2d_49[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_49 (Activation)     (None, 32, 32, 256)  0           ['batch_normalization_49[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_50 (Conv2D)             (None, 32, 32, 256)  589824      ['activation_49[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_50 (BatchN  (None, 32, 32, 256)  1024       ['conv2d_50[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_50 (Activation)     (None, 32, 32, 256)  0           ['batch_normalization_50[0][0]'] \n",
      "                                                                                                  \n",
      " tf.__operators__.add_19 (TFOpL  (None, 32, 32, 256)  0          ['tf.__operators__.add_18[0][0]',\n",
      " ambda)                                                           'activation_50[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_51 (Conv2D)             (None, 32, 32, 256)  65536       ['tf.__operators__.add_19[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_51 (BatchN  (None, 32, 32, 256)  1024       ['conv2d_51[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_51 (Activation)     (None, 32, 32, 256)  0           ['batch_normalization_51[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_52 (Conv2D)             (None, 32, 32, 256)  589824      ['activation_51[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_53 (Conv2D)             (None, 32, 32, 256)  131072      ['activation_33[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_52 (BatchN  (None, 32, 32, 256)  1024       ['conv2d_52[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_53 (BatchN  (None, 32, 32, 256)  1024       ['conv2d_53[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_52 (Activation)     (None, 32, 32, 256)  0           ['batch_normalization_52[0][0]'] \n",
      "                                                                                                  \n",
      " activation_53 (Activation)     (None, 32, 32, 256)  0           ['batch_normalization_53[0][0]'] \n",
      "                                                                                                  \n",
      " tf.__operators__.add_20 (TFOpL  (None, 32, 32, 256)  0          ['tf.__operators__.add_19[0][0]',\n",
      " ambda)                                                           'activation_52[0][0]']          \n",
      "                                                                                                  \n",
      " concatenate_2 (Concatenate)    (None, 32, 32, 512)  0           ['activation_53[0][0]',          \n",
      "                                                                  'tf.__operators__.add_20[0][0]']\n",
      "                                                                                                  \n",
      " conv2d_54 (Conv2D)             (None, 32, 32, 512)  262144      ['concatenate_2[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_54 (BatchN  (None, 32, 32, 512)  2048       ['conv2d_54[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_54 (Activation)     (None, 32, 32, 512)  0           ['batch_normalization_54[0][0]'] \n",
      "                                                                                                  \n",
      " zero_padding2d_3 (ZeroPadding2  (None, 33, 33, 512)  0          ['activation_54[0][0]']          \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " conv2d_55 (Conv2D)             (None, 16, 16, 1024  4718592     ['zero_padding2d_3[0][0]']       \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " batch_normalization_55 (BatchN  (None, 16, 16, 1024  4096       ['conv2d_55[0][0]']              \n",
      " ormalization)                  )                                                                 \n",
      "                                                                                                  \n",
      " activation_55 (Activation)     (None, 16, 16, 1024  0           ['batch_normalization_55[0][0]'] \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_56 (Conv2D)             (None, 16, 16, 512)  524288      ['activation_55[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_56 (BatchN  (None, 16, 16, 512)  2048       ['conv2d_56[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_56 (Activation)     (None, 16, 16, 512)  0           ['batch_normalization_56[0][0]'] \n",
      "                                                                                                  \n",
      " tf.nn.max_pool2d (TFOpLambda)  (None, 16, 16, 512)  0           ['activation_56[0][0]']          \n",
      "                                                                                                  \n",
      " tf.nn.max_pool2d_1 (TFOpLambda  (None, 16, 16, 512)  0          ['activation_56[0][0]']          \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.nn.max_pool2d_2 (TFOpLambda  (None, 16, 16, 512)  0          ['activation_56[0][0]']          \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " concatenate_3 (Concatenate)    (None, 16, 16, 2048  0           ['activation_56[0][0]',          \n",
      "                                )                                 'tf.nn.max_pool2d[0][0]',       \n",
      "                                                                  'tf.nn.max_pool2d_1[0][0]',     \n",
      "                                                                  'tf.nn.max_pool2d_2[0][0]']     \n",
      "                                                                                                  \n",
      " conv2d_57 (Conv2D)             (None, 16, 16, 1024  2097152     ['concatenate_3[0][0]']          \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " batch_normalization_57 (BatchN  (None, 16, 16, 1024  4096       ['conv2d_57[0][0]']              \n",
      " ormalization)                  )                                                                 \n",
      "                                                                                                  \n",
      " activation_57 (Activation)     (None, 16, 16, 1024  0           ['batch_normalization_57[0][0]'] \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_58 (Conv2D)             (None, 16, 16, 512)  524288      ['activation_57[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_58 (BatchN  (None, 16, 16, 512)  2048       ['conv2d_58[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_58 (Activation)     (None, 16, 16, 512)  0           ['batch_normalization_58[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_59 (Conv2D)             (None, 16, 16, 512)  262144      ['activation_58[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_59 (BatchN  (None, 16, 16, 512)  2048       ['conv2d_59[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_59 (Activation)     (None, 16, 16, 512)  0           ['batch_normalization_59[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_60 (Conv2D)             (None, 16, 16, 512)  2359296     ['activation_59[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_60 (BatchN  (None, 16, 16, 512)  2048       ['conv2d_60[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_60 (Activation)     (None, 16, 16, 512)  0           ['batch_normalization_60[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_61 (Conv2D)             (None, 16, 16, 512)  262144      ['activation_60[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_61 (BatchN  (None, 16, 16, 512)  2048       ['conv2d_61[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_61 (Activation)     (None, 16, 16, 512)  0           ['batch_normalization_61[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_62 (Conv2D)             (None, 16, 16, 512)  2359296     ['activation_61[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_62 (BatchN  (None, 16, 16, 512)  2048       ['conv2d_62[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_62 (Activation)     (None, 16, 16, 512)  0           ['batch_normalization_62[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_63 (Conv2D)             (None, 16, 16, 512)  262144      ['activation_62[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_63 (BatchN  (None, 16, 16, 512)  2048       ['conv2d_63[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_63 (Activation)     (None, 16, 16, 512)  0           ['batch_normalization_63[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_65 (Conv2D)             (None, 16, 16, 512)  524288      ['activation_57[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_64 (Conv2D)             (None, 16, 16, 512)  2359296     ['activation_63[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_65 (BatchN  (None, 16, 16, 512)  2048       ['conv2d_65[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_64 (BatchN  (None, 16, 16, 512)  2048       ['conv2d_64[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_65 (Activation)     (None, 16, 16, 512)  0           ['batch_normalization_65[0][0]'] \n",
      "                                                                                                  \n",
      " activation_64 (Activation)     (None, 16, 16, 512)  0           ['batch_normalization_64[0][0]'] \n",
      "                                                                                                  \n",
      " concatenate_4 (Concatenate)    (None, 16, 16, 1024  0           ['activation_65[0][0]',          \n",
      "                                )                                 'activation_64[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_66 (Conv2D)             (None, 16, 16, 1024  1048576     ['concatenate_4[0][0]']          \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " batch_normalization_66 (BatchN  (None, 16, 16, 1024  4096       ['conv2d_66[0][0]']              \n",
      " ormalization)                  )                                                                 \n",
      "                                                                                                  \n",
      " activation_66 (Activation)     (None, 16, 16, 1024  0           ['batch_normalization_66[0][0]'] \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_67 (Conv2D)             (None, 16, 16, 512)  524288      ['activation_66[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_67 (BatchN  (None, 16, 16, 512)  2048       ['conv2d_67[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_67 (Activation)     (None, 16, 16, 512)  0           ['batch_normalization_67[0][0]'] \n",
      "                                                                                                  \n",
      " up_sampling2d (UpSampling2D)   (None, 32, 32, 512)  0           ['activation_67[0][0]']          \n",
      "                                                                                                  \n",
      " concatenate_5 (Concatenate)    (None, 32, 32, 1024  0           ['up_sampling2d[0][0]',          \n",
      "                                )                                 'activation_54[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_68 (Conv2D)             (None, 32, 32, 256)  262144      ['concatenate_5[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_68 (BatchN  (None, 32, 32, 256)  1024       ['conv2d_68[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_68 (Activation)     (None, 32, 32, 256)  0           ['batch_normalization_68[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_69 (Conv2D)             (None, 32, 32, 256)  65536       ['activation_68[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_69 (BatchN  (None, 32, 32, 256)  1024       ['conv2d_69[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_69 (Activation)     (None, 32, 32, 256)  0           ['batch_normalization_69[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_70 (Conv2D)             (None, 32, 32, 256)  589824      ['activation_69[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_70 (BatchN  (None, 32, 32, 256)  1024       ['conv2d_70[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_70 (Activation)     (None, 32, 32, 256)  0           ['batch_normalization_70[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_71 (Conv2D)             (None, 32, 32, 256)  65536       ['activation_70[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_71 (BatchN  (None, 32, 32, 256)  1024       ['conv2d_71[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_71 (Activation)     (None, 32, 32, 256)  0           ['batch_normalization_71[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_72 (Conv2D)             (None, 32, 32, 256)  589824      ['activation_71[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_72 (BatchN  (None, 32, 32, 256)  1024       ['conv2d_72[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_72 (Activation)     (None, 32, 32, 256)  0           ['batch_normalization_72[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_73 (Conv2D)             (None, 32, 32, 256)  65536       ['activation_72[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_73 (BatchN  (None, 32, 32, 256)  1024       ['conv2d_73[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_73 (Activation)     (None, 32, 32, 256)  0           ['batch_normalization_73[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_75 (Conv2D)             (None, 32, 32, 256)  262144      ['concatenate_5[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_74 (Conv2D)             (None, 32, 32, 256)  589824      ['activation_73[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_75 (BatchN  (None, 32, 32, 256)  1024       ['conv2d_75[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_74 (BatchN  (None, 32, 32, 256)  1024       ['conv2d_74[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_75 (Activation)     (None, 32, 32, 256)  0           ['batch_normalization_75[0][0]'] \n",
      "                                                                                                  \n",
      " activation_74 (Activation)     (None, 32, 32, 256)  0           ['batch_normalization_74[0][0]'] \n",
      "                                                                                                  \n",
      " concatenate_6 (Concatenate)    (None, 32, 32, 512)  0           ['activation_75[0][0]',          \n",
      "                                                                  'activation_74[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_76 (Conv2D)             (None, 32, 32, 512)  262144      ['concatenate_6[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_76 (BatchN  (None, 32, 32, 512)  2048       ['conv2d_76[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_76 (Activation)     (None, 32, 32, 512)  0           ['batch_normalization_76[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_77 (Conv2D)             (None, 32, 32, 256)  131072      ['activation_76[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_77 (BatchN  (None, 32, 32, 256)  1024       ['conv2d_77[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_77 (Activation)     (None, 32, 32, 256)  0           ['batch_normalization_77[0][0]'] \n",
      "                                                                                                  \n",
      " up_sampling2d_1 (UpSampling2D)  (None, 64, 64, 256)  0          ['activation_77[0][0]']          \n",
      "                                                                                                  \n",
      " concatenate_7 (Concatenate)    (None, 64, 64, 512)  0           ['up_sampling2d_1[0][0]',        \n",
      "                                                                  'activation_32[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_78 (Conv2D)             (None, 64, 64, 128)  65536       ['concatenate_7[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_78 (BatchN  (None, 64, 64, 128)  512        ['conv2d_78[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_78 (Activation)     (None, 64, 64, 128)  0           ['batch_normalization_78[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_79 (Conv2D)             (None, 64, 64, 128)  16384       ['activation_78[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_79 (BatchN  (None, 64, 64, 128)  512        ['conv2d_79[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_79 (Activation)     (None, 64, 64, 128)  0           ['batch_normalization_79[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_80 (Conv2D)             (None, 64, 64, 128)  147456      ['activation_79[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_80 (BatchN  (None, 64, 64, 128)  512        ['conv2d_80[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_80 (Activation)     (None, 64, 64, 128)  0           ['batch_normalization_80[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_81 (Conv2D)             (None, 64, 64, 128)  16384       ['activation_80[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_81 (BatchN  (None, 64, 64, 128)  512        ['conv2d_81[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_81 (Activation)     (None, 64, 64, 128)  0           ['batch_normalization_81[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_82 (Conv2D)             (None, 64, 64, 128)  147456      ['activation_81[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_82 (BatchN  (None, 64, 64, 128)  512        ['conv2d_82[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_82 (Activation)     (None, 64, 64, 128)  0           ['batch_normalization_82[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_83 (Conv2D)             (None, 64, 64, 128)  16384       ['activation_82[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_83 (BatchN  (None, 64, 64, 128)  512        ['conv2d_83[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_83 (Activation)     (None, 64, 64, 128)  0           ['batch_normalization_83[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_85 (Conv2D)             (None, 64, 64, 128)  65536       ['concatenate_7[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_84 (Conv2D)             (None, 64, 64, 128)  147456      ['activation_83[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_85 (BatchN  (None, 64, 64, 128)  512        ['conv2d_85[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_84 (BatchN  (None, 64, 64, 128)  512        ['conv2d_84[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_85 (Activation)     (None, 64, 64, 128)  0           ['batch_normalization_85[0][0]'] \n",
      "                                                                                                  \n",
      " activation_84 (Activation)     (None, 64, 64, 128)  0           ['batch_normalization_84[0][0]'] \n",
      "                                                                                                  \n",
      " concatenate_8 (Concatenate)    (None, 64, 64, 256)  0           ['activation_85[0][0]',          \n",
      "                                                                  'activation_84[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_86 (Conv2D)             (None, 64, 64, 256)  65536       ['concatenate_8[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_86 (BatchN  (None, 64, 64, 256)  1024       ['conv2d_86[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_86 (Activation)     (None, 64, 64, 256)  0           ['batch_normalization_86[0][0]'] \n",
      "                                                                                                  \n",
      " zero_padding2d_4 (ZeroPadding2  (None, 65, 65, 256)  0          ['activation_86[0][0]']          \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " conv2d_87 (Conv2D)             (None, 32, 32, 256)  589824      ['zero_padding2d_4[0][0]']       \n",
      "                                                                                                  \n",
      " batch_normalization_87 (BatchN  (None, 32, 32, 256)  1024       ['conv2d_87[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_87 (Activation)     (None, 32, 32, 256)  0           ['batch_normalization_87[0][0]'] \n",
      "                                                                                                  \n",
      " concatenate_9 (Concatenate)    (None, 32, 32, 512)  0           ['activation_87[0][0]',          \n",
      "                                                                  'activation_77[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_88 (Conv2D)             (None, 32, 32, 256)  131072      ['concatenate_9[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_88 (BatchN  (None, 32, 32, 256)  1024       ['conv2d_88[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_88 (Activation)     (None, 32, 32, 256)  0           ['batch_normalization_88[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_89 (Conv2D)             (None, 32, 32, 256)  65536       ['activation_88[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_89 (BatchN  (None, 32, 32, 256)  1024       ['conv2d_89[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_89 (Activation)     (None, 32, 32, 256)  0           ['batch_normalization_89[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_90 (Conv2D)             (None, 32, 32, 256)  589824      ['activation_89[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_90 (BatchN  (None, 32, 32, 256)  1024       ['conv2d_90[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_90 (Activation)     (None, 32, 32, 256)  0           ['batch_normalization_90[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_91 (Conv2D)             (None, 32, 32, 256)  65536       ['activation_90[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_91 (BatchN  (None, 32, 32, 256)  1024       ['conv2d_91[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_91 (Activation)     (None, 32, 32, 256)  0           ['batch_normalization_91[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_92 (Conv2D)             (None, 32, 32, 256)  589824      ['activation_91[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_92 (BatchN  (None, 32, 32, 256)  1024       ['conv2d_92[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_92 (Activation)     (None, 32, 32, 256)  0           ['batch_normalization_92[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_93 (Conv2D)             (None, 32, 32, 256)  65536       ['activation_92[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_93 (BatchN  (None, 32, 32, 256)  1024       ['conv2d_93[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_93 (Activation)     (None, 32, 32, 256)  0           ['batch_normalization_93[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_95 (Conv2D)             (None, 32, 32, 256)  131072      ['concatenate_9[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_94 (Conv2D)             (None, 32, 32, 256)  589824      ['activation_93[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_95 (BatchN  (None, 32, 32, 256)  1024       ['conv2d_95[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_94 (BatchN  (None, 32, 32, 256)  1024       ['conv2d_94[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_95 (Activation)     (None, 32, 32, 256)  0           ['batch_normalization_95[0][0]'] \n",
      "                                                                                                  \n",
      " activation_94 (Activation)     (None, 32, 32, 256)  0           ['batch_normalization_94[0][0]'] \n",
      "                                                                                                  \n",
      " concatenate_10 (Concatenate)   (None, 32, 32, 512)  0           ['activation_95[0][0]',          \n",
      "                                                                  'activation_94[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_96 (Conv2D)             (None, 32, 32, 512)  262144      ['concatenate_10[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_96 (BatchN  (None, 32, 32, 512)  2048       ['conv2d_96[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_96 (Activation)     (None, 32, 32, 512)  0           ['batch_normalization_96[0][0]'] \n",
      "                                                                                                  \n",
      " zero_padding2d_5 (ZeroPadding2  (None, 33, 33, 512)  0          ['activation_96[0][0]']          \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " conv2d_97 (Conv2D)             (None, 16, 16, 512)  2359296     ['zero_padding2d_5[0][0]']       \n",
      "                                                                                                  \n",
      " batch_normalization_97 (BatchN  (None, 16, 16, 512)  2048       ['conv2d_97[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_97 (Activation)     (None, 16, 16, 512)  0           ['batch_normalization_97[0][0]'] \n",
      "                                                                                                  \n",
      " concatenate_11 (Concatenate)   (None, 16, 16, 1024  0           ['activation_97[0][0]',          \n",
      "                                )                                 'activation_67[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_98 (Conv2D)             (None, 16, 16, 512)  524288      ['concatenate_11[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_98 (BatchN  (None, 16, 16, 512)  2048       ['conv2d_98[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_98 (Activation)     (None, 16, 16, 512)  0           ['batch_normalization_98[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_99 (Conv2D)             (None, 16, 16, 512)  262144      ['activation_98[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_99 (BatchN  (None, 16, 16, 512)  2048       ['conv2d_99[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_99 (Activation)     (None, 16, 16, 512)  0           ['batch_normalization_99[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_100 (Conv2D)            (None, 16, 16, 512)  2359296     ['activation_99[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_100 (Batch  (None, 16, 16, 512)  2048       ['conv2d_100[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_100 (Activation)    (None, 16, 16, 512)  0           ['batch_normalization_100[0][0]']\n",
      "                                                                                                  \n",
      " conv2d_101 (Conv2D)            (None, 16, 16, 512)  262144      ['activation_100[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_101 (Batch  (None, 16, 16, 512)  2048       ['conv2d_101[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_101 (Activation)    (None, 16, 16, 512)  0           ['batch_normalization_101[0][0]']\n",
      "                                                                                                  \n",
      " conv2d_102 (Conv2D)            (None, 16, 16, 512)  2359296     ['activation_101[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_102 (Batch  (None, 16, 16, 512)  2048       ['conv2d_102[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_102 (Activation)    (None, 16, 16, 512)  0           ['batch_normalization_102[0][0]']\n",
      "                                                                                                  \n",
      " conv2d_103 (Conv2D)            (None, 16, 16, 512)  262144      ['activation_102[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_103 (Batch  (None, 16, 16, 512)  2048       ['conv2d_103[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_103 (Activation)    (None, 16, 16, 512)  0           ['batch_normalization_103[0][0]']\n",
      "                                                                                                  \n",
      " conv2d_105 (Conv2D)            (None, 16, 16, 512)  524288      ['concatenate_11[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_104 (Conv2D)            (None, 16, 16, 512)  2359296     ['activation_103[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_105 (Batch  (None, 16, 16, 512)  2048       ['conv2d_105[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " batch_normalization_104 (Batch  (None, 16, 16, 512)  2048       ['conv2d_104[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_105 (Activation)    (None, 16, 16, 512)  0           ['batch_normalization_105[0][0]']\n",
      "                                                                                                  \n",
      " activation_104 (Activation)    (None, 16, 16, 512)  0           ['batch_normalization_104[0][0]']\n",
      "                                                                                                  \n",
      " concatenate_12 (Concatenate)   (None, 16, 16, 1024  0           ['activation_105[0][0]',         \n",
      "                                )                                 'activation_104[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_106 (Conv2D)            (None, 16, 16, 1024  1048576     ['concatenate_12[0][0]']         \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " batch_normalization_106 (Batch  (None, 16, 16, 1024  4096       ['conv2d_106[0][0]']             \n",
      " Normalization)                 )                                                                 \n",
      "                                                                                                  \n",
      " activation_106 (Activation)    (None, 16, 16, 1024  0           ['batch_normalization_106[0][0]']\n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " p5_4 (Conv2D)                  (None, 16, 16, 27)   27675       ['activation_106[0][0]']         \n",
      "                                                                                                  \n",
      " p4_4 (Conv2D)                  (None, 32, 32, 27)   13851       ['activation_96[0][0]']          \n",
      "                                                                                                  \n",
      " p3_4 (Conv2D)                  (None, 64, 64, 27)   6939        ['activation_86[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 46,729,809\n",
      "Trainable params: 46,668,241\n",
      "Non-trainable params: 61,568\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "executionInfo": {
     "elapsed": 18,
     "status": "ok",
     "timestamp": 1638455631606,
     "user": {
      "displayName": "Jill Naiman",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "13328008118965652761"
     },
     "user_tz": 360
    },
    "id": "6tXmyitL7jy2"
   },
   "outputs": [],
   "source": [
    "# plot if you wanna\n",
    "#tf.keras.utils.plot_model(model_v5, \"yolo_v5.png\", show_shapes=True, show_layer_names=True, expand_nested=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AS4ZcPQg7jy2"
   },
   "source": [
    "For optimizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "executionInfo": {
     "elapsed": 17,
     "status": "ok",
     "timestamp": 1638455631606,
     "user": {
      "displayName": "Jill Naiman",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "13328008118965652761"
     },
     "user_tz": 360
    },
    "id": "IWLTDSz77jy2"
   },
   "outputs": [],
   "source": [
    "LRrate = 0.004\n",
    "#LRrate = 0.002\n",
    "\n",
    "class CosineLR(tf.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self,steps):\n",
    "        super().__init__()\n",
    "        self.lr = LRrate * batch_size / 64\n",
    "        self.warmup_init = LRrate/10.\n",
    "        self.warmup_step = steps\n",
    "        self.decay_steps = tf.cast((num_epochs - 1) * self.warmup_step, tf.float32)\n",
    "\n",
    "    def __call__(self, step):\n",
    "        linear_warmup = tf.cast(step, dtype=tf.float32) / self.warmup_step * (self.lr - self.warmup_init)\n",
    "        cosine_lr = 0.5 * self.lr * (1 + tf.cos(math.pi * tf.cast(step, tf.float32) / self.decay_steps))\n",
    "        return tf.where(step < self.warmup_step, self.warmup_init + linear_warmup, cosine_lr)\n",
    "\n",
    "    def get_config(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "executionInfo": {
     "elapsed": 295,
     "status": "ok",
     "timestamp": 1638455631885,
     "user": {
      "displayName": "Jill Naiman",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "13328008118965652761"
     },
     "user_tz": 360
    },
    "id": "JpreDZDZvvaW"
   },
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(CosineLR(steps_training), 0.937)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "executionInfo": {
     "elapsed": 39,
     "status": "ok",
     "timestamp": 1638455631885,
     "user": {
      "displayName": "Jill Naiman",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "13328008118965652761"
     },
     "user_tz": 360
    },
    "id": "_l3BLK6XDRms"
   },
   "outputs": [],
   "source": [
    "# load weights if you wanna\n",
    "if saved_weights_file is not None:\n",
    "    weightsFiles = glob.glob(weightsDir + 'weights/' + '*h5')\n",
    "    # OR\n",
    "    if saved_weights_file is not None:\n",
    "      weightsFiles = [classDirMain+saved_weights_file]\n",
    "    model.load_weights(weightsFiles[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 39,
     "status": "ok",
     "timestamp": 1638455631886,
     "user": {
      "displayName": "Jill Naiman",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "13328008118965652761"
     },
     "user_tz": 360
    },
    "id": "3MAgw3DHD_nA",
    "outputId": "bcaf2526-7c06-4d2e-d2b1-1c829fa38ec3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.000625"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print(weightsFiles)\n",
    "optimizer.learning_rate.lr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e6Y0CbKb0T76"
   },
   "source": [
    "For saving checkpoints:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1638455631886,
     "user": {
      "displayName": "Jill Naiman",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "13328008118965652761"
     },
     "user_tz": 360
    },
    "id": "REcMXF7W0T76"
   },
   "outputs": [],
   "source": [
    "# for saving model\n",
    "#save_model_name = chksDir + 'checkpoints/'+'model' + str(DATE.today().year).zfill(4) + str(DATE.today().month).zfill(2) + str(DATE.today().day).zfill(2) +'.h5'\n",
    "#checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(save_model_name, save_best_only=True)\n",
    "# today = str(DATE.today().year).zfill(4) + str(DATE.today().month).zfill(2) + str(DATE.today().day).zfill(2)\n",
    "# if not os.path.exists(chksDir + 'checkpoints/'+today):\n",
    "#     os.mkdir(chksDir + 'checkpoints/'+today)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1638455631886,
     "user": {
      "displayName": "Jill Naiman",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "13328008118965652761"
     },
     "user_tz": 360
    },
    "id": "h2o2DlWC0T77"
   },
   "outputs": [],
   "source": [
    "#model2 = tf.saved_model.load(save_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1638455631887,
     "user": {
      "displayName": "Jill Naiman",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "13328008118965652761"
     },
     "user_tz": 360
    },
    "id": "gBbSfeKP0T77"
   },
   "outputs": [],
   "source": [
    "# if restart_from_checkpoints:\n",
    "#     if saved_model_file is None:\n",
    "#         files = glob.glob(chksDir + 'checkpoints/*')\n",
    "#         files.sort()\n",
    "#         model = tf.saved_model.load(files[-1])\n",
    "#         fname = files[-1]\n",
    "#     else:\n",
    "#         model = tf.saved_model.load(chksDir + 'checkpoints/' +saved_model_file)\n",
    "#         fname = chksDir + 'checkpoints/' +saved_model_file\n",
    "#     print('Loading model from', fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RKk7HxgC7jy3"
   },
   "source": [
    "# For processing data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1638455631887,
     "user": {
      "displayName": "Jill Naiman",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "13328008118965652761"
     },
     "user_tz": 360
    },
    "id": "KFOuGj5l0T77",
    "outputId": "0c1dbc6c-9373-473f-b9a5-875440d50132"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'./classifications/'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classDirMain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1638455631887,
     "user": {
      "displayName": "Jill Naiman",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "13328008118965652761"
     },
     "user_tz": 360
    },
    "id": "qqdxpzdoBb6H",
    "outputId": "a62d0dbb-8966-42a6-ef3a-cde4fe98d590"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./classifications/binaries_model8/', './classifications/yolo_512x512_ann/')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classDir_main_to_imgs, classDir_main_to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1638455631888,
     "user": {
      "displayName": "Jill Naiman",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "13328008118965652761"
     },
     "user_tz": 360
    },
    "id": "-L428Dy7VxBV"
   },
   "outputs": [],
   "source": [
    "#import mega_yolo_utils\n",
    "#reload(mega_yolo_utils)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1638455631888,
     "user": {
      "displayName": "Jill Naiman",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "13328008118965652761"
     },
     "user_tz": 360
    },
    "id": "RiM8WUcKGrFn"
   },
   "outputs": [],
   "source": [
    "from mega_yolo_utils import augmentation_generator, csv_gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1638455631888,
     "user": {
      "displayName": "Jill Naiman",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "13328008118965652761"
     },
     "user_tz": 360
    },
    "id": "ewFdU9-77jy3"
   },
   "outputs": [],
   "source": [
    "splitsDirIn = None\n",
    "if onGoogle: splitsDirIn = classDirMain\n",
    "train_gen_csv = csv_gen('train', splitsDir=splitsDirIn)\n",
    "valid_gen_csv = csv_gen('valid',splitsDir=splitsDirIn)\n",
    "test_gen_csv = csv_gen('test',splitsDir=splitsDirIn)\n",
    "\n",
    "# def csv_gen(split):\n",
    "#     ftrain = open(splitsDir+'train.csv','r')\n",
    "#     fvalid = open(splitsDir+'valid.csv','r')\n",
    "#     ftest = open(splitsDir+'test.csv','r')    \n",
    "#     while True:\n",
    "#     #for i in range(100000):\n",
    "#         if b'train' in bytes(split, encoding='utf8'): # NOTE, must be bytes here!!!\n",
    "#             line = ftrain.readline()\n",
    "#             if line == \"\": # if EOF => loop back to start\n",
    "#                 ftrain.seek(0)\n",
    "#                 line = ftrain.readline()\n",
    "#         elif b'valid' in bytes(split, encoding='utf8'):\n",
    "#             line = fvalid.readline()\n",
    "#             if line == \"\": # if EOF => loop back to start\n",
    "#                 fvalid.seek(0)\n",
    "#                 line = fvalid.readline()\n",
    "#         elif b'test' in bytes(split, encoding='utf8'):\n",
    "#             line = ftest.readline()\n",
    "#             if line == \"\": # if EOF => loop back to start\n",
    "#                 ftest.seek(0)\n",
    "#                 line = ftest.readline()\n",
    "#                 # make sure if we are evaluating with the test set \n",
    "#                 #  we don't loop back to the start of the file\n",
    "#                 break \n",
    "#         else:\n",
    "#             print('NOPE!')\n",
    "#             sys.exit()\n",
    "#         yield line\n",
    "        \n",
    "# train_gen_csv = csv_gen('train')\n",
    "# valid_gen_csv = csv_gen('valid')\n",
    "# test_gen_csv = csv_gen('test')\n",
    "\n",
    "def dataset_gen(split, batch_size):\n",
    "    while True:\n",
    "        true_boxes = []; imgs = []; files = []\n",
    "        \n",
    "        while len(files) < batch_size:\n",
    "            if type(split) == str:\n",
    "                if b'train' in bytes(split, encoding='utf8'): # NOTE, must be bytes here!!!\n",
    "                    line = next(train_gen_csv)\n",
    "                elif b'valid' in bytes(split, encoding='utf8'):\n",
    "                    line = next(valid_gen_csv)\n",
    "                elif b'test' in bytes(split, encoding='utf8'):\n",
    "                    line = next(test_gen_csv)\n",
    "            else:\n",
    "                if b'train' in split: # NOTE, must be bytes here!!!\n",
    "                    line = next(train_gen_csv)\n",
    "                elif b'valid' in split:\n",
    "                    line = next(valid_gen_csv)\n",
    "                elif b'test' in split:\n",
    "                    line = next(test_gen_csv)\n",
    "            \n",
    "            files.append(line.strip())\n",
    "\n",
    "        # parse and get full names\n",
    "        try:\n",
    "            imgs_name, bbox = parse_annotation(files, LABELS, \n",
    "                                               feature_dir=classDir_main_to_imgs,\n",
    "                                               annotation_dir=classDir_main_to)\n",
    "        except:\n",
    "            print('error parsing:', imgs_name)\n",
    "        # do a debug check\n",
    "        for im in imgs_name:\n",
    "            if '.npz' not in im:\n",
    "                print('no np file')\n",
    "                import sys; sys.exit()\n",
    "        \n",
    "        # read in and keep images -- npy files\n",
    "        for im in imgs_name:\n",
    "            b = np.load(im)['arr_0']\n",
    "            \n",
    "            ########### DEBUGGING ##########\n",
    "            #b = b[:,:,:3]\n",
    "            ################################\n",
    "            \n",
    "            # convert 0-1\n",
    "            b = b/255.0\n",
    "            imgs.append(b)\n",
    "        \n",
    "        # finally, format for output\n",
    "        y_true1, y_true2, y_true3 = [],[],[]\n",
    "        for b in bbox:\n",
    "            y1,y2,y3= process_box(b[:,:4], b[:,4].astype('int'),anchors,CLASS)\n",
    "            y_true1.append(y1); y_true2.append(y2); y_true3.append(y3)\n",
    "        # if there is no box, do something different\n",
    "        if len(bbox) == 0:\n",
    "            # fake a box\n",
    "            b = np.array([[111.,  59., 403., 364. ,  4.]])\n",
    "            y1,y2,y3= process_box(b[:,:4], b[:,4].astype('int'),anchors,CLASS)\n",
    "            y1[:] = 0; y2[:]=0;y3[:]=0\n",
    "            y_true1.append(y1); y_true2.append(y2); y_true3.append(y3)        \n",
    "        img = tf.cast(np.array(imgs), tf.float32)        \n",
    "        yield img, tf.cast(y_true1, tf.float32), tf.cast(y_true2, tf.float32), tf.cast(y_true3, tf.float32)\n",
    "        \n",
    "\n",
    "def dataset_gen_for_aug(split, batch_size): # for training/validation datasets\n",
    "    while True:\n",
    "        true_boxes = []; imgs = []; files = []\n",
    "        \n",
    "        while len(files) < batch_size:\n",
    "            if type(split) == str:\n",
    "                if b'train' in bytes(split, encoding='utf8'): # NOTE, must be bytes here!!!\n",
    "                    line = next(train_gen_csv)\n",
    "                elif b'valid' in bytes(split, encoding='utf8'):\n",
    "                    line = next(valid_gen_csv)\n",
    "                elif b'test' in bytes(split, encoding='utf8'):\n",
    "                    line = next(test_gen_csv)\n",
    "            else:\n",
    "                if b'train' in split: # NOTE, must be bytes here!!!\n",
    "                    line = next(train_gen_csv)\n",
    "                elif b'valid' in split:\n",
    "                    line = next(valid_gen_csv)\n",
    "                elif b'test' in split:\n",
    "                    line = next(test_gen_csv)\n",
    "            \n",
    "            files.append(line.strip())\n",
    "\n",
    "        # parse and get full names\n",
    "        imgs_name, bbox = parse_annotation(files, LABELS, \n",
    "                                           feature_dir=classDir_main_to_imgs,\n",
    "                                            annotation_dir=classDir_main_to)\n",
    "        # do a debug check\n",
    "        for im in imgs_name:\n",
    "            if '.npz' not in im:\n",
    "                print('no np file')\n",
    "                import sys; sys.exit()\n",
    "        \n",
    "        # read in and keep images -- npy files\n",
    "        for im in imgs_name:\n",
    "            b = np.load(im)['arr_0']\n",
    "            # convert 0-1\n",
    "            b = b/255.0\n",
    "            imgs.append(b)\n",
    "                \n",
    "        img = tf.cast(np.array(imgs), tf.float32)        \n",
    "        yield img, tf.cast(bbox, tf.float32)\n",
    "        \n",
    "\n",
    "def get_dataset(split, labels, batch_size, use_aug=True):\n",
    "    if use_aug and ('test' not in split.lower()):\n",
    "        dataset = tf.data.Dataset.from_generator(dataset_gen_for_aug, args=[split, batch_size],\n",
    "                                                 output_types = (tf.float32, tf.float32))\n",
    "    else:\n",
    "        dataset = tf.data.Dataset.from_generator(dataset_gen, args=[split, batch_size],\n",
    "                                             output_types = (tf.float32, tf.float32, \n",
    "                                                             tf.float32, tf.float32))\n",
    "                                             \n",
    "    dataset = dataset.prefetch(10)\n",
    "    \n",
    "    # maybe?\n",
    "    iterator = iter(dataset)\n",
    "\n",
    "    #return dataset\n",
    "    return iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "executionInfo": {
     "elapsed": 349,
     "status": "ok",
     "timestamp": 1638455632229,
     "user": {
      "displayName": "Jill Naiman",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "13328008118965652761"
     },
     "user_tz": 360
    },
    "id": "iCHTV-qu7jy4"
   },
   "outputs": [],
   "source": [
    "# grab data!\n",
    "train_dataset = None\n",
    "train_dataset= get_dataset('train', LABELS, TRAIN_BATCH_SIZE)\n",
    "\n",
    "val_dataset = None\n",
    "val_dataset= get_dataset('valid', LABELS,VAL_BATCH_SIZE,use_aug=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1638455632230,
     "user": {
      "displayName": "Jill Naiman",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "13328008118965652761"
     },
     "user_tz": 360
    },
    "id": "lsQQuI7dzZJw"
   },
   "outputs": [],
   "source": [
    "#next(valid_gen_csv)\n",
    "#next(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1638455632230,
     "user": {
      "displayName": "Jill Naiman",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "13328008118965652761"
     },
     "user_tz": 360
    },
    "id": "OfxcXCqPyXZj"
   },
   "outputs": [],
   "source": [
    "#val_dataset= get_dataset('valid', LABELS, VAL_BATCH_SIZE,use_aug=True)\n",
    "#next(val_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uvgdY6ly7jy4"
   },
   "source": [
    "Including Augmentation like a boss!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1638455632230,
     "user": {
      "displayName": "Jill Naiman",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "13328008118965652761"
     },
     "user_tz": 360
    },
    "id": "nm5833D_7jy5"
   },
   "outputs": [],
   "source": [
    "aug_train_dataset = augmentation_generator(train_dataset, anchors, CLASS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b5doQu9r7jy5"
   },
   "source": [
    "For calculating the loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "executionInfo": {
     "elapsed": 523,
     "status": "ok",
     "timestamp": 1638455632751,
     "user": {
      "displayName": "Jill Naiman",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "13328008118965652761"
     },
     "user_tz": 360
    },
    "id": "oWeoMEdE7jy5"
   },
   "outputs": [],
   "source": [
    "class ComputeLoss(object):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    @staticmethod\n",
    "    def compute_loss(y_pred, y_true, anchors):\n",
    "        grid_size = tf.shape(y_pred)[1:3]\n",
    "        ratio = tf.cast(tf.constant([config.IMAGE_W, config.IMAGE_H]) / grid_size, tf.float32)\n",
    "        batch_size = tf.cast(tf.shape(y_pred)[0], tf.float32)\n",
    "\n",
    "        x_y_offset, pred_boxes, pred_conf, pred_prob = process_layer(y_pred, anchors,CLASS)\n",
    "\n",
    "        object_mask = y_true[..., 4:5]\n",
    "\n",
    "        def cond(idx, _):\n",
    "            return tf.less(idx, tf.cast(batch_size, tf.int32))\n",
    "\n",
    "        def body(idx, mask):\n",
    "            valid_true_boxes = tf.boolean_mask(y_true[idx, ..., 0:4],\n",
    "                                               tf.cast(object_mask[idx, ..., 0], 'bool'))\n",
    "            iou = box_iou(pred_boxes[idx], valid_true_boxes)\n",
    "            return idx + 1, mask.write(idx, tf.cast(tf.reduce_max(iou, axis=-1) < 0.2, tf.float32))\n",
    "\n",
    "        ignore_mask = tf.TensorArray(dtype=tf.float32, size=0, dynamic_size=True)\n",
    "        \n",
    "        #print('here1.1')\n",
    "        #print(cond, body, ignore_mask)\n",
    "\n",
    "        _, ignore_mask = tf.while_loop(cond=cond, body=body, loop_vars=[0, ignore_mask])\n",
    "        \n",
    "        #print('here1.2')\n",
    "        ignore_mask = ignore_mask.stack()\n",
    "        ignore_mask = tf.expand_dims(ignore_mask, -1)\n",
    "\n",
    "        true_xy = y_true[..., 0:2] / ratio[::-1] - x_y_offset\n",
    "        pred_xy = pred_boxes[..., 0:2] / ratio[::-1] - x_y_offset\n",
    "\n",
    "        true_tw_th = y_true[..., 2:4] / anchors\n",
    "        pred_tw_th = pred_boxes[..., 2:4] / anchors\n",
    "        true_tw_th = tf.where(tf.equal(true_tw_th, 0), tf.ones_like(true_tw_th), true_tw_th)\n",
    "        pred_tw_th = tf.where(tf.equal(pred_tw_th, 0), tf.ones_like(pred_tw_th), pred_tw_th)\n",
    "        true_tw_th = tf.math.log(tf.clip_by_value(true_tw_th, 1e-9, 1e+9))\n",
    "        pred_tw_th = tf.math.log(tf.clip_by_value(pred_tw_th, 1e-9, 1e+9))\n",
    "\n",
    "        box_loss_scale = y_true[..., 2:3] * y_true[..., 3:4]\n",
    "        # note -- assumes IMAGE_H == IMAGE_W\n",
    "        box_loss_scale = 2. - box_loss_scale / tf.cast(config.IMAGE_H ** 2, tf.float32)\n",
    "\n",
    "        xy_loss = tf.reduce_sum(tf.square(true_xy - pred_xy) * object_mask * box_loss_scale)\n",
    "        wh_loss = tf.reduce_sum(tf.square(true_tw_th - pred_tw_th) * object_mask * box_loss_scale)\n",
    "\n",
    "        conf_pos_mask = object_mask\n",
    "        conf_neg_mask = (1 - object_mask) * ignore_mask\n",
    "        conf_loss_pos = conf_pos_mask * tf.nn.sigmoid_cross_entropy_with_logits(labels=object_mask, logits=pred_conf)\n",
    "        conf_loss_neg = conf_neg_mask * tf.nn.sigmoid_cross_entropy_with_logits(labels=object_mask, logits=pred_conf)\n",
    "        # try this\n",
    "        #conf_loss_pos = conf_pos_mask * -tf.reduce_sum(object_mask*tf.math.log(tf.clip_by_value(pred_conf,1e-10,1.0)))\n",
    "        #conf_loss_neg = conf_neg_mask * -tf.reduce_sum(object_mask*tf.math.log(tf.clip_by_value(pred_conf,1e-10,1.0)))\n",
    "\n",
    "\n",
    "        conf_loss = tf.reduce_sum((conf_loss_pos + conf_loss_neg))\n",
    "\n",
    "        true_conf = y_true[..., 5:]\n",
    "\n",
    "        class_loss = object_mask * tf.nn.sigmoid_cross_entropy_with_logits(true_conf, pred_prob)\n",
    "        #class_loss = object_mask * -tf.reduce_sum(true_conf*tf.math.log(tf.clip_by_value(pred_conf,1e-10,1.0)))\n",
    "        #class_loss = object_mask * tf.losses.categorical_crossentropy(true_conf, pred_prob)\n",
    "        #tf.losses.sparse_softmax_cross_entropy(y, logits)\n",
    "        class_loss = tf.reduce_sum(class_loss) # sum across all -- 1 number for loss\n",
    "\n",
    "        if np.isnan(xy_loss):\n",
    "          print('xy_loss is NaN')\n",
    "        if np.isnan(wh_loss):\n",
    "          print('wh_loss is NaN')\n",
    "        if np.isnan(conf_loss):\n",
    "          print('conf_loss is NaN')#, conf_loss_pos, conf_loss_neg)\n",
    "        if np.isnan(class_loss):\n",
    "          print('class_loss is NaN')\n",
    "\n",
    "        if np.isnan(xy_loss + wh_loss + conf_loss + class_loss):\n",
    "          print('--- object mask ---')\n",
    "          print(object_mask.numpy().shape, pred_conf.numpy().shape, true_conf.numpy().shape)\n",
    "          print(object_mask.numpy().max(), object_mask.numpy().min())\n",
    "          print(object_mask)\n",
    "          print(' ')\n",
    "          print('--------')\n",
    "        #else:\n",
    "        #  print(object_mask.numpy().max(), object_mask.numpy().min())\n",
    "\n",
    "        return xy_loss + wh_loss + conf_loss + class_loss\n",
    "\n",
    "    def __call__(self, y_pred, y_true):\n",
    "        loss = 0.\n",
    "        anchor_group = [anchors[6:9], anchors[3:6], anchors[0:3]]\n",
    "\n",
    "        for i in range(len(y_pred)):\n",
    "            loss += self.compute_loss(y_pred[i], y_true[i], anchor_group[i])\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1638455632752,
     "user": {
      "displayName": "Jill Naiman",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "13328008118965652761"
     },
     "user_tz": 360
    },
    "id": "XZZsr5247jy6"
   },
   "outputs": [],
   "source": [
    "loss_object = ComputeLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1638455632752,
     "user": {
      "displayName": "Jill Naiman",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "13328008118965652761"
     },
     "user_tz": 360
    },
    "id": "9lBB1U-k0T7-"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1638455632752,
     "user": {
      "displayName": "Jill Naiman",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "13328008118965652761"
     },
     "user_tz": 360
    },
    "id": "ZbiMja467jy6"
   },
   "outputs": [],
   "source": [
    "def compute_loss(y_true, y_pred):\n",
    "    total_loss = loss_object(y_pred, y_true)\n",
    "    return tf.reduce_sum(total_loss) / batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1638455632753,
     "user": {
      "displayName": "Jill Naiman",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "13328008118965652761"
     },
     "user_tz": 360
    },
    "id": "fbZopKn57jy6"
   },
   "outputs": [],
   "source": [
    "def train_step(image, y_true):\n",
    "    with tf.GradientTape() as tape:\n",
    "        y_pred = model(image, training=True)\n",
    "        loss = compute_loss(y_true, y_pred)\n",
    "    if not np.isnan(loss):\n",
    "        variables = model.trainable_variables\n",
    "        gradients = tape.gradient(loss, variables)\n",
    "        #if np.isnan(loss):\n",
    "        #  print('nan')\n",
    "        #  print\n",
    "        optimizer.apply_gradients(zip(gradients, variables))\n",
    "    else: # this will stop if we have non-convergence \n",
    "        print('is NaN -- probably want to lower your learning rate!!!!')\n",
    "        import sys; sys.exit()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1638455632753,
     "user": {
      "displayName": "Jill Naiman",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "13328008118965652761"
     },
     "user_tz": 360
    },
    "id": "l1ftaeHUvvab"
   },
   "outputs": [],
   "source": [
    "# save weights\n",
    "def save_best_weights(model, name, val_loss_avg):\n",
    "    # delete existing weights file\n",
    "    files = glob.glob(os.path.join(weightsDir + 'weights/', name + '*'))\n",
    "    for file in files:\n",
    "        os.remove(file)\n",
    "    # create new weights file\n",
    "    name = name + '_model_' +version + str(val_loss_avg) + '.h5'\n",
    "    path_name = os.path.join(weightsDir +'weights/', name)\n",
    "    model.save_weights(path_name)\n",
    "    \n",
    "    \n",
    "# log (tensorboard)\n",
    "def log_loss(loss, val_loss, step):\n",
    "    tf.summary.scalar('loss', loss, step)\n",
    "    tf.summary.scalar('val_loss', val_loss, step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1638455632754,
     "user": {
      "displayName": "Jill Naiman",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "13328008118965652761"
     },
     "user_tz": 360
    },
    "id": "C19AoZV9loKh",
    "outputId": "9db600e0-59d8-486f-e8bb-0a0a5e120ccc"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'./classifications/yolo_512x512_ann/'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classDir_main_to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1638455632754,
     "user": {
      "displayName": "Jill Naiman",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "13328008118965652761"
     },
     "user_tz": 360
    },
    "id": "omK8Zjfc7jy6"
   },
   "outputs": [],
   "source": [
    "# training\n",
    "def train(epochs, model, train_dataset, val_dataset, steps_per_epoch_train, \n",
    "          steps_per_epoch_val, optimizer, train_name = 'train'):\n",
    "    '''\n",
    "    Train YOLO model for n epochs.\n",
    "    Eval loss on training and validation dataset.\n",
    "    Log training loss and validation loss for tensorboard.\n",
    "    Save best weights during training (according to validation loss).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    - epochs : integer, number of epochs to train the model.\n",
    "    - model : YOLO model.\n",
    "    - train_dataset : YOLO ground truth and image generator from training dataset.\n",
    "    - val_dataset : YOLO ground truth and image generator from validation dataset.\n",
    "    - steps_per_epoch_train : integer, number of batch to complete one epoch for train_dataset.\n",
    "    - steps_per_epoch_val : integer, number of batch to complete one epoch for val_dataset.\n",
    "    - train_name : string, training name used to log loss and save weights.\n",
    "    \n",
    "    Notes :\n",
    "    - train_dataset and val_dataset generate YOLO ground truth tensors : detector_mask,\n",
    "      matching_true_boxes, class_one_hot, true_boxes_grid. Shape of these tensors (batch size, tensor shape).\n",
    "    - steps per epoch = number of images in dataset // batch size of dataset\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    - loss history : [train_loss_history, val_loss_history] : list of average loss for each epoch.\n",
    "    '''\n",
    "    num_epochs1 = epochs\n",
    "    steps_per_epoch_train = steps_per_epoch_train\n",
    "    steps_per_epoch_val = steps_per_epoch_val\n",
    "    train_loss_history = []\n",
    "    val_loss_history = []\n",
    "    best_val_loss = 1e6\n",
    "    \n",
    "    # optimizer\n",
    "    #optimizer = tf.keras.optimizers.Adam(learning_rate=1e-5, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
    "    \n",
    "    # log (tensorboard)\n",
    "    summary_writer = tf.summary.create_file_writer(os.path.join(logsDir+'logs/', train_name), \n",
    "                                                   flush_millis=20000)\n",
    "    summary_writer.set_as_default()\n",
    "    \n",
    "    # training\n",
    "    for epoch in range(num_epochs1):\n",
    "        epoch_loss = []\n",
    "        epoch_val_loss = []\n",
    "        epoch_val_sub_loss = []\n",
    "        print('Epoch {} :'.format(epoch))\n",
    "        # train\n",
    "        for batch_idx in range(steps_per_epoch_train):        \n",
    "            image, y_true_1, y_true_2, y_true_3 = next(train_dataset)\n",
    "            y_true = (y_true_1, y_true_2, y_true_3)\n",
    "            loss = train_step(image, y_true)\n",
    "            # check for nans\n",
    "            optOrig = optimizer.learning_rate.lr\n",
    "            while np.isnan(loss):\n",
    "              print('loss nan')\n",
    "              optimizer.learning_rate.lr *= 0.5\n",
    "              loss = train_step(image, y_true)\n",
    "            epoch_loss.append(loss)\n",
    "            print('-', end='')\n",
    "        print(' | ', end='')\n",
    "        # val\n",
    "        for batch_idx in range(steps_per_epoch_val): \n",
    "            image, y_true_1, y_true_2, y_true_3 = next(val_dataset)\n",
    "            y_true = (y_true_1, y_true_2, y_true_3)\n",
    "            loss = train_step(image, y_true)\n",
    "            epoch_val_loss.append(loss)\n",
    "            print('-', end='')\n",
    "\n",
    "        loss_avg = np.mean(np.array(epoch_loss))\n",
    "        val_loss_avg = np.mean(np.array(epoch_val_loss))\n",
    "        train_loss_history.append(loss_avg)\n",
    "        val_loss_history.append(val_loss_avg)\n",
    "        \n",
    "        # log\n",
    "        log_loss(loss_avg, val_loss_avg, epoch)\n",
    "        \n",
    "        # save\n",
    "        if val_loss_avg < best_val_loss:\n",
    "            save_best_weights(model, train_name, val_loss_avg)\n",
    "            best_val_loss = val_loss_avg\n",
    "            #tf.saved_model.save(model, chksDir + 'checkpoints/'+today)        \n",
    "        print(' loss = {:.4f}, val_loss = {:.4f}'.format(loss_avg, val_loss_avg))\n",
    "        \n",
    "    return [train_loss_history, val_loss_history]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 30543295,
     "status": "error",
     "timestamp": 1638486176039,
     "user": {
      "displayName": "Jill Naiman",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "13328008118965652761"
     },
     "user_tz": 360
    },
    "id": "l2FpED3Dvvac",
    "outputId": "f22e2867-6b4a-4571-94d1-2b9a5b1cd2ee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 :\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------- loss = 1248.1118, val_loss = 70.4698\n",
      "Epoch 1 :\n",
      "---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------- loss = 45.5740, val_loss = 29.9030\n",
      "Epoch 2 :\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------- loss = 27.9317, val_loss = 21.0205\n",
      "Epoch 3 :\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------- loss = 21.4633, val_loss = 17.8319\n",
      "Epoch 4 :\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------- loss = 18.4384, val_loss = 15.2870\n",
      "Epoch 5 :\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------- loss = 16.3001, val_loss = 14.0304\n",
      "Epoch 6 :\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------- loss = 15.0627, val_loss = 12.0146\n",
      "Epoch 7 :\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------- loss = 13.2663, val_loss = 10.5298\n",
      "Epoch 8 :\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------- loss = 11.9219, val_loss = 9.5759\n",
      "Epoch 9 :\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------- loss = 10.8955, val_loss = 8.7724\n",
      "Epoch 10 :\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------- loss = 10.1621, val_loss = 7.6921\n",
      "Epoch 11 :\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------- loss = 9.1949, val_loss = 7.0505\n",
      "Epoch 12 :\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------- loss = 8.4634, val_loss = 6.6984\n",
      "Epoch 13 :\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------- loss = 8.1502, val_loss = 6.1214\n",
      "Epoch 14 :\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------- loss = 7.5067, val_loss = 5.6582\n",
      "Epoch 15 :\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------- loss = 7.0183, val_loss = 5.3483\n",
      "Epoch 16 :\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------- loss = 6.9992, val_loss = 5.1377\n",
      "Epoch 17 :\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------- loss = 6.7491, val_loss = 5.2978\n",
      "Epoch 18 :\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------- loss = 6.4777, val_loss = 4.7418\n",
      "Epoch 19 :\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------- loss = 6.1694, val_loss = 4.4927\n",
      "Epoch 20 :\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------- loss = 6.0417, val_loss = 4.3680\n",
      "Epoch 21 :\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------- loss = 6.0343, val_loss = 4.2173\n",
      "Epoch 22 :\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------- loss = 5.7581, val_loss = 4.0994\n",
      "Epoch 23 :\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------- loss = 5.4969, val_loss = 3.9718\n",
      "Epoch 24 :\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------- loss = 5.4462, val_loss = 3.7055\n",
      "Epoch 25 :\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------- loss = 5.3575, val_loss = 3.9045\n",
      "Epoch 26 :\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------- loss = 4.9502, val_loss = 3.3405\n",
      "Epoch 27 :\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------- loss = 4.9849, val_loss = 3.2992\n",
      "Epoch 28 :\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------- loss = 4.8162, val_loss = 3.2626\n",
      "Epoch 29 :\n",
      "-----------------"
     ]
    },
    {
     "ename": "UnknownError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnknownError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-49-60f949779889>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m results = train(num_epochs, model, aug_train_dataset, val_dataset, \n\u001b[0;32m----> 2\u001b[0;31m                steps_training, steps_val, optimizer,'training_1'+extraName)\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# debug\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#results = train(num_epochs, model, aug_train_dataset, val_dataset,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-48-686a16384a66>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epochs, model, train_dataset, val_dataset, steps_per_epoch_train, steps_per_epoch_val, optimizer, train_name)\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;31m# train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msteps_per_epoch_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m             \u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true_2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true_3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m             \u001b[0my_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0my_true_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true_2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true_3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/content/gdrive/My Drive/Colab Notebooks/scienceDigitization/mega_yolo_utils.py\u001b[0m in \u001b[0;36maugmentation_generator\u001b[0;34m(yolo_dataset, anchors, CLASS, flipUpDown)\u001b[0m\n\u001b[1;32m   1257\u001b[0m         \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mannotations\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mtensor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax\u001b[0m \u001b[0mannot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1258\u001b[0m     '''\n\u001b[0;32m-> 1259\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0myolo_dataset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1260\u001b[0m         \u001b[0;31m# conversion tensor->numpy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1261\u001b[0m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    798\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    799\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 800\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    801\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    802\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m_next_internal\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    784\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterator_resource\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    785\u001b[0m           \u001b[0moutput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flat_output_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 786\u001b[0;31m           output_shapes=self._flat_output_shapes)\n\u001b[0m\u001b[1;32m    787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/gen_dataset_ops.py\u001b[0m in \u001b[0;36miterator_get_next\u001b[0;34m(iterator, output_types, output_shapes, name)\u001b[0m\n\u001b[1;32m   2843\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2844\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2845\u001b[0;31m       \u001b[0m_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2846\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2847\u001b[0m       \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   7105\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mraise_from_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7106\u001b[0m   \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\" name: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 7107\u001b[0;31m   \u001b[0;32mraise\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   7108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnknownError\u001b[0m: NameError: name 'glob' is not defined\nTraceback (most recent call last):\n\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/script_ops.py\", line 275, in __call__\n    ret = func(*args)\n\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/autograph/impl/api.py\", line 649, in wrapper\n    return func(*args, **kwargs)\n\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/dataset_ops.py\", line 992, in generator_py_func\n    values = next(generator_state.get_iterator(iterator_id))\n\n  File \"<ipython-input-37-612f85b8c85a>\", line 128, in dataset_gen_for_aug\n    annotation_dir=classDir_main_to)\n\n  File \"/content/gdrive/My Drive/Colab Notebooks/scienceDigitization/general_utils.py\", line 95, in parse_annotation\n    iname = glob(iname+'*')[0]\n\nNameError: name 'glob' is not defined\n\n\n\t [[{{node PyFunc}}]] [Op:IteratorGetNext]"
     ]
    }
   ],
   "source": [
    "results = train(num_epochs, model, aug_train_dataset, val_dataset, \n",
    "               steps_training, steps_val, optimizer,'training_1'+extraName)\n",
    "\n",
    "# debug\n",
    "#results = train(num_epochs, model, aug_train_dataset, val_dataset, \n",
    "#               1, 1, optimizer,'training_1'+extraName)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oqGPwg0R7jy6"
   },
   "source": [
    "Plot diagnostics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 256,
     "status": "aborted",
     "timestamp": 1638486175177,
     "user": {
      "displayName": "Jill Naiman",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "13328008118965652761"
     },
     "user_tz": 360
    },
    "id": "68HC1xUF7jy6"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(30,14))\n",
    "ax.plot(results[0], label='Training Loss')\n",
    "ax.plot(results[1], label='Validation Loss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 256,
     "status": "aborted",
     "timestamp": 1638486175178,
     "user": {
      "displayName": "Jill Naiman",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "13328008118965652761"
     },
     "user_tz": 360
    },
    "id": "EjYMDlLo7jy7"
   },
   "outputs": [],
   "source": [
    "# imgs_name, bbox = parse_annotation([X_train[0]], LABELS, \n",
    "#                                    classDir_main_to_imgs=classDir_main_to_imgs,\n",
    "#                                        classDir_main_to_ann=classDir_main_to)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 256,
     "status": "aborted",
     "timestamp": 1638486175178,
     "user": {
      "displayName": "Jill Naiman",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "13328008118965652761"
     },
     "user_tz": 360
    },
    "id": "theCs-467jy8"
   },
   "outputs": [],
   "source": [
    "# from importlib import reload\n",
    "# reload(parse_annotation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 257,
     "status": "aborted",
     "timestamp": 1638486175179,
     "user": {
      "displayName": "Jill Naiman",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "13328008118965652761"
     },
     "user_tz": 360
    },
    "id": "orYNx10Y7jy8"
   },
   "outputs": [],
   "source": [
    "#import general_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 257,
     "status": "aborted",
     "timestamp": 1638486175179,
     "user": {
      "displayName": "Jill Naiman",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "13328008118965652761"
     },
     "user_tz": 360
    },
    "id": "1OXBhSDS7jy8"
   },
   "outputs": [],
   "source": [
    "#reload(general_utils)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 257,
     "status": "aborted",
     "timestamp": 1638486175179,
     "user": {
      "displayName": "Jill Naiman",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "13328008118965652761"
     },
     "user_tz": 360
    },
    "id": "tKhsRWNh7jy8"
   },
   "outputs": [],
   "source": [
    "#from general_utils import parse_annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 258,
     "status": "aborted",
     "timestamp": 1638486175180,
     "user": {
      "displayName": "Jill Naiman",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "13328008118965652761"
     },
     "user_tz": 360
    },
    "id": "8eIrQ2HH5C8Z"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 258,
     "status": "aborted",
     "timestamp": 1638486175180,
     "user": {
      "displayName": "Jill Naiman",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "13328008118965652761"
     },
     "user_tz": 360
    },
    "id": "wXSKvTs95Pp6"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 259,
     "status": "aborted",
     "timestamp": 1638486175181,
     "user": {
      "displayName": "Jill Naiman",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "13328008118965652761"
     },
     "user_tz": 360
    },
    "id": "ihigRX4m5QaB"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "mega_yolo_train.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
