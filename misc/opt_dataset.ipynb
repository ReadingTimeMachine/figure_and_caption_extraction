{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iC4Y4O7bneag"
   },
   "source": [
    "# Mega Yolo -- train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kado6I35Dzr1"
   },
   "source": [
    "## Some toggles for if you want to re-start from weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 300,
     "status": "ok",
     "timestamp": 1638305345334,
     "user": {
      "displayName": "Jill Naiman",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "13328008118965652761"
     },
     "user_tz": 360
    },
    "id": "0bgHP_GnD2SN"
   },
   "outputs": [],
   "source": [
    "# Do you want to re-run from an already generated train/valid/test split?\n",
    "#  -- this is useful for feature testing and/or re-starting from weights\n",
    "re_run_from_splits = True\n",
    "\n",
    "# if restarting, how many previous log files do we want to look at?\n",
    "nRecent = 7 # for the 1st restart, this will be 1, for the 2nd, 2, etc\n",
    "\n",
    "# set to true if you are not re-running from the same dataset\n",
    "regenAnchors = False\n",
    " \n",
    "# use a saved weights file? Set to None if not and training will start anew\n",
    "#saved_weights_file = 'weights/savedWeights/training_1_model_l0.017813377.h5'\n",
    "saved_weights_file = None\n",
    "\n",
    "#fileStorage = 'binaries/' # binaries is where things are -- MAIN   \n",
    "#extraName = '' # append to training weights name\n",
    "\n",
    "# for feature collections\n",
    "#fileStorage = 'binaries_model1/' # binaries is where things are -- MAIN   \n",
    "#extraName = 'model1' # never use 8, this is our usual model?\n",
    "\n",
    "#fileStorage = 'binaries_model1_inverted/' # binaries is where things are -- MAIN   \n",
    "#extraName = 'model1_inverted'\n",
    "\n",
    "#fileStorage = 'binaries_model1_inverted_palletized/'\n",
    "#extraName = 'model1_inverted_palletized'\n",
    "\n",
    "# fileStorage = 'binaries_model2/'\n",
    "# extraName = 'model2'\n",
    "\n",
    "#fileStorage = 'binaries_model3/'\n",
    "#extraName = 'model3'\n",
    "\n",
    "#fileStorage = 'binaries_model4/'\n",
    "#extraName = 'model4'\n",
    "\n",
    "#fileStorage = 'binaries_model5/'\n",
    "#extraName = 'model5'\n",
    "\n",
    "#fileStorage = 'binaries_model5_maxTag125/'\n",
    "#extraName = 'model5_maxTag125'\n",
    "\n",
    "# fileStorage = 'binaries_model6/'\n",
    "# extraName = 'model6'\n",
    "\n",
    "# fileStorage = 'binaries_model8/'\n",
    "# extraName = 'model8'\n",
    "\n",
    "fileStorage = 'binaries_model8/'; method='npz'\n",
    "#fileStorage = 'binaries_model8_pickle/'; method='pickle'\n",
    "#fileStorage = 'binaries_model8_noncom/'; method = 'npy'\n",
    "#fileStorage = 'binaries_model8_noncomz/'; method = 'npz'\n",
    "\n",
    "extraName = 'model8'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 229,
     "status": "ok",
     "timestamp": 1638305346236,
     "user": {
      "displayName": "Jill Naiman",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "13328008118965652761"
     },
     "user_tz": 360
    },
    "id": "dLbPjO38y8NV"
   },
   "outputs": [],
   "source": [
    "# toggle for if on google collab or not\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1638305346557,
     "user": {
      "displayName": "Jill Naiman",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "13328008118965652761"
     },
     "user_tz": 360
    },
    "id": "gnmYH7-3neak",
    "outputId": "25237af9-ee3b-46e2-dfc8-09db00289dff"
   },
   "outputs": [],
   "source": [
    "from sys import path\n",
    "path.append('../')\n",
    "import config\n",
    "classDirMain = config.save_binary_dir #+ fileStorage\n",
    "# where are raw images?\n",
    "images_pulled_dir = config.images_jpeg_dir #'/Users/jillnaiman/Dropbox/wwt_image_extraction/FigureLocalization/Pages/RandomSingleFromPDFIndexed/' ## what about Dropbox though????\n",
    "classDirMainHOME = fileStorage \n",
    "splitsDir = config.tmp_storage_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1638305346558,
     "user": {
      "displayName": "Jill Naiman",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "13328008118965652761"
     },
     "user_tz": 360
    },
    "id": "H67BF_sLneah"
   },
   "outputs": [],
   "source": [
    "# # some parameters for different architectures of YOLO\n",
    "batch_size = 10\n",
    "num_epochs = 125 #150 #300\n",
    "\n",
    "#IMAGE_H, IMAGE_W = 512, 512\n",
    "image_size = config.IMAGE_H # assume width=height\n",
    "\n",
    "TRAIN_BATCH_SIZE = batch_size #10\n",
    "VAL_BATCH_SIZE   = batch_size #10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1638305346558,
     "user": {
      "displayName": "Jill Naiman",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "13328008118965652761"
     },
     "user_tz": 360
    },
    "id": "9GGT2mW9nean",
    "outputId": "3a140232-6fdc-44f7-e6b1-b17e841e2644"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/Users/jillnaiman/MegaYolo/yolo_512x512_ann/',\n",
       " '/Users/jillnaiman/MegaYolo/binaries_model8/')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# where annotations and features files\n",
    "#classDir_main_to = classDirMain + 'yolo_512x512_cap_ann/'\n",
    "#classDir_main_to_imgs = classDirMain + 'binaries/'#+ 'yolo_512x512/'\n",
    "classDir_main_to = classDirMain + config.ann_name + str(int(config.IMAGE_H)) + 'x' + str(int(config.IMAGE_W))  + '_ann/'\n",
    "\n",
    "classDir_main_to_imgs = classDirMain + fileStorage.split('/')[-2] + '/'\n",
    "classDir_main_to, classDir_main_to_imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1638305346559,
     "user": {
      "displayName": "Jill Naiman",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "13328008118965652761"
     },
     "user_tz": 360
    },
    "id": "IVGN-eMPe0-c"
   },
   "outputs": [],
   "source": [
    "#!conda install numba --yes\n",
    "#logsDir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 2276,
     "status": "ok",
     "timestamp": 1638305348829,
     "user": {
      "displayName": "Jill Naiman",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "13328008118965652761"
     },
     "user_tz": 360
    },
    "id": "vwhXD8HOneap"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import re\n",
    "import numpy as np\n",
    "import cv2 as cv\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import xml.etree.cElementTree as ET\n",
    "\n",
    "# make more better?\n",
    "#from numba import jit\n",
    "from time import perf_counter\n",
    "import sys\n",
    "\n",
    "# for v5\n",
    "import math\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 414,
     "status": "ok",
     "timestamp": 1638305349239,
     "user": {
      "displayName": "Jill Naiman",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "13328008118965652761"
     },
     "user_tz": 360
    },
    "id": "TpGXEVAfnear",
    "outputId": "e5d1ec46-a9b1-4262-c794-7b98f42f0c54"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow version : 2.4.1\n",
      "GPU : []\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print('Tensorflow version : {}'.format(tf.__version__))\n",
    "print('GPU : {}'.format(tf.config.list_physical_devices('GPU')))\n",
    "from tensorflow import keras\n",
    "import tensorflow.keras.backend as K\n",
    "import imgaug as ia\n",
    "from imgaug import augmenters as iaa\n",
    "\n",
    "# my imports\n",
    "import pickle\n",
    "#from classification_utils import make_get_csv\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import json\n",
    "#from classification_utils import train_test_valid_split\n",
    "from scipy import stats\n",
    "import shutil\n",
    "\n",
    "# for restart\n",
    "from tensorflow.python.summary.summary_iterator import summary_iterator\n",
    "import struct\n",
    "from datetime import date as DATE\n",
    "\n",
    "# get parse\n",
    "from mega_yolo_utils import build_model, train_test_valid_split, \\\n",
    "    process_box, process_layer, box_iou, compute_nms, iou, num_cluster, generator, \\\n",
    "    get_n_features\n",
    "from general_utils import parse_annotation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HUJLjHVTneav"
   },
   "source": [
    "## First, data setup\n",
    "\n",
    " * Define classes\n",
    " * Grab image location info, grab boxes\n",
    " * Remap images to new size for running through YOLO\n",
    " * get 9 anchors -- why 9? because that is the code we are grabbing, that is why :P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1638305349240,
     "user": {
      "displayName": "Jill Naiman",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "13328008118965652761"
     },
     "user_tz": 360
    },
    "id": "EoCivx-Jneav",
    "outputId": "313ea5ea-2424-4907-dcdb-1516e523950c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5515"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Grab labels\n",
    "annotations = glob.glob(classDir_main_to + '*')\n",
    "len(annotations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tFWzDaooBb6A"
   },
   "source": [
    "Parse annotations -- **NOTE: this can take a while!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7058,
     "status": "ok",
     "timestamp": 1638305356292,
     "user": {
      "displayName": "Jill Naiman",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "13328008118965652761"
     },
     "user_tz": 360
    },
    "id": "N34O8BwTneay",
    "outputId": "b48f751d-1367-4f32-8dcf-202b6b625254"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "on  0  of  5515\n",
      "on  200  of  5515\n",
      "on  400  of  5515\n",
      "on  600  of  5515\n",
      "on  800  of  5515\n",
      "on  1000  of  5515\n",
      "on  1200  of  5515\n",
      "on  1400  of  5515\n",
      "on  1600  of  5515\n",
      "on  1800  of  5515\n",
      "on  2000  of  5515\n",
      "on  2200  of  5515\n",
      "on  2400  of  5515\n",
      "on  2600  of  5515\n",
      "on  2800  of  5515\n",
      "on  3000  of  5515\n",
      "on  3200  of  5515\n",
      "on  3400  of  5515\n",
      "on  3600  of  5515\n",
      "on  3800  of  5515\n",
      "on  4000  of  5515\n",
      "on  4200  of  5515\n",
      "on  4400  of  5515\n",
      "on  4600  of  5515\n",
      "on  4800  of  5515\n",
      "on  5000  of  5515\n",
      "on  5200  of  5515\n",
      "on  5400  of  5515\n",
      "    Elapsed wall clock time = 4.98519 seconds.\n"
     ]
    }
   ],
   "source": [
    "##### what about anchors -- do we want to regenerate? Generally keep this as True...\n",
    "#####regenAnchors = True\n",
    "# ... unless we are re-running from a previous split\n",
    "if re_run_from_splits: regenAnchors = False\n",
    "#if regenAnchorsAnyway: regenAnchors = True\n",
    "\n",
    "if regenAnchors:\n",
    "    bboxes = []\n",
    "    \n",
    "\n",
    "# this parsing does some loading on collab that I'm not 100% sure about, but seems necessary\n",
    "#   to load into memory, so keep it and figure it out later\n",
    "def load_parse_data_split(X_full):\n",
    "    Y_full_str = np.array([]) # have to loop and give best guesses for the pages that have multiple images/classes in them\n",
    "    slabels = np.array([])\n",
    "    for ii, X in enumerate(X_full):\n",
    "        if ii%200 == 0: print('on ', ii, ' of ', len(X_full))\n",
    "        tree = ET.parse(X)\n",
    "        tags = np.array([])\n",
    "        for elem in tree.iter(): \n",
    "            if 'object' in elem.tag or 'part' in elem.tag:                  \n",
    "                box = np.zeros((5))\n",
    "                for attr in list(elem):\n",
    "                    if 'name' in attr.tag:\n",
    "                        if attr.text is not None:\n",
    "                            tags = np.append(tags,attr.text)\n",
    "                            slabels = np.append(slabels,attr.text)\n",
    "                        #print(tags, slabels)\n",
    "                    if regenAnchors:\n",
    "                        if 'bndbox' in attr.tag and 'bndboxOrig' not in attr.tag:\n",
    "                            for dim in list(attr):\n",
    "                                if 'xmin' in dim.tag:\n",
    "                                    box[0] = int(round(float(dim.text)))\n",
    "                                if 'ymin' in dim.tag:\n",
    "                                    box[1] = int(round(float(dim.text)))\n",
    "                                if 'xmax' in dim.tag:\n",
    "                                    box[2] = int(round(float(dim.text)))\n",
    "                                if 'ymax' in dim.tag:\n",
    "                                    box[3] = int(round(float(dim.text)))\n",
    "                if regenAnchors and len(box)>0: bboxes.append(np.asarray(box))\n",
    "        if len(tags) > 0:\n",
    "            #print(tags)\n",
    "            modeClass = stats.mode(tags).mode[0] # most frequent class that pops up on this page\n",
    "            Y_full_str = np.append(Y_full_str, modeClass) # class in string\n",
    "        else:\n",
    "            Y_full_str = np.append(Y_full_str, 'none')\n",
    "    return Y_full_str,slabels\n",
    "\n",
    "# NEXT: do a quick test run-through of the data generator for train/test splits\n",
    "X_full = np.array(annotations)\n",
    "\n",
    "start_time = perf_counter( )\n",
    "Y_full_str,slabels = load_parse_data_split(X_full)\n",
    "stop_time = perf_counter( )\n",
    "print('    Elapsed wall clock time = %g seconds.' % (stop_time - start_time) )\n",
    "    \n",
    "# also do regeneration of anchors\n",
    "if regenAnchors: bboxes = np.array(bboxes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yUWutM847jyy"
   },
   "source": [
    "Get anchors, if that is what you wanna do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 25,
     "status": "ok",
     "timestamp": 1638305356293,
     "user": {
      "displayName": "Jill Naiman",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "13328008118965652761"
     },
     "user_tz": 360
    },
    "id": "Dvtr6msG7jyy",
    "outputId": "d75a4d9f-85d2-4c87-aeae-5908282da486"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from saved:\n",
      "[[195.  21.]\n",
      " [203.   7.]\n",
      " [ 51.   5.]\n",
      " [313. 199.]\n",
      " [359. 391.]\n",
      " [435.  16.]\n",
      " [399. 307.]\n",
      " [204. 114.]\n",
      " [ 15. 355.]]\n"
     ]
    }
   ],
   "source": [
    "# assume location of saved anchors:\n",
    "saveFileAnchors = classDirMain + 'weights/anchors.pickle'\n",
    "# hack for local debugging\n",
    "#if '/Users/jillnaiman' in thisDir:\n",
    "saveFileAnchors = splitsDir + 'anchors.pickle'\n",
    "\n",
    "if regenAnchors:\n",
    "    boxes = []\n",
    "    for b in bboxes:\n",
    "        boxes.append([b[2]-b[0], b[3]-b[1]])\n",
    "    boxes = np.array(boxes)\n",
    "    \n",
    "    anchors = generator(boxes,k=num_cluster)\n",
    "    print('NEW ANCHORS:')\n",
    "    \n",
    "    # save!\n",
    "    with open(saveFileAnchors, 'wb') as ff:\n",
    "        pickle.dump(anchors, ff)\n",
    "else:\n",
    "    print('from saved:')\n",
    "    with open(saveFileAnchors, 'rb') as f:\n",
    "        anchors = pickle.load(f)    \n",
    "    \n",
    "print(anchors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 21,
     "status": "ok",
     "timestamp": 1638305356293,
     "user": {
      "displayName": "Jill Naiman",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "13328008118965652761"
     },
     "user_tz": 360
    },
    "id": "Z1G0r3vYnea0",
    "outputId": "75ac0211-6d6a-4e9a-a368-3b40bd104023"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['figure', 'figure caption', 'math formula', 'table']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LABELS = np.unique(slabels).tolist()\n",
    "CLASS = len(LABELS)\n",
    "##if use_only_one_class: CLASS = 1\n",
    "LABELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "executionInfo": {
     "elapsed": 20,
     "status": "ok",
     "timestamp": 1638305356294,
     "user": {
      "displayName": "Jill Naiman",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "13328008118965652761"
     },
     "user_tz": 360
    },
    "id": "yZJXzsfVnea2"
   },
   "outputs": [],
   "source": [
    "# strings to integers\n",
    "labels = np.arange(0,len(LABELS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 20,
     "status": "ok",
     "timestamp": 1638305356294,
     "user": {
      "displayName": "Jill Naiman",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "13328008118965652761"
     },
     "user_tz": 360
    },
    "id": "0R42fLapLPI-",
    "outputId": "88612ccb-2478-4462-dd44-710d55e35142"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1, 2, 3]), ['figure', 'figure caption', 'math formula', 'table'])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels, LABELS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6wKLOv6xBb6D"
   },
   "source": [
    "Create splits one way or the other.  Using the function makes sure the classes are evenly split as there can be un-even classes (for example, there might be way more figures/figure captions than tables).  Note: each instance is tagged as having one class but this is just the most frequent type on that page -- pages can sometimes have multiple types.\n",
    "\n",
    "Or, load from previous file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 438
    },
    "executionInfo": {
     "elapsed": 315,
     "status": "ok",
     "timestamp": 1638305356591,
     "user": {
      "displayName": "Jill Naiman",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "13328008118965652761"
     },
     "user_tz": 360
    },
    "id": "nIG98zeHnea6",
    "outputId": "faf8c608-e844-4503-92da-12d5c5cc2b52"
   },
   "outputs": [],
   "source": [
    "# # splits\n",
    "# X_train = np.loadtxt(splitsDir + 'train.csv', dtype=str, delimiter=',')\n",
    "# X_test = np.loadtxt(splitsDir + 'test.csv', dtype=str, delimiter=',')\n",
    "# X_valid = np.loadtxt(splitsDir + 'valid.csv', dtype=str, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mega_yolo_utils import augmentation_generator, csv_gen\n",
    "onGoogle=False\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import general_utils\n",
    "from importlib import reload\n",
    "reload(general_utils)\n",
    "from general_utils import parse_annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitsDirIn = None\n",
    "if onGoogle: splitsDirIn = classDirMain\n",
    "train_gen_csv = csv_gen('train', splitsDir=splitsDirIn)\n",
    "valid_gen_csv = csv_gen('valid',splitsDir=splitsDirIn)\n",
    "test_gen_csv = csv_gen('test',splitsDir=splitsDirIn)\n",
    "\n",
    "class ArtificialDatasetNonAug(tf.data.Dataset):\n",
    "\n",
    "    #@tf.function\n",
    "    def _dataset_gen(split, batch_size, method='npz'):\n",
    "        while True:\n",
    "            true_boxes = []; imgs = []; files = []\n",
    "            #time1 = time.perf_counter()\n",
    "\n",
    "            while len(files) < batch_size:\n",
    "                if type(split) == str:\n",
    "                    if b'train' in bytes(split, encoding='utf8'): # NOTE, must be bytes here!!!\n",
    "                        line = next(train_gen_csv)\n",
    "                    elif b'valid' in bytes(split, encoding='utf8'):\n",
    "                        line = next(valid_gen_csv)\n",
    "                    elif b'test' in bytes(split, encoding='utf8'):\n",
    "                        line = next(test_gen_csv)\n",
    "                else:\n",
    "                    if b'train' in split: # NOTE, must be bytes here!!!\n",
    "                        line = next(train_gen_csv)\n",
    "                    elif b'valid' in split:\n",
    "                        line = next(valid_gen_csv)\n",
    "                    elif b'test' in split:\n",
    "                        line = next(test_gen_csv)\n",
    "\n",
    "                files.append(line.strip())\n",
    "\n",
    "            # parse and get full names\n",
    "            #time2 = time.perf_counter()\n",
    "            #print('parse CSV:', time2-time1)\n",
    "            try:\n",
    "                imgs_name, bbox = parse_annotation(files, LABELS, \n",
    "                                                   feature_dir=classDir_main_to_imgs,\n",
    "                                                   annotation_dir=classDir_main_to)\n",
    "            except:\n",
    "                print('error parsing:', imgs_name)\n",
    "                \n",
    "            #print('method is ', method)\n",
    "            if method == bytes('npz', encoding='utf8'): ender = '.npz'\n",
    "            if method == bytes('npy', encoding='utf8'): ender = '.npy'\n",
    "            if method == bytes('pickle', encoding='utf8'): ender = '.pickle'\n",
    "            #print(imgs_name)\n",
    "            # do a debug check\n",
    "            for im in imgs_name:\n",
    "                #print(im)\n",
    "                if ender not in im:\n",
    "                    print('no np file, no pickle file, no npy file')\n",
    "                    import sys; sys.exit()\n",
    "                    \n",
    "            #time3 = time.perf_counter()\n",
    "            #print('Parse annotation:',time3-time2)\n",
    "\n",
    "            # read in and keep images -- npy files/npz\n",
    "            if method == bytes('npz', encoding='utf8'):\n",
    "                for im in imgs_name:\n",
    "                    with np.load(im) as b:\n",
    "                        #b = b['arr_0']/255.0\n",
    "                        imgs.append(b['arr_0']/255.0)\n",
    "            elif method == bytes('npy', encoding='utf8'):\n",
    "                for im in imgs_name:\n",
    "                    b=np.load(im)\n",
    "                    if type(b) != np.ndarray:\n",
    "                        b = b['arr_0'] # WHY????\n",
    "                    imgs.append(b/255.0)\n",
    "            elif method == bytes('pickle', encoding='utf8'):\n",
    "                for im in imgs_name:\n",
    "                    with open(im, 'rb') as f:\n",
    "                        b = pickle.load(f) \n",
    "                        imgs.append(np.array(b)/255.0)\n",
    "            else:\n",
    "                print('no idea what this method is')\n",
    "                        \n",
    "                \n",
    "            #time4 = time.perf_counter()\n",
    "            #print('Load data:', time4-time3)\n",
    "\n",
    "            # finally, format for output\n",
    "            y_true1, y_true2, y_true3 = [],[],[]\n",
    "            for b in bbox:\n",
    "                y1,y2,y3= process_box(b[:,:4], b[:,4].astype('int'),anchors,CLASS)\n",
    "                y_true1.append(y1); y_true2.append(y2); y_true3.append(y3)\n",
    "            # if there is no box, do something different\n",
    "            if len(bbox) == 0:\n",
    "                # fake a box\n",
    "                b = np.array([[111.,  59., 403., 364. ,  4.]])\n",
    "                y1,y2,y3= process_box(b[:,:4], b[:,4].astype('int'),anchors,CLASS)\n",
    "                y1[:] = 0; y2[:]=0;y3[:]=0\n",
    "                y_true1.append(y1); y_true2.append(y2); y_true3.append(y3)        \n",
    "            img = tf.cast(np.array(imgs), tf.float32)     \n",
    "            #time5 = time.perf_counter()\n",
    "            #print('process blocks:', time5-time4)\n",
    "            del imgs\n",
    "            yield img, tf.cast(y_true1, tf.float32), tf.cast(y_true2, tf.float32), tf.cast(y_true3, tf.float32)\n",
    "\n",
    "\n",
    "\n",
    "    def __new__(cls,batch_size=32,split='valid', method='npz'):\n",
    "        dataset = tf.data.Dataset.from_generator(cls._dataset_gen, args=[split, batch_size,method],\n",
    "                                             output_types = (tf.float32, tf.float32, \n",
    "                                                             tf.float32, tf.float32))\n",
    "        return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs_name, bbox = parse_annotation([annotations[0]], LABELS, \n",
    "                                                   feature_dir=classDir_main_to_imgs,\n",
    "                                                   annotation_dir=classDir_main_to)\n",
    "\n",
    "for im in imgs_name:\n",
    "    arr = np.load(im)['arr_0']\n",
    "    \n",
    "#features_dataset = tf.data.Dataset.from_tensor_slices((feature0, feature1, feature2, feature3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bbox, arr\n",
    "arr.shape\n",
    "arr.reshape(1,arr.shape[0]*arr.shape[1]*arr.shape[2]).shape\n",
    "bbox[0].shape\n",
    "\n",
    "# maxboxes stuff\n",
    "maxboxes = 20\n",
    "\n",
    "boxout = np.zeros([maxboxes+1,5])\n",
    "boxout[:bbox[0].shape[0],:] = bbox[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "## start here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mInit signature:\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatList\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m/\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m      A ProtocolMessage\n",
       "\u001b[0;31mFile:\u001b[0m           ~/anaconda3/envs/Paper1/lib/python3.7/site-packages/tensorflow/core/example/feature_pb2.py\n",
       "\u001b[0;31mType:\u001b[0m           GeneratedProtocolMessageType\n",
       "\u001b[0;31mSubclasses:\u001b[0m     \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tf.train.FloatList?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _bytes_feature(value):\n",
    "    \"\"\"Returns a bytes_list from a string / byte.\"\"\"\n",
    "    if isinstance(value, type(tf.constant(0))):\n",
    "        value = value.numpy() # BytesList won't unpack a string from an EagerTensor.\n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "\n",
    "def _float_feature(value):\n",
    "    \"\"\"Returns a float_list from a float / double.\"\"\"\n",
    "    return tf.train.Feature(float_list=tf.train.FloatList(value=[value]))\n",
    "\n",
    "def _int64_feature(value):\n",
    "    \"\"\"Returns an int64_list from a bool / enum / int / uint.\"\"\"\n",
    "    return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n",
    "\n",
    "# def _float32_feature(value):\n",
    "#     \"\"\"Returns an int64_list from a bool / enum / int / uint.\"\"\"\n",
    "#     return tf.train.Feature(float_list=tf.train.Float32List(value=[value]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary with features that may be relevant.\n",
    "def image_example(image, boxes,img_name):\n",
    "    #image_shape = tf.io.decode_jpeg(image_string).shape\n",
    "    image_string = image.astype('float32')/255.0\n",
    "    image_string = image.reshape(image.shape[0]*image.shape[1]*image.shape[2])\n",
    "    \n",
    "    nfeatures = image.shape[2]\n",
    "    nboxes = boxes.shape[0]\n",
    "    if nboxes>0:\n",
    "        boxout = boxes.reshape(boxes.shape[0]*boxes.shape[1])\n",
    "    else:\n",
    "        boxout = np.array([])\n",
    "    \n",
    "    feature = {\n",
    "      #'nbox': _int64_feature(np.int64(nboxes)),\n",
    "      #'nfeatures': _int64_feature(np.int64(nfeatures)),\n",
    "      'nbox': _float_feature(np.float32(nboxes)),\n",
    "      'nfeatures': _float_feature(np.float32(nfeatures)),\n",
    "      'boxes': _bytes_feature(boxout.astype('float32').tobytes()),\n",
    "      'image_raw': _bytes_feature(image_string.astype('float32').tobytes()),\n",
    "      'image_name': _bytes_feature(img_name.astype('float32').tobytes()),\n",
    "    }\n",
    "\n",
    "    return tf.train.Example(features=tf.train.Features(feature=feature))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many TFRecord files to have? So that each are $\\sim$100Mb: https://docs.w3cub.com/tensorflow~guide/performance/performance_guide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['/Users/jillnaiman/tmpMegaYolo/binaries/yolo_512x512_cap_ann/1952ApJ___115___82K_p1.xml',\n",
       "       '/Users/jillnaiman/tmpMegaYolo/binaries/yolo_512x512_cap_ann/1994ApJ___427L__43F_p4.xml',\n",
       "       '/Users/jillnaiman/tmpMegaYolo/binaries/yolo_512x512_cap_ann/1995ApJ___445__246C_p6.xml',\n",
       "       ...,\n",
       "       '/Users/jillnaiman/tmpMegaYolo/binaries/yolo_512x512_cap_ann/1978ApJ___220___14H_p1.xml',\n",
       "       '/Users/jillnaiman/tmpMegaYolo/binaries/yolo_512x512_cap_ann/1982ApJ___252__269Y_p2.xml',\n",
       "       '/Users/jillnaiman/tmpMegaYolo/binaries/yolo_512x512_cap_ann/1982ApJ___258__467Y_p2.xml'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filelist = pd.read_csv(config.tmp_storage_dir+'train.csv',names=['filename'])['filename'].values\n",
    "filelist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/Users/jillnaiman/MegaYolo/binaries_model8/',\n",
       " '/Users/jillnaiman/MegaYolo/yolo_512x512_ann/')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classDir_main_to_imgs, classDir_main_to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "random(size=None)\n",
       "\n",
       "Return random floats in the half-open interval [0.0, 1.0). Alias for\n",
       "`random_sample` to ease forward-porting to the new random API.\n",
       "\u001b[0;31mType:\u001b[0m      builtin_function_or_method\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "np.random.random?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 5)"
      ]
     },
     "execution_count": 312,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.random([maxboxes,5]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['figure', 'figure caption', 'math formula', 'table']"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#np.savetxt(splitsDir + 'LABELS.csv', X_train, fmt='%s', delimiter=',')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no file /Users/jillnaiman/MegaYolo/yolo_512x512_ann/1992AJ____103_1151C_p9.xml\n",
      "no file /Users/jillnaiman/MegaYolo/yolo_512x512_ann/1991ApJS___76__455E_p6.xml\n",
      "no file /Users/jillnaiman/MegaYolo/yolo_512x512_ann/1990ApJ___365__510C_p2.xml\n",
      "no file /Users/jillnaiman/MegaYolo/yolo_512x512_ann/1996ApJ___467__887H_p4.xml\n",
      "no file /Users/jillnaiman/MegaYolo/yolo_512x512_ann/1994ApJS___95__457W_p15.xml\n"
     ]
    }
   ],
   "source": [
    "# write one image file and see how big it is\n",
    "record_file = config.tmp_storage_dir+'test.tfrecords'\n",
    "# what is our max boxes\n",
    "maxboxes = -1\n",
    "for a in filelist:\n",
    "    a = classDir_main_to + a.split('/')[-1]\n",
    "    try:\n",
    "        imgs_name, bbox = parse_annotation([a], LABELS,\n",
    "                                           feature_dir=classDir_main_to_imgs,\n",
    "                                           annotation_dir=classDir_main_to) \n",
    "    except:\n",
    "        print('no file', a)\n",
    "    if len(bbox) > 0:\n",
    "        maxboxes = max([maxboxes,len(bbox[0])])\n",
    "    \n",
    "# print with maxboxes\n",
    "with tf.io.TFRecordWriter(record_file) as writer:\n",
    "    \n",
    "    a = classDir_main_to + filelist[0].split('/')[-1]\n",
    "    imgs_name, bbox = parse_annotation([a], LABELS,\n",
    "                                           feature_dir=classDir_main_to_imgs,\n",
    "                                           annotation_dir=classDir_main_to) \n",
    "    # fake boxes\n",
    "    fakebox = np.random.random([maxboxes,5])\n",
    "    tf_example = image_example(arr,fakebox,imgs_name[0])\n",
    "    writer.write(tf_example.SerializeToString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/t5/9xgccmv92hnfvjwd62mk8zqh0000gn/T/ipykernel_45883/1373489018.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfilesize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetsize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecord_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mfilesize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilesize\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100.\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m1e6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "filesize = os.path.getsize(record_file)\n",
    "filesize, filesize/(100.*1e6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.0"
      ]
     },
     "execution_count": 452,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#i.e we want:\n",
    "nfiles_per_file = 100*1e6//filesize\n",
    "nfiles_per_file # per recordio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "592"
      ]
     },
     "execution_count": 453,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nfiles = int(np.ceil(len(filelist)*1.0/nfiles_per_file))\n",
    "nfiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "on 0 of 592\n",
      "on 50 of 592\n",
      "on 100 of 592\n",
      "on 150 of 592\n",
      "cant find /Users/jillnaiman/MegaYolo/yolo_512x512_ann/1992AJ____103_1151C_p9.xml , moving on...\n",
      "on 200 of 592\n",
      "on 250 of 592\n",
      "cant find /Users/jillnaiman/MegaYolo/yolo_512x512_ann/1991ApJS___76__455E_p6.xml , moving on...\n",
      "cant find /Users/jillnaiman/MegaYolo/yolo_512x512_ann/1990ApJ___365__510C_p2.xml , moving on...\n",
      "on 300 of 592\n",
      "on 350 of 592\n",
      "cant find /Users/jillnaiman/MegaYolo/yolo_512x512_ann/1996ApJ___467__887H_p4.xml , moving on...\n",
      "on 400 of 592\n",
      "cant find /Users/jillnaiman/MegaYolo/yolo_512x512_ann/1994ApJS___95__457W_p15.xml , moving on...\n",
      "on 450 of 592\n",
      "on 500 of 592\n",
      "on 550 of 592\n"
     ]
    }
   ],
   "source": [
    "# write some training\n",
    "\n",
    "itrain = 0\n",
    "#iloop = 0\n",
    "#record_file = '/Users/jillnaiman/Downloads/tmp/rio/train'\n",
    "#record_file = '/Users/jillnaiman/Downloads/tmp/rio/train_{}.tfrecords'\n",
    "record_file = '/Users/jillnaiman/MegaYolo/binaries_model8_tfrecordz/train_{}.tfrecords'\n",
    "\n",
    "itotalLoop = 0\n",
    "for index in range(nfiles):\n",
    "    if index%50 == 0: print('on', index,'of',nfiles)\n",
    "    with tf.io.TFRecordWriter(record_file.format(index)) as writer:\n",
    "        #print(index*int(nfiles_per_file),min([(index+1)*int(nfiles_per_file),len(filelist)]))\n",
    "        for iloop,a in enumerate(filelist[index*int(nfiles_per_file):min([(index+1)*int(nfiles_per_file),len(filelist)])]):\n",
    "            #print(iloop,a)\n",
    "            a = classDir_main_to + a.split('/')[-1]\n",
    "            \n",
    "            try:\n",
    "                imgs_name, bbox = parse_annotation([a], LABELS,\n",
    "                                                   feature_dir=classDir_main_to_imgs,\n",
    "                                                   annotation_dir=classDir_main_to)\n",
    "            except:\n",
    "                print('cant find', a, ', moving on...')\n",
    "                continue\n",
    "            arr = np.load(imgs_name[0])['arr_0']\n",
    "\n",
    "            if len(bbox) > 0: \n",
    "                bbox = np.array(bbox[0])\n",
    "            else:\n",
    "                bbox = np.array([])\n",
    "            tf_example = image_example(arr,bbox,imgs_name[0])\n",
    "            writer.write(tf_example.SerializeToString())\n",
    "\n",
    "            # if itotalLoop%int(nfiles_per_file)==0: \n",
    "            #     print(index,iloop,itrain)\n",
    "            #     itrain+=1\n",
    "            # itotalLoop += 1\n",
    "\n",
    "    #if index == 2: import sys; sys.exit()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "***HERE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list_of_records = glob.glob('/Users/jillnaiman/Downloads/tmp/rio/train_*.tfrecords')\n",
    "#list_of_records = glob.glob('/Users/jillnaiman/Downloads/tmp/rio/train_*.tfrecords')\n",
    "list_of_records = glob.glob('/Users/jillnaiman/MegaYolo/binaries_model8_tfrecordz/train_*.tfrecords')\n",
    "#list_of_records\n",
    "buffer_size = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "#raw_image_dataset = tf.data.TFRecordDataset(list_of_records)\n",
    "raw_image_dataset = tf.data.TFRecordDataset(filenames=list_of_records, compression_type='GZIP', buffer_size=buffer_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary describing the features.\n",
    "image_feature_description = {\n",
    "    #'nbox': tf.io.FixedLenFeature([], tf.int64),\n",
    "    #'nfeatures': tf.io.FixedLenFeature([], tf.int64),\n",
    "    'nbox': tf.io.FixedLenFeature([], tf.float32),\n",
    "    'nfeatures': tf.io.FixedLenFeature([], tf.float32),\n",
    "    'boxes': tf.io.FixedLenFeature([], tf.string),\n",
    "    'image_raw': tf.io.FixedLenFeature([], tf.string),\n",
    "    'image_name': tf.io.FixedLenFeature([], tf.string),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[195.,  21.],\n",
       "       [203.,   7.],\n",
       "       [ 51.,   5.],\n",
       "       [313., 199.],\n",
       "       [359., 391.],\n",
       "       [435.,  16.],\n",
       "       [399., 307.],\n",
       "       [204., 114.],\n",
       "       [ 15., 355.]])"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anchors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "#image_size\n",
    "#also: https://github.com/tensorflow/tensorflow/blob/r1.3/tensorflow/examples/how_tos/reading_data/convert_to_records.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#@tf.function\n",
    "def process_box2(boxes, labels,anchors,CLASS,image_size=config.IMAGE_H):\n",
    "    '''\n",
    "    labels: integer numbers associated with labels, JPN: does this start at 0 or 1??\n",
    "    boxes: [number of boxes on page, xmin, ymin, xmax, ymax]\n",
    "    returns: the correctly formatted 3 y-trues\n",
    "    '''\n",
    "    anchors_mask = [[6, 7, 8], [3, 4, 5], [0, 1, 2]]\n",
    "    #anchors = anchors\n",
    "    box_centers = (boxes[:, 0:2] + boxes[:, 2:4]) / 2\n",
    "    box_size = boxes[:, 2:4] - boxes[:, 0:2]\n",
    "    #print(box_size)\n",
    "    \n",
    "    # y_true_1 = tf.zeros((tf.math.floordiv(image_size,32),\n",
    "    #                         tf.math.floordiv(image_size,32),\n",
    "    #                         3, 5 + CLASS), tf.float32)\n",
    "    # y_true_2 = tf.zeros((tf.math.floordiv(image_size,16),\n",
    "    #                         tf.math.floordiv(image_size,16),\n",
    "    #                         3, 5 + CLASS), tf.float32)\n",
    "    # y_true_3 = tf.zeros((tf.math.floordiv(image_size,8),\n",
    "    #                         tf.math.floordiv(image_size,8),\n",
    "    #                         3, 5 + CLASS), tf.float32)\n",
    "    #print('hi')\n",
    "\n",
    "    y_true_1 = np.zeros((image_size // 32,\n",
    "                            image_size // 32,\n",
    "                            3, 5 + CLASS.numpy()), np.float32)\n",
    "    y_true_2 = np.zeros((image_size // 16,\n",
    "                            image_size // 16,\n",
    "                            3, 5 + CLASS.numpy()), np.float32)\n",
    "    y_true_3 = np.zeros((image_size // 8,\n",
    "                            image_size // 8,\n",
    "                            3, 5 + CLASS.numpy()), np.float32)\n",
    "    # y_true_1 = np.zeros((tf.math.floordiv(image_size,32),\n",
    "    #                         tf.math.floordiv(image_size,32),\n",
    "    #                         3, 5 + CLASS.numpy()), np.float32)\n",
    "    # y_true_2 = np.zeros((tf.math.floordiv(image_size,16),\n",
    "    #                         tf.math.floordiv(image_size,16),\n",
    "    #                         3, 5 + CLASS), np.float32)\n",
    "    # y_true_3 = np.zeros((tf.math.floordiv(image_size,8),\n",
    "    #                         tf.math.floordiv(image_size,8),\n",
    "    #                         3, 5 + CLASS), np.float32)\n",
    "    y_true = [y_true_1, y_true_2, y_true_3]\n",
    "\n",
    "    box_size = tf.expand_dims(box_size, 1)\n",
    "\n",
    "    min_np = tf.maximum(- box_size / 2, - anchors / 2)\n",
    "    max_np = tf.minimum(box_size / 2, anchors / 2)\n",
    "\n",
    "    whs = max_np - min_np\n",
    "\n",
    "    overlap = whs[:, :, 0] * whs[:, :, 1]\n",
    "    union = box_size[:, :, 0] * box_size[:, :, 1] + anchors[:, 0] * anchors[:, 1] - whs[:, :, 0] * whs[:, :, 1] + 1e-10\n",
    "\n",
    "    iou = overlap / union\n",
    "    best_match_idx = tf.argmax(iou, axis=1)\n",
    "\n",
    "    #ratio_dict = {1.: 8., 2.: 16., 3.: 32.}\n",
    "    ratio_dict = tf.constant([8,16,32],tf.int32)\n",
    "    #ratio = -1\n",
    "    #for i, idx in enumerate(best_match_idx):\n",
    "    for i in tf.range(tf.shape(best_match_idx)[0]):\n",
    "        idx = best_match_idx[i]\n",
    "        feature_map_group = 2 - idx // 3\n",
    "        ind = tf.cast(tf.math.ceil((tf.cast(idx + 1,tf.float32)) / 3.),tf.int32)\n",
    "        #ratio = ratio_dict[np.ceil((idx + 1) / 3.)]\n",
    "        ratio = tf.slice(ratio_dict,[ind],[1])\n",
    "        x = int(tf.math.floor(box_centers[i, 0] / tf.cast(ratio,tf.float32)))\n",
    "        y = int(tf.math.floor(box_centers[i, 1] / tf.cast(ratio,tf.float32)))\n",
    "        #k = anchors_mask[feature_map_group].index(idx)\n",
    "        m1 = tf.gather(anchors_mask,feature_map_group)\n",
    "        k = tf.gather(m1,idx)\n",
    "        #k = tf.gather(anchors_mask[feature_map_group],idx)\n",
    "        c = labels[i]\n",
    "\n",
    "        y_true[feature_map_group][y, x, k, :2] = box_centers[i]\n",
    "        # y_true = y_true[feature_map_group][y, x, k, :2].assign(box_centers[i])\n",
    "        y_true[feature_map_group][y, x, k, 2:4] = box_size[i]\n",
    "        y_true[feature_map_group][y, x, k, 4] = 1.\n",
    "        # if feature_map_group == 0:\n",
    "        #     yt2 = np\n",
    "        #     y_true_11[y,x,k,:2] = box_centers[i]\n",
    "        try:\n",
    "            #y_true[feature_map_group][y, x, k, 5 + c] = 1.\n",
    "            y_true[feature_map_group][y, x, k, 5 + c -1] = 1. # labels start at 0\n",
    "        except:\n",
    "            print('in parse')\n",
    "            print(y,x,k,c, 5+c)\n",
    "            print(labels)\n",
    "\n",
    "    return y_true_1, y_true_2, y_true_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#@tf.function\n",
    "def process_box3(boxes, labels,anchors,CLASS):\n",
    "    '''\n",
    "    labels: integer numbers associated with labels, JPN: does this start at 0 or 1??\n",
    "    boxes: [number of boxes on page, xmin, ymin, xmax, ymax]\n",
    "    returns: the correctly formatted 3 y-trues\n",
    "    '''\n",
    "    anchors_mask = [[6, 7, 8], [3, 4, 5], [0, 1, 2]]\n",
    "    #anchors = anchors\n",
    "    box_centers = (boxes[:, 0:2] + boxes[:, 2:4]) / 2\n",
    "    box_size = boxes[:, 2:4] - boxes[:, 0:2]\n",
    "\n",
    "    y_true_1 = np.zeros((image_size // 32,\n",
    "                            image_size // 32,\n",
    "                            3, 5 + CLASS), np.float32)\n",
    "    y_true_2 = np.zeros((image_size // 16,\n",
    "                            image_size // 16,\n",
    "                            3, 5 + CLASS), np.float32)\n",
    "    y_true_3 = np.zeros((image_size // 8,\n",
    "                            image_size // 8,\n",
    "                            3, 5 + CLASS), np.float32)\n",
    "\n",
    "    y_true = [y_true_1, y_true_2, y_true_3]\n",
    "\n",
    "    box_size = np.expand_dims(box_size, 1)\n",
    "\n",
    "    min_np = np.maximum(- box_size / 2, - anchors / 2)\n",
    "    max_np = np.minimum(box_size / 2, anchors / 2)\n",
    "\n",
    "    whs = max_np - min_np\n",
    "\n",
    "    overlap = whs[:, :, 0] * whs[:, :, 1]\n",
    "    union = box_size[:, :, 0] * box_size[:, :, 1] + anchors[:, 0] * anchors[:, 1] - whs[:, :, 0] * whs[:, :, 1] + 1e-10\n",
    "\n",
    "    iou = overlap / union\n",
    "    best_match_idx = np.argmax(iou, axis=1)\n",
    "\n",
    "    ratio_dict = {1.: 8., 2.: 16., 3.: 32.}\n",
    "    for i, idx in enumerate(best_match_idx):\n",
    "        feature_map_group = 2 - idx // 3\n",
    "        ratio = ratio_dict[np.ceil((idx + 1) / 3.)]\n",
    "        x = int(np.floor(box_centers[i, 0] / ratio))\n",
    "        y = int(np.floor(box_centers[i, 1] / ratio))\n",
    "        k = anchors_mask[feature_map_group].index(idx)\n",
    "        c = labels[i].numpy().astype('int')\n",
    "\n",
    "        y_true[feature_map_group][y, x, k, :2] = box_centers[i]\n",
    "        y_true[feature_map_group][y, x, k, 2:4] = box_size[i]\n",
    "        y_true[feature_map_group][y, x, k, 4] = 1.\n",
    "        try:\n",
    "            y_true[feature_map_group][y, x, k, 5 + c -1] = 1. # labels start at 0\n",
    "        except:\n",
    "            print('in parse')\n",
    "            print('y','x','k','c')\n",
    "            print(y,x,k,c)\n",
    "            print(labels.numpy())\n",
    "    #print(y_true_1)\n",
    "\n",
    "    return y_true_1, y_true_2, y_true_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "# debug\n",
    "import mega_yolo_utils\n",
    "from importlib import reload\n",
    "reload(mega_yolo_utils)\n",
    "from mega_yolo_utils import process_box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _parse_image_function(example_proto,anchors,CLASS):\n",
    "    image_features = tf.io.parse_single_example(example_proto, image_feature_description)\n",
    "    #print(image_features)\n",
    "\n",
    "    nboxes = image_features['nbox']\n",
    "    nfeatures = image_features['nfeatures']\n",
    "    boxes = tf.io.decode_raw(image_features['boxes'],tf.float32)\n",
    "    boxes = tf.reshape(boxes,[nboxes,5])\n",
    "    images_raw = image_features['image_raw']\n",
    "    image = tf.io.decode_raw(images_raw,tf.float32)\n",
    "    image = tf.reshape(image,[config.IMAGE_H,config.IMAGE_W,nfeatures])\n",
    "\n",
    "    # process boxes\n",
    "    y1,y2,y3 = tf.py_function(process_box,\n",
    "                              (boxes[:,:4], boxes[:,4],anchors,CLASS),\n",
    "                              (tf.float32,tf.float32,tf.float32))   \n",
    "    \n",
    "    #return image, nboxes, nfeatures, boxes,y1,y2,y3\n",
    "    return image, y1,y2,y3\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<MapDataset shapes: ((512, 512, None), <unknown>, <unknown>, <unknown>), types: (tf.float32, tf.float32, tf.float32, tf.float32)>"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#parsed_image_dataset = raw_image_dataset.map(_parse_image_function)\n",
    "anchors_in = tf.constant(anchors,tf.float32)\n",
    "CLASS_in = tf.constant(CLASS,tf.int32)\n",
    "parsed_image_dataset = raw_image_dataset.map(lambda example_proto:_parse_image_function(example_proto,anchors,CLASS))\n",
    "parsed_image_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "#boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for checking:\n",
    "import IPython.display as display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for image,y1,y2,y3 in parsed_image_dataset.take(1):\n",
    "    # nboxes = image_features['nbox']\n",
    "    # nfeatures = image_features['nfeatures']\n",
    "    # boxes = tf.io.decode_raw(image_features['boxes'],tf.float32)\n",
    "    # boxes = tf.reshape(boxes,[nboxes,5])\n",
    "    # images_raw = image_features['image_raw']\n",
    "    # image = tf.io.decode_raw(images_raw,tf.float32)\n",
    "    # image = tf.reshape(image,[config.IMAGE_H,config.IMAGE_W,nfeatures])\n",
    "    imgPlot = image.numpy()\n",
    "    plt.imshow(imgPlot[:,:,0],cmap='gray')\n",
    "    #print(nboxes,nfeatures,boxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "# benchmark tests\n",
    "import time\n",
    "def benchmark(dataset, num_epochs=1, nbatch=32):\n",
    "    start_time = time.perf_counter()\n",
    "    for epoch_num in range(num_epochs):\n",
    "        ienum=0\n",
    "        for sample in dataset:\n",
    "            #if ienum%10==0: print(ienum)\n",
    "            #print(ienum)\n",
    "            # Performing a training step\n",
    "            time.sleep(0.01)\n",
    "            ienum+=1\n",
    "            if ienum>nbatch: break\n",
    "    print(\"Execution time:\", time.perf_counter() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time: 0.685586901003262\n",
      "Execution time: 0.6815236510010436\n",
      "Execution time: 0.6834734799922444\n",
      "Execution time: 0.645947023993358\n",
      "Execution time: 0.6645191499992507\n",
      "Execution time: 0.6825437999941641\n",
      "Execution time: 0.6579061590018682\n",
      "Execution time: 0.6619017500051996\n",
      "Execution time: 0.6777665850095218\n",
      "Execution time: 0.6790914300072473\n"
     ]
    }
   ],
   "source": [
    "# try different methods and time\n",
    "nbatch = 10; nloop = 10\n",
    "#nbatch = 25; nloop = 4\n",
    "th = []\n",
    "\n",
    "for n in range(nloop):\n",
    "    t1 = time.perf_counter()\n",
    "    benchmark(parsed_image_dataset,\n",
    "              nbatch=nbatch)\n",
    "    t2 = time.perf_counter()\n",
    "    th.append(t2-t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.6786171035055304, 0.012802154496212498)"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.median(th), np.std(th)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a description of the features.\n",
    "feature_description = {\n",
    "    'feature0': tf.io.FixedLenFeature([], tf.int64, default_value=0),\n",
    "    'feature1': tf.io.FixedLenFeature([], tf.int64, default_value=0),\n",
    "    'feature2': tf.io.FixedLenFeature([], tf.string, default_value=''),\n",
    "    'feature3': tf.io.FixedLenFeature([], tf.float32, default_value=0.0),\n",
    "}\n",
    "\n",
    "def _parse_function(example_proto):\n",
    "  # Parse the input `tf.train.Example` proto using the dictionary above.\n",
    "  return tf.io.parse_single_example(example_proto, feature_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [],
   "source": [
    "def serialize_example(nbox,nfeatures,boxes,image_raw):\n",
    "    \"\"\"\n",
    "    Creates a tf.train.Example message ready to be written to a file.\n",
    "    \"\"\"\n",
    "    # Create a dictionary mapping the feature name to the tf.train.Example-compatible\n",
    "    # data type.\n",
    "    feature = {\n",
    "      'nbox': _int64_feature(nbox),\n",
    "      'nfeatures': _int64_feature(nfeatures),\n",
    "      'boxes': _bytes_feature(boxes),\n",
    "      'image_raw': _bytes_feature(image_raw),\n",
    "    }\n",
    "\n",
    "    # Create a Features message using tf.train.Example.\n",
    "\n",
    "    example_proto = tf.train.Example(features=tf.train.Features(feature=feature))\n",
    "    return example_proto.SerializeToString()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bytes_list {\n",
       "  value: \"\"\n",
       "}"
      ]
     },
     "execution_count": 322,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to read\n",
    "def parse_proto(example_proto):\n",
    "  features = {\n",
    "    'X': tf.FixedLenFeature((345,), tf.float32),\n",
    "    'y': tf.FixedLenFeature((5,), tf.float32),\n",
    "  }\n",
    "  parsed_features = tf.parse_single_example(example_proto, features)\n",
    "  return parsed_features['X'], parsed_features['y']\n",
    "\n",
    "def read_tfrecords(file_names=(\"file1.tfrecord\", \"file2.tfrecord\", \"file3.tfrecord\"),\n",
    "                   buffer_size=10000,\n",
    "                   batch_size=100):\n",
    "    dataset = tf.contrib.data.TFRecordDataset(file_names)\n",
    "    dataset = dataset.map(parse_proto)\n",
    "    dataset = dataset.shuffle(buffer_size)\n",
    "    dataset = dataset.repeat()\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    return tf.contrib.data.Iterator.from_structure(dataset.output_types, dataset.output_shapes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'serialized_example' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/t5/9xgccmv92hnfvjwd62mk8zqh0000gn/T/ipykernel_25922/2583518312.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mexample_proto\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExample\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFromString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mserialized_example\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'serialized_example' is not defined"
     ]
    }
   ],
   "source": [
    "example_proto = tf.train.Example.FromString(serialized_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5,)"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bbox[0].reshape(1*5).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 5)"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bbox[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_dataset = tf.data.Dataset.from_tensor_slices((arr.reshape(1,arr.shape[0]*arr.shape[1]*arr.shape[2]),\n",
    "                                                       bbox[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TensorSliceDataset shapes: ((3145728,), (5,)), types: (tf.uint8, tf.float64)>"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([0 0 0 ... 0 0 0], shape=(3145728,), dtype=uint8)\n",
      "tf.Tensor([ 46. 451. 257. 473.   2.], shape=(5,), dtype=float64)\n"
     ]
    }
   ],
   "source": [
    "for f0,f1 in features_dataset.take(1):\n",
    "    print(f0)\n",
    "    print(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _float_feature(value):\n",
    "    \"\"\"Returns a float_list from a float / double.\"\"\"\n",
    "    return tf.train.Feature(float_list=tf.train.FloatList(value=[value]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "def serialize_example(feature0, feature1):\n",
    "    \"\"\"\n",
    "    Creates a tf.train.Example message ready to be written to a file.\n",
    "    \"\"\"\n",
    "    # Create a dictionary mapping the feature name to the tf.train.Example-compatible\n",
    "    # data type.\n",
    "    feature = {\n",
    "      'feature0': _float_feature(feature0),\n",
    "      'feature1': _float_feature(feature1),\n",
    "    }\n",
    "\n",
    "    # Create a Features message using tf.train.Example.\n",
    "\n",
    "    example_proto = tf.train.Example(features=tf.train.Features(feature=feature))\n",
    "    return example_proto.SerializeToString()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_serialize_example(f0,f1):\n",
    "    tf_string = tf.py_function(\n",
    "        serialize_example,\n",
    "        (f0, f1),  # Pass these args to the above function.\n",
    "        tf.string)      # The return type is `tf.string`.\n",
    "    return tf.reshape(tf_string, ()) # The result is a scalar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([0. 0. 0. ... 0. 0. 0.], shape=(3145728,), dtype=float64) tf.Tensor(\n",
      "[ 46. 451. 257. 473.   2.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.], shape=(105,), dtype=float64)\n"
     ]
    }
   ],
   "source": [
    "#tf.cast(f0,tf.float32)\n",
    "features_dataset\n",
    "for f0,f1 in features_dataset.take(1):\n",
    "    print(f0,f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "array([[0., 0., 0., ..., 0., 0., 0.]]) has type numpy.ndarray, but expected one of: int, long, float",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/t5/9xgccmv92hnfvjwd62mk8zqh0000gn/T/ipykernel_25922/4247669254.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#tf_serialize_example(f0,f1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mserialize_example\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf0np\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mf1np\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/t5/9xgccmv92hnfvjwd62mk8zqh0000gn/T/ipykernel_25922/1118630037.py\u001b[0m in \u001b[0;36mserialize_example\u001b[0;34m(feature0, feature1)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;31m# data type.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     feature = {\n\u001b[0;32m----> 8\u001b[0;31m       \u001b[0;34m'feature0'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0m_float_feature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m       \u001b[0;34m'feature1'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0m_float_feature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     }\n",
      "\u001b[0;32m/var/folders/t5/9xgccmv92hnfvjwd62mk8zqh0000gn/T/ipykernel_25922/2948262194.py\u001b[0m in \u001b[0;36m_float_feature\u001b[0;34m(value)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_float_feature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;34m\"\"\"Returns a float_list from a float / double.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFeature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfloat_list\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatList\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: array([[0., 0., 0., ..., 0., 0., 0.]]) has type numpy.ndarray, but expected one of: int, long, float"
     ]
    }
   ],
   "source": [
    "#tf_serialize_example(f0,f1)\n",
    "\n",
    "serialize_example(f0np,f1np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Dimensions 3145728 and 105 are not compatible",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/t5/9xgccmv92hnfvjwd62mk8zqh0000gn/T/ipykernel_25922/3470775508.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mf1np\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mboxout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mboxout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mboxout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mfeatures_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_tensor_slices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf0np\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mf1np\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mf1\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfeatures_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/Paper1/lib/python3.7/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36mfrom_tensor_slices\u001b[0;34m(tensors)\u001b[0m\n\u001b[1;32m    689\u001b[0m       \u001b[0mDataset\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mA\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m     \"\"\"\n\u001b[0;32m--> 691\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mTensorSliceDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    692\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    693\u001b[0m   \u001b[0;32mclass\u001b[0m \u001b[0m_GeneratorState\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/Paper1/lib/python3.7/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, element)\u001b[0m\n\u001b[1;32m   3163\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tensors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3164\u001b[0m       batch_dim.assert_is_compatible_with(tensor_shape.Dimension(\n\u001b[0;32m-> 3165\u001b[0;31m           tensor_shape.dimension_value(t.get_shape()[0])))\n\u001b[0m\u001b[1;32m   3166\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3167\u001b[0m     variant_tensor = gen_dataset_ops.tensor_slice_dataset(\n",
      "\u001b[0;32m~/anaconda3/envs/Paper1/lib/python3.7/site-packages/tensorflow/python/framework/tensor_shape.py\u001b[0m in \u001b[0;36massert_is_compatible_with\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    280\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_compatible_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m       raise ValueError(\"Dimensions %s and %s are not compatible\" %\n\u001b[0;32m--> 282\u001b[0;31m                        (self, other))\n\u001b[0m\u001b[1;32m    283\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mmerge_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Dimensions 3145728 and 105 are not compatible"
     ]
    }
   ],
   "source": [
    "\n",
    "# for each annotation\n",
    "boxout = np.zeros([maxboxes+1,5])\n",
    "boxout[:bbox[0].shape[0],:] = bbox[0]\n",
    "\n",
    "f0np = arr.reshape(arr.shape[0]*arr.shape[1]*arr.shape[2])\n",
    "f0np = f0np.astype('float')/255.\n",
    "f1np = boxout.reshape(boxout.shape[0]*boxout.shape[1])\n",
    "\n",
    "features_dataset = tf.data.Dataset.from_tensor_slices((f0np,f1np))\n",
    "\n",
    "for f0,f1 in features_dataset.take(1):\n",
    "    #print(f0)\n",
    "    #print(f1)\n",
    "    tf_serialize_example(f0,f1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<TensorSliceDataset shapes: (3145728,), types: tf.float64>,\n",
       " <TensorSliceDataset shapes: (105,), types: tf.float64>)"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f0,f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TensorSliceDataset shapes: (21, 5), types: tf.float64>"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # load 1\n",
    "# imgs_name, bbox = parse_annotation([annotations[0]], LABELS, \n",
    "#                                                    feature_dir=classDir_main_to_imgs,\n",
    "#                                                    annotation_dir=classDir_main_to)\n",
    "\n",
    "\n",
    "# # read in and keep images -- npy files\n",
    "# for im in imgs_name:\n",
    "#     arr = np.load(im)#['arr_0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.load(im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "#arr.astype('uint8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nbatch = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #dataset = ArtificialDatasetNonAug(batch_size=32,split='valid')\n",
    "# b = np.load('/Users/jillnaiman/MegaYolo/binaries_model8_noncom/1988ApJ___334__144K_p8.npy')\n",
    "# b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "#b['arr_0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# benchmark tests\n",
    "import time\n",
    "def benchmark(dataset, num_epochs=1, nbatch=32):\n",
    "    start_time = time.perf_counter()\n",
    "    for epoch_num in range(num_epochs):\n",
    "        ienum=0\n",
    "        for sample in dataset:\n",
    "            #if ienum%10==0: print(ienum)\n",
    "            #print(ienum)\n",
    "            # Performing a training step\n",
    "            time.sleep(0.01)\n",
    "            ienum+=1\n",
    "            if ienum>nbatch: break\n",
    "    print(\"Execution time:\", time.perf_counter() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple\n",
    "#benchmark(ArtificialDatasetNonAug(batch_size=10,split='valid'),nbatch=nbatch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try different methods and time\n",
    "#nbatch = 10; nloop = 10\n",
    "nbatch = 25; nloop = 4\n",
    "th = []\n",
    "\n",
    "for n in range(nloop):\n",
    "    t1 = time.perf_counter()\n",
    "    benchmark(ArtificialDatasetNonAug(batch_size=nbatch,split='valid',method=method),\n",
    "              nbatch=nbatch)\n",
    "    t2 = time.perf_counter()\n",
    "    th.append(t2-t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40.807152897499996, 0.9870036160257788)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.median(th), np.std(th)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/jillnaiman/MegaYolo/binaries_model8_pickle/'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classDir_main_to_imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for tfrecords files\n",
    "def parse_proto(example_proto):\n",
    "    features = {\n",
    "        'X': tf.io.FixedLenFeature([], tf.float32),\n",
    "        'x1': tf.io.FixedLenSequenceFeature([10], tf.float32,allow_missing = True,default_value=0),\n",
    "        'x2': tf.io.FixedLenSequenceFeature([10], tf.float32,allow_missing = True,default_value=0),\n",
    "        'y1': tf.io.FixedLenSequenceFeature([10], tf.float32,allow_missing = True,default_value=0),\n",
    "        'y2': tf.io.FixedLenSequenceFeature([10], tf.float32,allow_missing = True,default_value=0),\n",
    "        'class': tf.io.FixedLenSequenceFeature([10], tf.float32,allow_missing = True,default_value=0),\n",
    "    }\n",
    "    parsed_features = tf.io.parse_single_example(example_proto, features)\n",
    "    print(parsed_features)\n",
    "    return parsed_features['X']\n",
    "#     y_true1, y_true2, y_true3 = [],[],[]\n",
    "#     for b in bbox:\n",
    "#         y1,y2,y3= process_box(b[:,:4], b[:,4].astype('int'),anchors,CLASS)\n",
    "#         y_true1.append(y1); y_true2.append(y2); y_true3.append(y3)\n",
    "#     # if there is no box, do something different\n",
    "#     if len(bbox) == 0:\n",
    "#         # fake a box\n",
    "#         b = np.array([[111.,  59., 403., 364. ,  4.]])\n",
    "#         y1,y2,y3= process_box(b[:,:4], b[:,4].astype('int'),anchors,CLASS)\n",
    "#         y1[:] = 0; y2[:]=0;y3[:]=0\n",
    "#         y_true1.append(y1); y_true2.append(y2); y_true3.append(y3)        \n",
    "#     img = tf.cast(np.array(imgs), tf.float32)     \n",
    "#     #time5 = time.perf_counter()\n",
    "#     #print('process blocks:', time5-time4)\n",
    "#     del imgs\n",
    "#     yield img, tf.cast(y_true1, tf.float32), tf.cast(y_true2, tf.float32), tf.cast(y_true3, tf.float32)\n",
    "\n",
    "    \n",
    "#     return parsed_features['X'], parsed_features['y']\n",
    "\n",
    "def read_tfrecords(file_names=(\"file1.tfrecord\", \"file2.tfrecord\", \"file3.tfrecord\"),\n",
    "                   buffer_size=10000,\n",
    "                   batch_size=100):\n",
    "    dataset = tf.data.TFRecordDataset(file_names)\n",
    "    dataset = dataset.map(parse_proto)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'X': <tf.Tensor 'ParseSingleExample/ParseExample/ParseExampleV2:0' shape=() dtype=float32>, 'class': <tf.Tensor 'ParseSingleExample/ParseExample/ParseExampleV2:1' shape=(None, 10) dtype=float32>, 'x1': <tf.Tensor 'ParseSingleExample/ParseExample/ParseExampleV2:2' shape=(None, 10) dtype=float32>, 'x2': <tf.Tensor 'ParseSingleExample/ParseExample/ParseExampleV2:3' shape=(None, 10) dtype=float32>, 'y1': <tf.Tensor 'ParseSingleExample/ParseExample/ParseExampleV2:4' shape=(None, 10) dtype=float32>, 'y2': <tf.Tensor 'ParseSingleExample/ParseExample/ParseExampleV2:5' shape=(None, 10) dtype=float32>}\n"
     ]
    }
   ],
   "source": [
    "dirtf = '/Users/jillnaiman/MegaYolo/binaries_model8_tfrecord/'\n",
    "tffiles = [dirtf+'1981AJ_____86__206V_p1.tfrecord',\n",
    "           dirtf+'1992AJ____104_2161O_p5.tfrecord',\n",
    "           dirtf+'1997AJ____114__913N_p13.tfrecord']\n",
    "\n",
    "\n",
    "dataset = read_tfrecords(file_names=tffiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-01 18:31:54.646748: W tensorflow/core/framework/op_kernel.cc:1763] OP_REQUIRES failed at example_parsing_ops.cc:94 : Invalid argument: Key: x2.  Can't parse serialized Example.\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Key: x2.  Can't parse serialized Example.\n\t [[{{node ParseSingleExample/ParseExample/ParseExampleV2}}]]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/Paper1/lib/python3.7/site-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36mexecution_mode\u001b[0;34m(mode)\u001b[0m\n\u001b[1;32m   2112\u001b[0m       \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecutor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexecutor_new\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2113\u001b[0;31m       \u001b[0;32myield\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2114\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/Paper1/lib/python3.7/site-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m_next_internal\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    732\u001b[0m           \u001b[0moutput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flat_output_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 733\u001b[0;31m           output_shapes=self._flat_output_shapes)\n\u001b[0m\u001b[1;32m    734\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/Paper1/lib/python3.7/site-packages/tensorflow/python/ops/gen_dataset_ops.py\u001b[0m in \u001b[0;36miterator_get_next\u001b[0;34m(iterator, output_types, output_shapes, name)\u001b[0m\n\u001b[1;32m   2578\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2579\u001b[0;31m       \u001b[0m_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2580\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/Paper1/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   6861\u001b[0m   \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6862\u001b[0;31m   \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6863\u001b[0m   \u001b[0;31m# pylint: enable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/Paper1/lib/python3.7/site-packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Key: x2.  Can't parse serialized Example.\n\t [[{{node ParseSingleExample/ParseExample/ParseExampleV2}}]] [Op:IteratorGetNext]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/t5/9xgccmv92hnfvjwd62mk8zqh0000gn/T/ipykernel_25922/3134862119.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mparsed_record\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparsed_record\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/Paper1/lib/python3.7/site-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    745\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    746\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 747\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    748\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    749\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/Paper1/lib/python3.7/site-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m_next_internal\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    737\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_element_spec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_from_compatible_tensor_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    738\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 739\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mstructure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_compatible_tensor_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_element_spec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    741\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/Paper1/lib/python3.7/contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type, value, traceback)\u001b[0m\n\u001b[1;32m    128\u001b[0m                 \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthrow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraceback\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m                 \u001b[0;31m# Suppress StopIteration *unless* it's the same exception that\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/Paper1/lib/python3.7/site-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36mexecution_mode\u001b[0;34m(mode)\u001b[0m\n\u001b[1;32m   2114\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2115\u001b[0m       \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecutor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexecutor_old\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2116\u001b[0;31m       \u001b[0mexecutor_new\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/Paper1/lib/python3.7/site-packages/tensorflow/python/eager/executor.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     67\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;34m\"\"\"Waits for ops dispatched in this executor to finish.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m     \u001b[0mpywrap_tfe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTFE_ExecutorWaitForAllPendingNodes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mclear_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Key: x2.  Can't parse serialized Example.\n\t [[{{node ParseSingleExample/ParseExample/ParseExampleV2}}]]"
     ]
    }
   ],
   "source": [
    "for parsed_record in dataset.take(2):\n",
    "    print(parsed_record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now, following: https://stackoverflow.com/questions/48889482/feeding-npy-numpy-files-into-tensorflow-data-pipeline\n",
    "# try with io decode?\n",
    "def npy_header_offset(npy_path):\n",
    "    with open(str(npy_path), 'rb') as f:\n",
    "        print(f.read(6).decode('utf-8'))\n",
    "        if f.read(6) != b'\\x93NUMPY':\n",
    "            raise ValueError('Invalid NPY file.')\n",
    "        version_major, version_minor = f.read(2)\n",
    "        if version_major == 1:\n",
    "            header_len_size = 2\n",
    "        elif version_major == 2:\n",
    "            header_len_size = 4\n",
    "        else:\n",
    "            raise ValueError('Unknown NPY file version {}.{}.'.format(version_major, version_minor))\n",
    "        header_len = sum(b << (8 * i) for i, b in enumerate(f.read(header_len_size)))\n",
    "        header = f.read(header_len)\n",
    "        if not header.endswith(b'\\n'):\n",
    "            raise ValueError('Invalid NPY file.')\n",
    "        return f.tell()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PK\u0003\u0004\u0014\u0000\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Invalid NPY file.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/t5/9xgccmv92hnfvjwd62mk8zqh0000gn/T/ipykernel_25922/704836339.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m                 \u001b[0mnfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'arr_0'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m                 \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'arr_0'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m             \u001b[0mheader_offset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnpy_header_offset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mmod\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'npy'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0mb\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/t5/9xgccmv92hnfvjwd62mk8zqh0000gn/T/ipykernel_25922/838241765.py\u001b[0m in \u001b[0;36mnpy_header_offset\u001b[0;34m(npy_path)\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34mb'\\x93NUMPY'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Invalid NPY file.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0mversion_major\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mversion_minor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mversion_major\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Invalid NPY file."
     ]
    }
   ],
   "source": [
    "file_list1 = pd.read_csv(splitsDir+'train.csv',names=['filename'])['filename'].values.astype('str')\n",
    "file_list = []; nfeatures = -1; dtype = -1\n",
    "for iff,f in enumerate(file_list1):\n",
    "    f = f.rstrip('.xml').split('/')[-1]\n",
    "    if iff==0: # if first one, fill in the gaps\n",
    "        fg = glob.glob(classDir_main_to_imgs + f + '*')[0]\n",
    "        mod = fg.split('/')[-1]\n",
    "        mod = mod[mod.rfind('.')+1:]\n",
    "        # read in and keep images -- npy files/npz\n",
    "        if mod == 'npz':\n",
    "            with np.load(fg) as b:\n",
    "                nfeatures = b['arr_0'].shape[-1]\n",
    "                dtype = b['arr_0'].dtype\n",
    "            header_offset = npy_header_offset(fg)\n",
    "        elif mod == 'npy':\n",
    "            b=np.load(fg)\n",
    "            if type(b) != np.ndarray:\n",
    "                b = b['arr_0'] # WHY????\n",
    "                nfeatures = b.shape[-1]\n",
    "                dtype = b.dtype\n",
    "        elif mod == 'pickle':\n",
    "            print('not implemented!')\n",
    "            import sys; sys.exit()\n",
    "            with open(fg, 'rb') as ff:\n",
    "                b = pickle.load(ff) \n",
    "                nfeatures = np.array(b).shape[-1]\n",
    "                dtype = np.array(b).dtype\n",
    "        else:\n",
    "            print('no idea what this method is')\n",
    "        # what is dtype?\n",
    "        if dtype == np.dtype('uint8'): dtype = tf.uint8\n",
    "        if dtype == np.dtype('float64'): dtype = tf.float64\n",
    "    file_list.append(classDir_main_to_imgs+f+'.'+mod)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([], 12, dtype('uint8'))"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# alright maybe try recordio afterall?\n",
    "#https://github.com/tensorflow/tensorflow/blob/r1.3/tensorflow/examples/how_tos/reading_data/convert_to_records.py\n",
    "#https://stackoverflow.com/questions/50665144/create-tfrecord-for-object-detection-task\n",
    "#https://stackoverflow.com/questions/46820500/how-to-handle-large-amouts-of-data-in-tensorflow/47040165#47040165"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "recordIOdir = config.tmp_storage_dir + 'rio/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "def array_to_tfrecords(X, boxes, output_file):\n",
    "    if len(boxes)>0:\n",
    "        x1 = boxes[0][:,0]; y1 = boxes[0][:,1]; x2 = boxes[0][:,2]; y2 = boxes[0][:,3]\n",
    "        classes = boxes[0][:,4]\n",
    "    else:\n",
    "        x1=np.array([]);y1=np.array([]);x2=np.array([]);y2=np.array([]);classes=np.array([])\n",
    "    # do division already\n",
    "    X = X/255.\n",
    "    feature = {\n",
    "        'X': tf.train.Feature(float_list=tf.train.FloatList(value=X.flatten())),\n",
    "        'x1': tf.train.Feature(float_list=tf.train.FloatList(value=x1.flatten())),\n",
    "        'y1': tf.train.Feature(float_list=tf.train.FloatList(value=y1.flatten())),\n",
    "        'x2': tf.train.Feature(float_list=tf.train.FloatList(value=x2.flatten())),\n",
    "        'y2': tf.train.Feature(float_list=tf.train.FloatList(value=y2.flatten())),\n",
    "        'class': tf.train.Feature(float_list=tf.train.FloatList(value=classes.flatten()))\n",
    "    }\n",
    "    example = tf.train.Example(features=tf.train.Features(feature=feature))\n",
    "    serialized = example.SerializeToString()\n",
    "\n",
    "    writer = tf.io.TFRecordWriter(output_file)\n",
    "    writer.write(serialized)\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imgs_name, bbox = parse_annotation([classDir_main_to+f+'.xml'], LABELS, \n",
    "#                                                    feature_dir=classDir_main_to_imgs,\n",
    "#                                                    annotation_dir=classDir_main_to)\n",
    "\n",
    "# X = np.load(imgs_name[0])['arr_0']\n",
    "# array_to_tfrecords(X, bbox, recordIOdir+imgs_name[0].rstrip('.npz').split('/')[-1]+'.tfrecord')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 of 5515\n",
      "100 of 5515\n",
      "200 of 5515\n",
      "300 of 5515\n",
      "400 of 5515\n",
      "500 of 5515\n",
      "600 of 5515\n",
      "700 of 5515\n",
      "800 of 5515\n",
      "900 of 5515\n",
      "1000 of 5515\n",
      "1100 of 5515\n",
      "1200 of 5515\n",
      "1300 of 5515\n",
      "1400 of 5515\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/t5/9xgccmv92hnfvjwd62mk8zqh0000gn/T/ipykernel_25922/913782921.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimgs_name\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'arr_0'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0marray_to_tfrecords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbbox\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecordIOdir\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mimgs_name\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.npz'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'.tfrecord'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0;32mdel\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/t5/9xgccmv92hnfvjwd62mk8zqh0000gn/T/ipykernel_25922/1937598323.py\u001b[0m in \u001b[0;36marray_to_tfrecords\u001b[0;34m(X, boxes, output_file)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m255.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     feature = {\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0;34m'X'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFeature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfloat_list\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatList\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0;34m'x1'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFeature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfloat_list\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatList\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;34m'y1'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFeature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfloat_list\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatList\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# convert all\n",
    "for iff,f in enumerate(annotations):\n",
    "    imgs_name, bbox = parse_annotation([classDir_main_to+f.split('/')[-1]], LABELS, \n",
    "                                                       feature_dir=classDir_main_to_imgs,\n",
    "                                                       annotation_dir=classDir_main_to)\n",
    "\n",
    "    X = np.load(imgs_name[0])['arr_0']\n",
    "    array_to_tfrecords(X, bbox, recordIOdir+imgs_name[0].rstrip('.npz').split('/')[-1]+'.tfrecord')\n",
    "    del X\n",
    "    \n",
    "    if iff%100==0: print(iff,'of',len(annotations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5515"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(annotations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['/Users/jillnaiman/tmpMegaYolo/binaries/yolo_512x512_cap_ann/1952ApJ___115___82K_p1.xml',\n",
       "       '/Users/jillnaiman/tmpMegaYolo/binaries/yolo_512x512_cap_ann/1994ApJ___427L__43F_p4.xml',\n",
       "       '/Users/jillnaiman/tmpMegaYolo/binaries/yolo_512x512_cap_ann/1995ApJ___445__246C_p6.xml',\n",
       "       ...,\n",
       "       '/Users/jillnaiman/tmpMegaYolo/binaries/yolo_512x512_cap_ann/1978ApJ___220___14H_p1.xml',\n",
       "       '/Users/jillnaiman/tmpMegaYolo/binaries/yolo_512x512_cap_ann/1982ApJ___252__269Y_p2.xml',\n",
       "       '/Users/jillnaiman/tmpMegaYolo/binaries/yolo_512x512_cap_ann/1982ApJ___258__467Y_p2.xml'],\n",
       "      dtype='<U88')"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_list1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parse CSV: 6.593299985979684e-05\n",
      "parse CSV: 7.796400132065173e-05\n",
      "Parse annotation: 0.14502690299923415\n",
      "Parse annotation: 0.14677225699961127\n",
      "Load data: 0.12156924800001434\n",
      "Load data: 0.11779988200032676\n",
      "process blocks:process blocks: 0.1032422560001578\n",
      " 0.10012778300006175\n",
      "0\n",
      "parse CSV:parse CSV: 0.00013102500088280067\n",
      " 5.655000131810084e-05\n",
      "1\n",
      "Parse annotation: 0.15298127499954717\n",
      "Parse annotation: 0.15567094499965606\n",
      "Load data: 0.12399737299892877\n",
      "Load data: 0.12808158799998637\n",
      "process blocks: process blocks: 0.12697629399917787\n",
      "0.1203326869999728\n",
      "2parse CSV:parse CSV: 5.4875999921932817e-05\n",
      " 6.429899985960219e-05\n",
      "\n",
      "3\n",
      "Parse annotation: 0.14686419899953762\n",
      "Parse annotation: 0.16171898799984774\n",
      "Load data: 0.1252131900000677\n",
      "Load data: 0.12484830600078567\n",
      "process blocks:process blocks:  0.13700858399897697\n",
      "0.12357732499913254\n",
      "4parse CSV:parse CSV: 6.186500104377046e-05\n",
      "\n",
      " 8.753600013733376e-05\n",
      "5\n",
      "Parse annotation: 0.14494591199945717\n",
      "Parse annotation: 0.14763279299950227\n",
      "Load data: 0.1285129509997205\n",
      "Load data: 0.1277994949996355\n",
      "process blocks:process blocks: 0.12277748500127927\n",
      " 0.12455471900102566\n",
      "parse CSV: 6.833099905634299e-05\n",
      "parse CSV:6 \n",
      "7.652000022062566e-05\n",
      "7\n",
      "Parse annotation: 0.4687751359997492\n",
      "Parse annotation: 0.172946922999472\n",
      "Load data: 0.13505229000111285\n",
      "Load data: 0.1318750820009882\n",
      "process blocks: process blocks: 0.1395990349992644\n",
      "0.1316464569990785\n",
      "8\n",
      "parse CSV: 7.178299892984796e-05\n",
      "9parse CSV:\n",
      " 0.0015495119987463113\n",
      "Parse annotation:Parse annotation:  0.18537510900023335\n",
      "0.18108187100006035\n",
      "Load data: 0.13149454800077365\n",
      "Load data: 0.12941067999963707\n",
      "process blocks:process blocks: 0.12608493399966392\n",
      " 0.1266717679991416\n",
      "parse CSV:10parse CSV: 6.294599916145671e-05\n",
      "\n",
      " 7.425900002999697e-05\n",
      "Parse annotation: 0.14292050900075992\n",
      "Parse annotation: 0.13754443200014066\n",
      "Load data: 0.18013043599967204\n",
      "Load data: 0.20016973999918264\n",
      "process blocks: 0.10201240500100539\n",
      "process blocks: 0.4326179309991858\n",
      "Execution time: 7.084822689001157\n"
     ]
    }
   ],
   "source": [
    "benchmark(\n",
    "    tf.data.Dataset.range(2)\n",
    "    .interleave(\n",
    "        lambda _: ArtificialDatasetNonAug(batch_size=nbatch,split='valid'),\n",
    "        num_parallel_calls=tf.data.AUTOTUNE\n",
    "    )#.shuffle(3)\n",
    "    , nbatch=nbatch\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m\n",
       "\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterleave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmap_func\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mcycle_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mblock_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mnum_parallel_calls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mdeterministic\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "Maps `map_func` across this dataset, and interleaves the results.\n",
       "\n",
       "For example, you can use `Dataset.interleave()` to process many input files\n",
       "concurrently:\n",
       "\n",
       ">>> # Preprocess 4 files concurrently, and interleave blocks of 16 records\n",
       ">>> # from each file.\n",
       ">>> filenames = [\"/var/data/file1.txt\", \"/var/data/file2.txt\",\n",
       "...              \"/var/data/file3.txt\", \"/var/data/file4.txt\"]\n",
       ">>> dataset = tf.data.Dataset.from_tensor_slices(filenames)\n",
       ">>> def parse_fn(filename):\n",
       "...   return tf.data.Dataset.range(10)\n",
       ">>> dataset = dataset.interleave(lambda x:\n",
       "...     tf.data.TextLineDataset(x).map(parse_fn, num_parallel_calls=1),\n",
       "...     cycle_length=4, block_length=16)\n",
       "\n",
       "The `cycle_length` and `block_length` arguments control the order in which\n",
       "elements are produced. `cycle_length` controls the number of input elements\n",
       "that are processed concurrently. If you set `cycle_length` to 1, this\n",
       "transformation will handle one input element at a time, and will produce\n",
       "identical results to `tf.data.Dataset.flat_map`. In general,\n",
       "this transformation will apply `map_func` to `cycle_length` input elements,\n",
       "open iterators on the returned `Dataset` objects, and cycle through them\n",
       "producing `block_length` consecutive elements from each iterator, and\n",
       "consuming the next input element each time it reaches the end of an\n",
       "iterator.\n",
       "\n",
       "For example:\n",
       "\n",
       ">>> dataset = Dataset.range(1, 6)  # ==> [ 1, 2, 3, 4, 5 ]\n",
       ">>> # NOTE: New lines indicate \"block\" boundaries.\n",
       ">>> dataset = dataset.interleave(\n",
       "...     lambda x: Dataset.from_tensors(x).repeat(6),\n",
       "...     cycle_length=2, block_length=4)\n",
       ">>> list(dataset.as_numpy_iterator())\n",
       "[1, 1, 1, 1,\n",
       " 2, 2, 2, 2,\n",
       " 1, 1,\n",
       " 2, 2,\n",
       " 3, 3, 3, 3,\n",
       " 4, 4, 4, 4,\n",
       " 3, 3,\n",
       " 4, 4,\n",
       " 5, 5, 5, 5,\n",
       " 5, 5]\n",
       "\n",
       "Note: The order of elements yielded by this transformation is\n",
       "deterministic, as long as `map_func` is a pure function and\n",
       "`deterministic=True`. If `map_func` contains any stateful operations, the\n",
       "order in which that state is accessed is undefined.\n",
       "\n",
       "Performance can often be improved by setting `num_parallel_calls` so that\n",
       "`interleave` will use multiple threads to fetch elements. If determinism\n",
       "isn't required, it can also improve performance to set\n",
       "`deterministic=False`.\n",
       "\n",
       ">>> filenames = [\"/var/data/file1.txt\", \"/var/data/file2.txt\",\n",
       "...              \"/var/data/file3.txt\", \"/var/data/file4.txt\"]\n",
       ">>> dataset = tf.data.Dataset.from_tensor_slices(filenames)\n",
       ">>> dataset = dataset.interleave(lambda x: tf.data.TFRecordDataset(x),\n",
       "...     cycle_length=4, num_parallel_calls=tf.data.AUTOTUNE,\n",
       "...     deterministic=False)\n",
       "\n",
       "Args:\n",
       "  map_func: A function mapping a dataset element to a dataset.\n",
       "  cycle_length: (Optional.) The number of input elements that will be\n",
       "    processed concurrently. If not set, the tf.data runtime decides what it\n",
       "    should be based on available CPU. If `num_parallel_calls` is set to\n",
       "    `tf.data.AUTOTUNE`, the `cycle_length` argument identifies\n",
       "    the maximum degree of parallelism.\n",
       "  block_length: (Optional.) The number of consecutive elements to produce\n",
       "    from each input element before cycling to another input element. If not\n",
       "    set, defaults to 1.\n",
       "  num_parallel_calls: (Optional.) If specified, the implementation creates a\n",
       "    threadpool, which is used to fetch inputs from cycle elements\n",
       "    asynchronously and in parallel. The default behavior is to fetch inputs\n",
       "    from cycle elements synchronously with no parallelism. If the value\n",
       "    `tf.data.AUTOTUNE` is used, then the number of parallel\n",
       "    calls is set dynamically based on available CPU.\n",
       "  deterministic: (Optional.) A boolean controlling whether determinism\n",
       "    should be traded for performance by allowing elements to be produced out\n",
       "    of order.  If `deterministic` is `None`, the\n",
       "    `tf.data.Options.experimental_deterministic` dataset option (`True` by\n",
       "    default) is used to decide whether to produce elements\n",
       "    deterministically.\n",
       "\n",
       "Returns:\n",
       "  Dataset: A `Dataset`.\n",
       "\u001b[0;31mFile:\u001b[0m      ~/anaconda3/envs/Paper1/lib/python3.7/site-packages/tensorflow/python/data/ops/dataset_ops.py\n",
       "\u001b[0;31mType:\u001b[0m      function\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tf.data.Dataset.interleave?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RRyluqn6vvaW"
   },
   "source": [
    "Steps per epoch -- training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1638305356592,
     "user": {
      "displayName": "Jill Naiman",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "13328008118965652761"
     },
     "user_tz": 360
    },
    "id": "SdEYPW-tvvaW",
    "outputId": "6c038327-6fe5-439a-c5be-9cedf4f87cec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps per epoch = training: 828 , validation: 164\n"
     ]
    }
   ],
   "source": [
    "#steps = len(X_train) // batch_size\n",
    "#print(len(X_train)//batch_size)\n",
    "# can do larger with augmentation\n",
    "\n",
    "aug_fac = 2 # 2 or 3\n",
    "\n",
    "steps_training = (len(X_train)//batch_size)*aug_fac\n",
    "# factor of 2 from: https://stackoverflow.com/questions/49922252/choosing-number-of-steps-per-epoch\n",
    "\n",
    "steps_val = (len(X_valid)//batch_size)*aug_fac\n",
    "\n",
    "print('Steps per epoch = training:', steps_training, ', validation:', steps_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sExlEUTwIYBy"
   },
   "source": [
    "Save also the names of the test instances to use later:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1638305356592,
     "user": {
      "displayName": "Jill Naiman",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "13328008118965652761"
     },
     "user_tz": 360
    },
    "id": "y4LLclWnIQ49",
    "outputId": "958a13f5-ddcb-4b0f-f31f-4f7dc9795857"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hey, saved tests!\n"
     ]
    }
   ],
   "source": [
    "# save test list in an extra place... this is a bit redundant since its saved another place too\n",
    "#if not re_run_from_splits:\n",
    "np.savetxt(saveFile, X_test, delimiter=',',fmt='%s')\n",
    "print('Hey, saved tests!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7gQi6kS8nea8"
   },
   "source": [
    "# 1. Define YOLO model\n",
    "\n",
    "For v5, see: https://github.com/jahongir7174/YOLOv5-tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zoBEY7n67jy1"
   },
   "source": [
    "For creating the model -- how many features are we using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1638305356593,
     "user": {
      "displayName": "Jill Naiman",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "13328008118965652761"
     },
     "user_tz": 360
    },
    "id": "ymfe7jyL7jy1",
    "outputId": "eb9c56e9-a90b-4bec-cd53-f2747631615b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_features = get_n_features(classDir_main_to_imgs)\n",
    "n_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "executionInfo": {
     "elapsed": 2663,
     "status": "ok",
     "timestamp": 1638305359251,
     "user": {
      "displayName": "Jill Naiman",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "13328008118965652761"
     },
     "user_tz": 360
    },
    "id": "KRQsM_X7XC-V"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-30 14:53:47.147006: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "version = 'l' # large version\n",
    "model = build_model(n_features, anchors, version, len(LABELS))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yXdkSQK9Emky"
   },
   "source": [
    "Build YOLOv5:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 276,
     "status": "ok",
     "timestamp": 1638305359522,
     "user": {
      "displayName": "Jill Naiman",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "13328008118965652761"
     },
     "user_tz": 360
    },
    "id": "0Vnf5mjn7jy2",
    "outputId": "6a1d77e9-e8ef-41d8-b1b3-d11ccad3c0fa",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 512, 512, 12 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.space_to_depth (TFOpLambd (None, 256, 256, 48) 0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 256, 256, 64) 27648       tf.nn.space_to_depth[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 256, 256, 64) 256         conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 256, 256, 64) 0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d (ZeroPadding2D)  (None, 257, 257, 64) 0           activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 128, 128, 128 73728       zero_padding2d[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 128, 128, 128 512         conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 128, 128, 128 0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 128, 128, 64) 8192        activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 128, 128, 64) 256         conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 128, 128, 64) 0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 128, 128, 64) 4096        activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 128, 128, 64) 256         conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 128, 128, 64) 0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 128, 128, 64) 36864       activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 128, 128, 64) 256         conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 128, 128, 64) 0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add (TFOpLambd (None, 128, 128, 64) 0           activation_2[0][0]               \n",
      "                                                                 activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 128, 128, 64) 4096        tf.__operators__.add[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 128, 128, 64) 256         conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 128, 128, 64) 0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 128, 128, 64) 36864       activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 128, 128, 64) 256         conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 128, 128, 64) 0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_1 (TFOpLam (None, 128, 128, 64) 0           tf.__operators__.add[0][0]       \n",
      "                                                                 activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 128, 128, 64) 4096        tf.__operators__.add_1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 128, 128, 64) 256         conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 128, 128, 64) 0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 128, 128, 64) 36864       activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 128, 128, 64) 8192        activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 128, 128, 64) 256         conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 128, 128, 64) 256         conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 128, 128, 64) 0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 128, 128, 64) 0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_2 (TFOpLam (None, 128, 128, 64) 0           tf.__operators__.add_1[0][0]     \n",
      "                                                                 activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 128, 128, 128 0           activation_9[0][0]               \n",
      "                                                                 tf.__operators__.add_2[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 128, 128, 128 16384       concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 128, 128, 128 512         conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 128, 128, 128 0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_1 (ZeroPadding2D (None, 129, 129, 128 0           activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 64, 64, 256)  294912      zero_padding2d_1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 64, 64, 256)  1024        conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 64, 64, 256)  0           batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 64, 64, 128)  32768       activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 64, 64, 128)  512         conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 64, 64, 128)  0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 64, 64, 128)  16384       activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 64, 64, 128)  512         conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 64, 64, 128)  0           batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 64, 64, 128)  147456      activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 64, 64, 128)  512         conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 64, 64, 128)  0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_3 (TFOpLam (None, 64, 64, 128)  0           activation_12[0][0]              \n",
      "                                                                 activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 64, 64, 128)  16384       tf.__operators__.add_3[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 64, 64, 128)  512         conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 64, 64, 128)  0           batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 64, 64, 128)  147456      activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 64, 64, 128)  512         conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 64, 64, 128)  0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_4 (TFOpLam (None, 64, 64, 128)  0           tf.__operators__.add_3[0][0]     \n",
      "                                                                 activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 64, 64, 128)  16384       tf.__operators__.add_4[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 64, 64, 128)  512         conv2d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 64, 64, 128)  0           batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, 64, 64, 128)  147456      activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNo (None, 64, 64, 128)  512         conv2d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 64, 64, 128)  0           batch_normalization_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_5 (TFOpLam (None, 64, 64, 128)  0           tf.__operators__.add_4[0][0]     \n",
      "                                                                 activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_19 (Conv2D)              (None, 64, 64, 128)  16384       tf.__operators__.add_5[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_19 (BatchNo (None, 64, 64, 128)  512         conv2d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, 64, 64, 128)  0           batch_normalization_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_20 (Conv2D)              (None, 64, 64, 128)  147456      activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_20 (BatchNo (None, 64, 64, 128)  512         conv2d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_20 (Activation)      (None, 64, 64, 128)  0           batch_normalization_20[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_6 (TFOpLam (None, 64, 64, 128)  0           tf.__operators__.add_5[0][0]     \n",
      "                                                                 activation_20[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_21 (Conv2D)              (None, 64, 64, 128)  16384       tf.__operators__.add_6[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_21 (BatchNo (None, 64, 64, 128)  512         conv2d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_21 (Activation)      (None, 64, 64, 128)  0           batch_normalization_21[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_22 (Conv2D)              (None, 64, 64, 128)  147456      activation_21[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_22 (BatchNo (None, 64, 64, 128)  512         conv2d_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_22 (Activation)      (None, 64, 64, 128)  0           batch_normalization_22[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_7 (TFOpLam (None, 64, 64, 128)  0           tf.__operators__.add_6[0][0]     \n",
      "                                                                 activation_22[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_23 (Conv2D)              (None, 64, 64, 128)  16384       tf.__operators__.add_7[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_23 (BatchNo (None, 64, 64, 128)  512         conv2d_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_23 (Activation)      (None, 64, 64, 128)  0           batch_normalization_23[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_24 (Conv2D)              (None, 64, 64, 128)  147456      activation_23[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_24 (BatchNo (None, 64, 64, 128)  512         conv2d_24[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_24 (Activation)      (None, 64, 64, 128)  0           batch_normalization_24[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_8 (TFOpLam (None, 64, 64, 128)  0           tf.__operators__.add_7[0][0]     \n",
      "                                                                 activation_24[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_25 (Conv2D)              (None, 64, 64, 128)  16384       tf.__operators__.add_8[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_25 (BatchNo (None, 64, 64, 128)  512         conv2d_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_25 (Activation)      (None, 64, 64, 128)  0           batch_normalization_25[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_26 (Conv2D)              (None, 64, 64, 128)  147456      activation_25[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_26 (BatchNo (None, 64, 64, 128)  512         conv2d_26[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_26 (Activation)      (None, 64, 64, 128)  0           batch_normalization_26[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_9 (TFOpLam (None, 64, 64, 128)  0           tf.__operators__.add_8[0][0]     \n",
      "                                                                 activation_26[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_27 (Conv2D)              (None, 64, 64, 128)  16384       tf.__operators__.add_9[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_27 (BatchNo (None, 64, 64, 128)  512         conv2d_27[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_27 (Activation)      (None, 64, 64, 128)  0           batch_normalization_27[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_28 (Conv2D)              (None, 64, 64, 128)  147456      activation_27[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_28 (BatchNo (None, 64, 64, 128)  512         conv2d_28[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_28 (Activation)      (None, 64, 64, 128)  0           batch_normalization_28[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_10 (TFOpLa (None, 64, 64, 128)  0           tf.__operators__.add_9[0][0]     \n",
      "                                                                 activation_28[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_29 (Conv2D)              (None, 64, 64, 128)  16384       tf.__operators__.add_10[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_29 (BatchNo (None, 64, 64, 128)  512         conv2d_29[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_29 (Activation)      (None, 64, 64, 128)  0           batch_normalization_29[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_30 (Conv2D)              (None, 64, 64, 128)  147456      activation_29[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_31 (Conv2D)              (None, 64, 64, 128)  32768       activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_30 (BatchNo (None, 64, 64, 128)  512         conv2d_30[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_31 (BatchNo (None, 64, 64, 128)  512         conv2d_31[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_30 (Activation)      (None, 64, 64, 128)  0           batch_normalization_30[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_31 (Activation)      (None, 64, 64, 128)  0           batch_normalization_31[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_11 (TFOpLa (None, 64, 64, 128)  0           tf.__operators__.add_10[0][0]    \n",
      "                                                                 activation_30[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 64, 64, 256)  0           activation_31[0][0]              \n",
      "                                                                 tf.__operators__.add_11[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_32 (Conv2D)              (None, 64, 64, 256)  65536       concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_32 (BatchNo (None, 64, 64, 256)  1024        conv2d_32[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_32 (Activation)      (None, 64, 64, 256)  0           batch_normalization_32[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_2 (ZeroPadding2D (None, 65, 65, 256)  0           activation_32[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_33 (Conv2D)              (None, 32, 32, 512)  1179648     zero_padding2d_2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_33 (BatchNo (None, 32, 32, 512)  2048        conv2d_33[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_33 (Activation)      (None, 32, 32, 512)  0           batch_normalization_33[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_34 (Conv2D)              (None, 32, 32, 256)  131072      activation_33[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_34 (BatchNo (None, 32, 32, 256)  1024        conv2d_34[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_34 (Activation)      (None, 32, 32, 256)  0           batch_normalization_34[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_35 (Conv2D)              (None, 32, 32, 256)  65536       activation_34[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_35 (BatchNo (None, 32, 32, 256)  1024        conv2d_35[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_35 (Activation)      (None, 32, 32, 256)  0           batch_normalization_35[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_36 (Conv2D)              (None, 32, 32, 256)  589824      activation_35[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_36 (BatchNo (None, 32, 32, 256)  1024        conv2d_36[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_36 (Activation)      (None, 32, 32, 256)  0           batch_normalization_36[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_12 (TFOpLa (None, 32, 32, 256)  0           activation_34[0][0]              \n",
      "                                                                 activation_36[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_37 (Conv2D)              (None, 32, 32, 256)  65536       tf.__operators__.add_12[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_37 (BatchNo (None, 32, 32, 256)  1024        conv2d_37[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_37 (Activation)      (None, 32, 32, 256)  0           batch_normalization_37[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_38 (Conv2D)              (None, 32, 32, 256)  589824      activation_37[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_38 (BatchNo (None, 32, 32, 256)  1024        conv2d_38[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_38 (Activation)      (None, 32, 32, 256)  0           batch_normalization_38[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_13 (TFOpLa (None, 32, 32, 256)  0           tf.__operators__.add_12[0][0]    \n",
      "                                                                 activation_38[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_39 (Conv2D)              (None, 32, 32, 256)  65536       tf.__operators__.add_13[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_39 (BatchNo (None, 32, 32, 256)  1024        conv2d_39[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_39 (Activation)      (None, 32, 32, 256)  0           batch_normalization_39[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_40 (Conv2D)              (None, 32, 32, 256)  589824      activation_39[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_40 (BatchNo (None, 32, 32, 256)  1024        conv2d_40[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_40 (Activation)      (None, 32, 32, 256)  0           batch_normalization_40[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_14 (TFOpLa (None, 32, 32, 256)  0           tf.__operators__.add_13[0][0]    \n",
      "                                                                 activation_40[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_41 (Conv2D)              (None, 32, 32, 256)  65536       tf.__operators__.add_14[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_41 (BatchNo (None, 32, 32, 256)  1024        conv2d_41[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_41 (Activation)      (None, 32, 32, 256)  0           batch_normalization_41[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_42 (Conv2D)              (None, 32, 32, 256)  589824      activation_41[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_42 (BatchNo (None, 32, 32, 256)  1024        conv2d_42[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_42 (Activation)      (None, 32, 32, 256)  0           batch_normalization_42[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_15 (TFOpLa (None, 32, 32, 256)  0           tf.__operators__.add_14[0][0]    \n",
      "                                                                 activation_42[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_43 (Conv2D)              (None, 32, 32, 256)  65536       tf.__operators__.add_15[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_43 (BatchNo (None, 32, 32, 256)  1024        conv2d_43[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_43 (Activation)      (None, 32, 32, 256)  0           batch_normalization_43[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_44 (Conv2D)              (None, 32, 32, 256)  589824      activation_43[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_44 (BatchNo (None, 32, 32, 256)  1024        conv2d_44[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_44 (Activation)      (None, 32, 32, 256)  0           batch_normalization_44[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_16 (TFOpLa (None, 32, 32, 256)  0           tf.__operators__.add_15[0][0]    \n",
      "                                                                 activation_44[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_45 (Conv2D)              (None, 32, 32, 256)  65536       tf.__operators__.add_16[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_45 (BatchNo (None, 32, 32, 256)  1024        conv2d_45[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_45 (Activation)      (None, 32, 32, 256)  0           batch_normalization_45[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_46 (Conv2D)              (None, 32, 32, 256)  589824      activation_45[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_46 (BatchNo (None, 32, 32, 256)  1024        conv2d_46[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_46 (Activation)      (None, 32, 32, 256)  0           batch_normalization_46[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_17 (TFOpLa (None, 32, 32, 256)  0           tf.__operators__.add_16[0][0]    \n",
      "                                                                 activation_46[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_47 (Conv2D)              (None, 32, 32, 256)  65536       tf.__operators__.add_17[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_47 (BatchNo (None, 32, 32, 256)  1024        conv2d_47[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_47 (Activation)      (None, 32, 32, 256)  0           batch_normalization_47[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_48 (Conv2D)              (None, 32, 32, 256)  589824      activation_47[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_48 (BatchNo (None, 32, 32, 256)  1024        conv2d_48[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_48 (Activation)      (None, 32, 32, 256)  0           batch_normalization_48[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_18 (TFOpLa (None, 32, 32, 256)  0           tf.__operators__.add_17[0][0]    \n",
      "                                                                 activation_48[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_49 (Conv2D)              (None, 32, 32, 256)  65536       tf.__operators__.add_18[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_49 (BatchNo (None, 32, 32, 256)  1024        conv2d_49[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_49 (Activation)      (None, 32, 32, 256)  0           batch_normalization_49[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_50 (Conv2D)              (None, 32, 32, 256)  589824      activation_49[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_50 (BatchNo (None, 32, 32, 256)  1024        conv2d_50[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_50 (Activation)      (None, 32, 32, 256)  0           batch_normalization_50[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_19 (TFOpLa (None, 32, 32, 256)  0           tf.__operators__.add_18[0][0]    \n",
      "                                                                 activation_50[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_51 (Conv2D)              (None, 32, 32, 256)  65536       tf.__operators__.add_19[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_51 (BatchNo (None, 32, 32, 256)  1024        conv2d_51[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_51 (Activation)      (None, 32, 32, 256)  0           batch_normalization_51[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_52 (Conv2D)              (None, 32, 32, 256)  589824      activation_51[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_53 (Conv2D)              (None, 32, 32, 256)  131072      activation_33[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_52 (BatchNo (None, 32, 32, 256)  1024        conv2d_52[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_53 (BatchNo (None, 32, 32, 256)  1024        conv2d_53[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_52 (Activation)      (None, 32, 32, 256)  0           batch_normalization_52[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_53 (Activation)      (None, 32, 32, 256)  0           batch_normalization_53[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_20 (TFOpLa (None, 32, 32, 256)  0           tf.__operators__.add_19[0][0]    \n",
      "                                                                 activation_52[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 32, 32, 512)  0           activation_53[0][0]              \n",
      "                                                                 tf.__operators__.add_20[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_54 (Conv2D)              (None, 32, 32, 512)  262144      concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_54 (BatchNo (None, 32, 32, 512)  2048        conv2d_54[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_54 (Activation)      (None, 32, 32, 512)  0           batch_normalization_54[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_3 (ZeroPadding2D (None, 33, 33, 512)  0           activation_54[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_55 (Conv2D)              (None, 16, 16, 1024) 4718592     zero_padding2d_3[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_55 (BatchNo (None, 16, 16, 1024) 4096        conv2d_55[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_55 (Activation)      (None, 16, 16, 1024) 0           batch_normalization_55[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_56 (Conv2D)              (None, 16, 16, 512)  524288      activation_55[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_56 (BatchNo (None, 16, 16, 512)  2048        conv2d_56[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_56 (Activation)      (None, 16, 16, 512)  0           batch_normalization_56[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.max_pool2d (TFOpLambda)   (None, 16, 16, 512)  0           activation_56[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.max_pool2d_1 (TFOpLambda) (None, 16, 16, 512)  0           activation_56[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.max_pool2d_2 (TFOpLambda) (None, 16, 16, 512)  0           activation_56[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 16, 16, 2048) 0           activation_56[0][0]              \n",
      "                                                                 tf.nn.max_pool2d[0][0]           \n",
      "                                                                 tf.nn.max_pool2d_1[0][0]         \n",
      "                                                                 tf.nn.max_pool2d_2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_57 (Conv2D)              (None, 16, 16, 1024) 2097152     concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_57 (BatchNo (None, 16, 16, 1024) 4096        conv2d_57[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_57 (Activation)      (None, 16, 16, 1024) 0           batch_normalization_57[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_58 (Conv2D)              (None, 16, 16, 512)  524288      activation_57[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_58 (BatchNo (None, 16, 16, 512)  2048        conv2d_58[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_58 (Activation)      (None, 16, 16, 512)  0           batch_normalization_58[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_59 (Conv2D)              (None, 16, 16, 512)  262144      activation_58[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_59 (BatchNo (None, 16, 16, 512)  2048        conv2d_59[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_59 (Activation)      (None, 16, 16, 512)  0           batch_normalization_59[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_60 (Conv2D)              (None, 16, 16, 512)  2359296     activation_59[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_60 (BatchNo (None, 16, 16, 512)  2048        conv2d_60[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_60 (Activation)      (None, 16, 16, 512)  0           batch_normalization_60[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_61 (Conv2D)              (None, 16, 16, 512)  262144      activation_60[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_61 (BatchNo (None, 16, 16, 512)  2048        conv2d_61[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_61 (Activation)      (None, 16, 16, 512)  0           batch_normalization_61[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_62 (Conv2D)              (None, 16, 16, 512)  2359296     activation_61[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_62 (BatchNo (None, 16, 16, 512)  2048        conv2d_62[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_62 (Activation)      (None, 16, 16, 512)  0           batch_normalization_62[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_63 (Conv2D)              (None, 16, 16, 512)  262144      activation_62[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_63 (BatchNo (None, 16, 16, 512)  2048        conv2d_63[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_63 (Activation)      (None, 16, 16, 512)  0           batch_normalization_63[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_65 (Conv2D)              (None, 16, 16, 512)  524288      activation_57[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_64 (Conv2D)              (None, 16, 16, 512)  2359296     activation_63[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_65 (BatchNo (None, 16, 16, 512)  2048        conv2d_65[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_64 (BatchNo (None, 16, 16, 512)  2048        conv2d_64[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_65 (Activation)      (None, 16, 16, 512)  0           batch_normalization_65[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_64 (Activation)      (None, 16, 16, 512)  0           batch_normalization_64[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 16, 16, 1024) 0           activation_65[0][0]              \n",
      "                                                                 activation_64[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_66 (Conv2D)              (None, 16, 16, 1024) 1048576     concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_66 (BatchNo (None, 16, 16, 1024) 4096        conv2d_66[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_66 (Activation)      (None, 16, 16, 1024) 0           batch_normalization_66[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_67 (Conv2D)              (None, 16, 16, 512)  524288      activation_66[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_67 (BatchNo (None, 16, 16, 512)  2048        conv2d_67[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_67 (Activation)      (None, 16, 16, 512)  0           batch_normalization_67[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d (UpSampling2D)    (None, 32, 32, 512)  0           activation_67[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 32, 32, 1024) 0           up_sampling2d[0][0]              \n",
      "                                                                 activation_54[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_68 (Conv2D)              (None, 32, 32, 256)  262144      concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_68 (BatchNo (None, 32, 32, 256)  1024        conv2d_68[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_68 (Activation)      (None, 32, 32, 256)  0           batch_normalization_68[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_69 (Conv2D)              (None, 32, 32, 256)  65536       activation_68[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_69 (BatchNo (None, 32, 32, 256)  1024        conv2d_69[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_69 (Activation)      (None, 32, 32, 256)  0           batch_normalization_69[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_70 (Conv2D)              (None, 32, 32, 256)  589824      activation_69[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_70 (BatchNo (None, 32, 32, 256)  1024        conv2d_70[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_70 (Activation)      (None, 32, 32, 256)  0           batch_normalization_70[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_71 (Conv2D)              (None, 32, 32, 256)  65536       activation_70[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_71 (BatchNo (None, 32, 32, 256)  1024        conv2d_71[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_71 (Activation)      (None, 32, 32, 256)  0           batch_normalization_71[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_72 (Conv2D)              (None, 32, 32, 256)  589824      activation_71[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_72 (BatchNo (None, 32, 32, 256)  1024        conv2d_72[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_72 (Activation)      (None, 32, 32, 256)  0           batch_normalization_72[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_73 (Conv2D)              (None, 32, 32, 256)  65536       activation_72[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_73 (BatchNo (None, 32, 32, 256)  1024        conv2d_73[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_73 (Activation)      (None, 32, 32, 256)  0           batch_normalization_73[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_75 (Conv2D)              (None, 32, 32, 256)  262144      concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_74 (Conv2D)              (None, 32, 32, 256)  589824      activation_73[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_75 (BatchNo (None, 32, 32, 256)  1024        conv2d_75[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_74 (BatchNo (None, 32, 32, 256)  1024        conv2d_74[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_75 (Activation)      (None, 32, 32, 256)  0           batch_normalization_75[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_74 (Activation)      (None, 32, 32, 256)  0           batch_normalization_74[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 32, 32, 512)  0           activation_75[0][0]              \n",
      "                                                                 activation_74[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_76 (Conv2D)              (None, 32, 32, 512)  262144      concatenate_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_76 (BatchNo (None, 32, 32, 512)  2048        conv2d_76[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_76 (Activation)      (None, 32, 32, 512)  0           batch_normalization_76[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_77 (Conv2D)              (None, 32, 32, 256)  131072      activation_76[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_77 (BatchNo (None, 32, 32, 256)  1024        conv2d_77[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_77 (Activation)      (None, 32, 32, 256)  0           batch_normalization_77[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_1 (UpSampling2D)  (None, 64, 64, 256)  0           activation_77[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)     (None, 64, 64, 512)  0           up_sampling2d_1[0][0]            \n",
      "                                                                 activation_32[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_78 (Conv2D)              (None, 64, 64, 128)  65536       concatenate_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_78 (BatchNo (None, 64, 64, 128)  512         conv2d_78[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_78 (Activation)      (None, 64, 64, 128)  0           batch_normalization_78[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_79 (Conv2D)              (None, 64, 64, 128)  16384       activation_78[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_79 (BatchNo (None, 64, 64, 128)  512         conv2d_79[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_79 (Activation)      (None, 64, 64, 128)  0           batch_normalization_79[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_80 (Conv2D)              (None, 64, 64, 128)  147456      activation_79[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_80 (BatchNo (None, 64, 64, 128)  512         conv2d_80[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_80 (Activation)      (None, 64, 64, 128)  0           batch_normalization_80[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_81 (Conv2D)              (None, 64, 64, 128)  16384       activation_80[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_81 (BatchNo (None, 64, 64, 128)  512         conv2d_81[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_81 (Activation)      (None, 64, 64, 128)  0           batch_normalization_81[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_82 (Conv2D)              (None, 64, 64, 128)  147456      activation_81[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_82 (BatchNo (None, 64, 64, 128)  512         conv2d_82[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_82 (Activation)      (None, 64, 64, 128)  0           batch_normalization_82[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_83 (Conv2D)              (None, 64, 64, 128)  16384       activation_82[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_83 (BatchNo (None, 64, 64, 128)  512         conv2d_83[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_83 (Activation)      (None, 64, 64, 128)  0           batch_normalization_83[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_85 (Conv2D)              (None, 64, 64, 128)  65536       concatenate_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_84 (Conv2D)              (None, 64, 64, 128)  147456      activation_83[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_85 (BatchNo (None, 64, 64, 128)  512         conv2d_85[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_84 (BatchNo (None, 64, 64, 128)  512         conv2d_84[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_85 (Activation)      (None, 64, 64, 128)  0           batch_normalization_85[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_84 (Activation)      (None, 64, 64, 128)  0           batch_normalization_84[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_8 (Concatenate)     (None, 64, 64, 256)  0           activation_85[0][0]              \n",
      "                                                                 activation_84[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_86 (Conv2D)              (None, 64, 64, 256)  65536       concatenate_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_86 (BatchNo (None, 64, 64, 256)  1024        conv2d_86[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_86 (Activation)      (None, 64, 64, 256)  0           batch_normalization_86[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_4 (ZeroPadding2D (None, 65, 65, 256)  0           activation_86[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_87 (Conv2D)              (None, 32, 32, 256)  589824      zero_padding2d_4[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_87 (BatchNo (None, 32, 32, 256)  1024        conv2d_87[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_87 (Activation)      (None, 32, 32, 256)  0           batch_normalization_87[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_9 (Concatenate)     (None, 32, 32, 512)  0           activation_87[0][0]              \n",
      "                                                                 activation_77[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_88 (Conv2D)              (None, 32, 32, 256)  131072      concatenate_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_88 (BatchNo (None, 32, 32, 256)  1024        conv2d_88[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_88 (Activation)      (None, 32, 32, 256)  0           batch_normalization_88[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_89 (Conv2D)              (None, 32, 32, 256)  65536       activation_88[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_89 (BatchNo (None, 32, 32, 256)  1024        conv2d_89[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_89 (Activation)      (None, 32, 32, 256)  0           batch_normalization_89[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_90 (Conv2D)              (None, 32, 32, 256)  589824      activation_89[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_90 (BatchNo (None, 32, 32, 256)  1024        conv2d_90[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_90 (Activation)      (None, 32, 32, 256)  0           batch_normalization_90[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_91 (Conv2D)              (None, 32, 32, 256)  65536       activation_90[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_91 (BatchNo (None, 32, 32, 256)  1024        conv2d_91[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_91 (Activation)      (None, 32, 32, 256)  0           batch_normalization_91[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_92 (Conv2D)              (None, 32, 32, 256)  589824      activation_91[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_92 (BatchNo (None, 32, 32, 256)  1024        conv2d_92[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_92 (Activation)      (None, 32, 32, 256)  0           batch_normalization_92[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_93 (Conv2D)              (None, 32, 32, 256)  65536       activation_92[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_93 (BatchNo (None, 32, 32, 256)  1024        conv2d_93[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_93 (Activation)      (None, 32, 32, 256)  0           batch_normalization_93[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_95 (Conv2D)              (None, 32, 32, 256)  131072      concatenate_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_94 (Conv2D)              (None, 32, 32, 256)  589824      activation_93[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_95 (BatchNo (None, 32, 32, 256)  1024        conv2d_95[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_94 (BatchNo (None, 32, 32, 256)  1024        conv2d_94[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_95 (Activation)      (None, 32, 32, 256)  0           batch_normalization_95[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_94 (Activation)      (None, 32, 32, 256)  0           batch_normalization_94[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_10 (Concatenate)    (None, 32, 32, 512)  0           activation_95[0][0]              \n",
      "                                                                 activation_94[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_96 (Conv2D)              (None, 32, 32, 512)  262144      concatenate_10[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_96 (BatchNo (None, 32, 32, 512)  2048        conv2d_96[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_96 (Activation)      (None, 32, 32, 512)  0           batch_normalization_96[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_5 (ZeroPadding2D (None, 33, 33, 512)  0           activation_96[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_97 (Conv2D)              (None, 16, 16, 512)  2359296     zero_padding2d_5[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_97 (BatchNo (None, 16, 16, 512)  2048        conv2d_97[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_97 (Activation)      (None, 16, 16, 512)  0           batch_normalization_97[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_11 (Concatenate)    (None, 16, 16, 1024) 0           activation_97[0][0]              \n",
      "                                                                 activation_67[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_98 (Conv2D)              (None, 16, 16, 512)  524288      concatenate_11[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_98 (BatchNo (None, 16, 16, 512)  2048        conv2d_98[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_98 (Activation)      (None, 16, 16, 512)  0           batch_normalization_98[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_99 (Conv2D)              (None, 16, 16, 512)  262144      activation_98[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_99 (BatchNo (None, 16, 16, 512)  2048        conv2d_99[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_99 (Activation)      (None, 16, 16, 512)  0           batch_normalization_99[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_100 (Conv2D)             (None, 16, 16, 512)  2359296     activation_99[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_100 (BatchN (None, 16, 16, 512)  2048        conv2d_100[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_100 (Activation)     (None, 16, 16, 512)  0           batch_normalization_100[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_101 (Conv2D)             (None, 16, 16, 512)  262144      activation_100[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_101 (BatchN (None, 16, 16, 512)  2048        conv2d_101[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_101 (Activation)     (None, 16, 16, 512)  0           batch_normalization_101[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_102 (Conv2D)             (None, 16, 16, 512)  2359296     activation_101[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_102 (BatchN (None, 16, 16, 512)  2048        conv2d_102[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_102 (Activation)     (None, 16, 16, 512)  0           batch_normalization_102[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_103 (Conv2D)             (None, 16, 16, 512)  262144      activation_102[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_103 (BatchN (None, 16, 16, 512)  2048        conv2d_103[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_103 (Activation)     (None, 16, 16, 512)  0           batch_normalization_103[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_105 (Conv2D)             (None, 16, 16, 512)  524288      concatenate_11[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_104 (Conv2D)             (None, 16, 16, 512)  2359296     activation_103[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_105 (BatchN (None, 16, 16, 512)  2048        conv2d_105[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_104 (BatchN (None, 16, 16, 512)  2048        conv2d_104[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_105 (Activation)     (None, 16, 16, 512)  0           batch_normalization_105[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_104 (Activation)     (None, 16, 16, 512)  0           batch_normalization_104[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_12 (Concatenate)    (None, 16, 16, 1024) 0           activation_105[0][0]             \n",
      "                                                                 activation_104[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_106 (Conv2D)             (None, 16, 16, 1024) 1048576     concatenate_12[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_106 (BatchN (None, 16, 16, 1024) 4096        conv2d_106[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_106 (Activation)     (None, 16, 16, 1024) 0           batch_normalization_106[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "p5_4 (Conv2D)                   (None, 16, 16, 27)   27675       activation_106[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "p4_4 (Conv2D)                   (None, 32, 32, 27)   13851       activation_96[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "p3_4 (Conv2D)                   (None, 64, 64, 27)   6939        activation_86[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 46,729,809\n",
      "Trainable params: 46,668,241\n",
      "Non-trainable params: 61,568\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "executionInfo": {
     "elapsed": 34,
     "status": "ok",
     "timestamp": 1638305359523,
     "user": {
      "displayName": "Jill Naiman",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "13328008118965652761"
     },
     "user_tz": 360
    },
    "id": "6tXmyitL7jy2"
   },
   "outputs": [],
   "source": [
    "# plot if you wanna\n",
    "#tf.keras.utils.plot_model(model_v5, \"yolo_v5.png\", show_shapes=True, show_layer_names=True, expand_nested=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AS4ZcPQg7jy2"
   },
   "source": [
    "For optimizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "executionInfo": {
     "elapsed": 34,
     "status": "ok",
     "timestamp": 1638305359523,
     "user": {
      "displayName": "Jill Naiman",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "13328008118965652761"
     },
     "user_tz": 360
    },
    "id": "IWLTDSz77jy2"
   },
   "outputs": [],
   "source": [
    "#LRrate = 0.004\n",
    "LRrate = 0.002\n",
    "\n",
    "class CosineLR(tf.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self,steps):\n",
    "        super().__init__()\n",
    "        self.lr = LRrate * batch_size / 64\n",
    "        self.warmup_init = LRrate/10.\n",
    "        self.warmup_step = steps\n",
    "        self.decay_steps = tf.cast((num_epochs - 1) * self.warmup_step, tf.float32)\n",
    "\n",
    "    def __call__(self, step):\n",
    "        linear_warmup = tf.cast(step, dtype=tf.float32) / self.warmup_step * (self.lr - self.warmup_init)\n",
    "        cosine_lr = 0.5 * self.lr * (1 + tf.cos(math.pi * tf.cast(step, tf.float32) / self.decay_steps))\n",
    "        return tf.where(step < self.warmup_step, self.warmup_init + linear_warmup, cosine_lr)\n",
    "\n",
    "    def get_config(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "executionInfo": {
     "elapsed": 34,
     "status": "ok",
     "timestamp": 1638305359524,
     "user": {
      "displayName": "Jill Naiman",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "13328008118965652761"
     },
     "user_tz": 360
    },
    "id": "JpreDZDZvvaW"
   },
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(CosineLR(steps_training), 0.937)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "executionInfo": {
     "elapsed": 34,
     "status": "ok",
     "timestamp": 1638305359524,
     "user": {
      "displayName": "Jill Naiman",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "13328008118965652761"
     },
     "user_tz": 360
    },
    "id": "_l3BLK6XDRms"
   },
   "outputs": [],
   "source": [
    "# load weights if you wanna\n",
    "if saved_weights_file is not None:\n",
    "    weightsFiles = glob.glob(weightsDir + 'weights/' + '*h5')\n",
    "    # OR\n",
    "    if saved_weights_file is not None:\n",
    "      weightsFiles = [classDirMain+saved_weights_file]\n",
    "    model.load_weights(weightsFiles[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 34,
     "status": "ok",
     "timestamp": 1638305359524,
     "user": {
      "displayName": "Jill Naiman",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "13328008118965652761"
     },
     "user_tz": 360
    },
    "id": "3MAgw3DHD_nA",
    "outputId": "df181c03-e403-4537-e6fc-05fc98946489"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0003125"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print(weightsFiles)\n",
    "optimizer.learning_rate.lr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e6Y0CbKb0T76"
   },
   "source": [
    "For saving checkpoints:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1638305359525,
     "user": {
      "displayName": "Jill Naiman",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "13328008118965652761"
     },
     "user_tz": 360
    },
    "id": "REcMXF7W0T76"
   },
   "outputs": [],
   "source": [
    "# for saving model\n",
    "#save_model_name = chksDir + 'checkpoints/'+'model' + str(DATE.today().year).zfill(4) + str(DATE.today().month).zfill(2) + str(DATE.today().day).zfill(2) +'.h5'\n",
    "#checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(save_model_name, save_best_only=True)\n",
    "# today = str(DATE.today().year).zfill(4) + str(DATE.today().month).zfill(2) + str(DATE.today().day).zfill(2)\n",
    "# if not os.path.exists(chksDir + 'checkpoints/'+today):\n",
    "#     os.mkdir(chksDir + 'checkpoints/'+today)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1638305359525,
     "user": {
      "displayName": "Jill Naiman",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "13328008118965652761"
     },
     "user_tz": 360
    },
    "id": "h2o2DlWC0T77"
   },
   "outputs": [],
   "source": [
    "#model2 = tf.saved_model.load(save_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1638305359526,
     "user": {
      "displayName": "Jill Naiman",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "13328008118965652761"
     },
     "user_tz": 360
    },
    "id": "gBbSfeKP0T77"
   },
   "outputs": [],
   "source": [
    "# if restart_from_checkpoints:\n",
    "#     if saved_model_file is None:\n",
    "#         files = glob.glob(chksDir + 'checkpoints/*')\n",
    "#         files.sort()\n",
    "#         model = tf.saved_model.load(files[-1])\n",
    "#         fname = files[-1]\n",
    "#     else:\n",
    "#         model = tf.saved_model.load(chksDir + 'checkpoints/' +saved_model_file)\n",
    "#         fname = chksDir + 'checkpoints/' +saved_model_file\n",
    "#     print('Loading model from', fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RKk7HxgC7jy3"
   },
   "source": [
    "# For processing data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1638305359526,
     "user": {
      "displayName": "Jill Naiman",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "13328008118965652761"
     },
     "user_tz": 360
    },
    "id": "KFOuGj5l0T77",
    "outputId": "0060855f-3b00-47a4-b3bb-9f6eb4786ff2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/jillnaiman/MegaYolo/'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classDirMain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1638305359526,
     "user": {
      "displayName": "Jill Naiman",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "13328008118965652761"
     },
     "user_tz": 360
    },
    "id": "qqdxpzdoBb6H",
    "outputId": "04774847-7c14-4b6f-a99f-902d04403e71"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/Users/jillnaiman/MegaYolo/binaries_model8/',\n",
       " '/Users/jillnaiman/MegaYolo/yolo_512x512_ann/')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classDir_main_to_imgs, classDir_main_to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1638305359527,
     "user": {
      "displayName": "Jill Naiman",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "13328008118965652761"
     },
     "user_tz": 360
    },
    "id": "-L428Dy7VxBV"
   },
   "outputs": [],
   "source": [
    "#import mega_yolo_utils\n",
    "#reload(mega_yolo_utils)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1638305359527,
     "user": {
      "displayName": "Jill Naiman",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "13328008118965652761"
     },
     "user_tz": 360
    },
    "id": "RiM8WUcKGrFn"
   },
   "outputs": [],
   "source": [
    "from mega_yolo_utils import augmentation_generator, csv_gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1638305360505,
     "user": {
      "displayName": "Jill Naiman",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "13328008118965652761"
     },
     "user_tz": 360
    },
    "id": "ewFdU9-77jy3"
   },
   "outputs": [],
   "source": [
    "splitsDirIn = None\n",
    "if onGoogle: splitsDirIn = classDirMain\n",
    "train_gen_csv = csv_gen('train', splitsDir=splitsDirIn)\n",
    "valid_gen_csv = csv_gen('valid',splitsDir=splitsDirIn)\n",
    "test_gen_csv = csv_gen('test',splitsDir=splitsDirIn)\n",
    "\n",
    "\n",
    "def dataset_gen(split, batch_size):\n",
    "    while True:\n",
    "        true_boxes = []; imgs = []; files = []\n",
    "        \n",
    "        while len(files) < batch_size:\n",
    "            if type(split) == str:\n",
    "                if b'train' in bytes(split, encoding='utf8'): # NOTE, must be bytes here!!!\n",
    "                    line = next(train_gen_csv)\n",
    "                elif b'valid' in bytes(split, encoding='utf8'):\n",
    "                    line = next(valid_gen_csv)\n",
    "                elif b'test' in bytes(split, encoding='utf8'):\n",
    "                    line = next(test_gen_csv)\n",
    "            else:\n",
    "                if b'train' in split: # NOTE, must be bytes here!!!\n",
    "                    line = next(train_gen_csv)\n",
    "                elif b'valid' in split:\n",
    "                    line = next(valid_gen_csv)\n",
    "                elif b'test' in split:\n",
    "                    line = next(test_gen_csv)\n",
    "            \n",
    "            files.append(line.strip())\n",
    "\n",
    "        # parse and get full names\n",
    "        try:\n",
    "            imgs_name, bbox = parse_annotation(files, LABELS, \n",
    "                                               feature_dir=classDir_main_to_imgs,\n",
    "                                               annotation_dir=classDir_main_to)\n",
    "        except:\n",
    "            print('error parsing:', imgs_name)\n",
    "        # do a debug check\n",
    "        for im in imgs_name:\n",
    "            if '.npz' not in im:\n",
    "                print('no np file')\n",
    "                import sys; sys.exit()\n",
    "        \n",
    "        # read in and keep images -- npy files\n",
    "        for im in imgs_name:\n",
    "            b = np.load(im)['arr_0']\n",
    "            \n",
    "            ########### DEBUGGING ##########\n",
    "            #b = b[:,:,:3]\n",
    "            ################################\n",
    "            \n",
    "            # convert 0-1\n",
    "            b = b/255.0\n",
    "            imgs.append(b)\n",
    "        \n",
    "        # finally, format for output\n",
    "        y_true1, y_true2, y_true3 = [],[],[]\n",
    "        for b in bbox:\n",
    "            y1,y2,y3= process_box(b[:,:4], b[:,4].astype('int'),anchors,CLASS)\n",
    "            y_true1.append(y1); y_true2.append(y2); y_true3.append(y3)\n",
    "        # if there is no box, do something different\n",
    "        if len(bbox) == 0:\n",
    "            # fake a box\n",
    "            b = np.array([[111.,  59., 403., 364. ,  4.]])\n",
    "            y1,y2,y3= process_box(b[:,:4], b[:,4].astype('int'),anchors,CLASS)\n",
    "            y1[:] = 0; y2[:]=0;y3[:]=0\n",
    "            y_true1.append(y1); y_true2.append(y2); y_true3.append(y3)        \n",
    "        img = tf.cast(np.array(imgs), tf.float32)        \n",
    "        yield img, tf.cast(y_true1, tf.float32), tf.cast(y_true2, tf.float32), tf.cast(y_true3, tf.float32)\n",
    "        \n",
    "\n",
    "def dataset_gen_for_aug(split, batch_size): # for training/validation datasets\n",
    "    while True:\n",
    "        true_boxes = []; imgs = []; files = []\n",
    "        \n",
    "        while len(files) < batch_size:\n",
    "            if type(split) == str:\n",
    "                if b'train' in bytes(split, encoding='utf8'): # NOTE, must be bytes here!!!\n",
    "                    line = next(train_gen_csv)\n",
    "                elif b'valid' in bytes(split, encoding='utf8'):\n",
    "                    line = next(valid_gen_csv)\n",
    "                elif b'test' in bytes(split, encoding='utf8'):\n",
    "                    line = next(test_gen_csv)\n",
    "            else:\n",
    "                if b'train' in split: # NOTE, must be bytes here!!!\n",
    "                    line = next(train_gen_csv)\n",
    "                elif b'valid' in split:\n",
    "                    line = next(valid_gen_csv)\n",
    "                elif b'test' in split:\n",
    "                    line = next(test_gen_csv)\n",
    "            \n",
    "            files.append(line.strip())\n",
    "\n",
    "        # parse and get full names\n",
    "        imgs_name, bbox = parse_annotation(files, LABELS, \n",
    "                                           feature_dir=classDir_main_to_imgs,\n",
    "                                            annotation_dir=classDir_main_to)\n",
    "        # do a debug check\n",
    "        for im in imgs_name:\n",
    "            if '.npz' not in im:\n",
    "                print('no np file')\n",
    "                import sys; sys.exit()\n",
    "        \n",
    "        # read in and keep images -- npy files\n",
    "        for im in imgs_name:\n",
    "            b = np.load(im)['arr_0']\n",
    "            # convert 0-1\n",
    "            b = b/255.0\n",
    "            imgs.append(b)\n",
    "                \n",
    "        img = tf.cast(np.array(imgs), tf.float32)        \n",
    "        yield img, tf.cast(bbox, tf.float32)\n",
    "        \n",
    "\n",
    "def get_dataset(split, labels, batch_size, use_aug=True):\n",
    "    if use_aug and ('test' not in split.lower()):\n",
    "        dataset = tf.data.Dataset.from_generator(dataset_gen_for_aug, args=[split, batch_size],\n",
    "                                                 output_types = (tf.float32, tf.float32))\n",
    "    else:\n",
    "        dataset = tf.data.Dataset.from_generator(dataset_gen, args=[split, batch_size],\n",
    "                                             output_types = (tf.float32, tf.float32, \n",
    "                                                             tf.float32, tf.float32))\n",
    "                                             \n",
    "    #dataset = dataset.prefetch(10)\n",
    "    dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
    "    \n",
    "    # maybe?\n",
    "    iterator = iter(dataset)\n",
    "\n",
    "    #return dataset\n",
    "    return iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitsDirIn = None\n",
    "if onGoogle: splitsDirIn = classDirMain\n",
    "train_gen_csv = csv_gen('train', splitsDir=splitsDirIn)\n",
    "valid_gen_csv = csv_gen('valid',splitsDir=splitsDirIn)\n",
    "test_gen_csv = csv_gen('test',splitsDir=splitsDirIn)\n",
    "\n",
    "class ArtificialDatasetNonAug(tf.data.Dataset):\n",
    "\n",
    "    #@tf.function\n",
    "    def _dataset_gen(split, batch_size):\n",
    "        while True:\n",
    "            true_boxes = []; imgs = []; files = []\n",
    "\n",
    "            while len(files) < batch_size:\n",
    "                if type(split) == str:\n",
    "                    if b'train' in bytes(split, encoding='utf8'): # NOTE, must be bytes here!!!\n",
    "                        line = next(train_gen_csv)\n",
    "                    elif b'valid' in bytes(split, encoding='utf8'):\n",
    "                        line = next(valid_gen_csv)\n",
    "                    elif b'test' in bytes(split, encoding='utf8'):\n",
    "                        line = next(test_gen_csv)\n",
    "                else:\n",
    "                    if b'train' in split: # NOTE, must be bytes here!!!\n",
    "                        line = next(train_gen_csv)\n",
    "                    elif b'valid' in split:\n",
    "                        line = next(valid_gen_csv)\n",
    "                    elif b'test' in split:\n",
    "                        line = next(test_gen_csv)\n",
    "\n",
    "                files.append(line.strip())\n",
    "\n",
    "            # parse and get full names\n",
    "            try:\n",
    "                imgs_name, bbox = parse_annotation(files, LABELS, \n",
    "                                                   feature_dir=classDir_main_to_imgs,\n",
    "                                                   annotation_dir=classDir_main_to)\n",
    "            except:\n",
    "                print('error parsing:', imgs_name)\n",
    "            # do a debug check\n",
    "            for im in imgs_name:\n",
    "                if '.npz' not in im:\n",
    "                    print('no np file')\n",
    "                    import sys; sys.exit()\n",
    "\n",
    "            # read in and keep images -- npy files\n",
    "            for im in imgs_name:\n",
    "                b = np.load(im)['arr_0']\n",
    "\n",
    "                ########### DEBUGGING ##########\n",
    "                #b = b[:,:,:3]\n",
    "                ################################\n",
    "\n",
    "                # convert 0-1\n",
    "                b = b/255.0\n",
    "                imgs.append(b)\n",
    "\n",
    "            # finally, format for output\n",
    "            y_true1, y_true2, y_true3 = [],[],[]\n",
    "            for b in bbox:\n",
    "                y1,y2,y3= process_box(b[:,:4], b[:,4].astype('int'),anchors,CLASS)\n",
    "                y_true1.append(y1); y_true2.append(y2); y_true3.append(y3)\n",
    "            # if there is no box, do something different\n",
    "            if len(bbox) == 0:\n",
    "                # fake a box\n",
    "                b = np.array([[111.,  59., 403., 364. ,  4.]])\n",
    "                y1,y2,y3= process_box(b[:,:4], b[:,4].astype('int'),anchors,CLASS)\n",
    "                y1[:] = 0; y2[:]=0;y3[:]=0\n",
    "                y_true1.append(y1); y_true2.append(y2); y_true3.append(y3)        \n",
    "            img = tf.cast(np.array(imgs), tf.float32)        \n",
    "            yield img, tf.cast(y_true1, tf.float32), tf.cast(y_true2, tf.float32), tf.cast(y_true3, tf.float32)\n",
    "\n",
    "\n",
    "\n",
    "    def __new__(cls,batch_size=32,split='valid'):\n",
    "        dataset = tf.data.Dataset.from_generator(cls._dataset_gen, args=[split, batch_size],\n",
    "                                             output_types = (tf.float32, tf.float32, \n",
    "                                                             tf.float32, tf.float32))\n",
    "        return dataset\n",
    "\n",
    "#         #dataset = dataset.prefetch(10)\n",
    "#         dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "#         # maybe?\n",
    "#         iterator = iter(dataset)\n",
    "\n",
    "#         #return dataset\n",
    "#         return iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1638305360505,
     "user": {
      "displayName": "Jill Naiman",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "13328008118965652761"
     },
     "user_tz": 360
    },
    "id": "iCHTV-qu7jy4"
   },
   "outputs": [],
   "source": [
    "# # grab data!\n",
    "# train_dataset = None\n",
    "# # train_dataset= get_dataset('train', LABELS, TRAIN_BATCH_SIZE)\n",
    "\n",
    "# val_dataset = None\n",
    "# val_dataset= get_dataset('valid', LABELS,VAL_BATCH_SIZE,use_aug=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ArtificialDatasetNonAug(batch_size=32,split='valid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# benchmark tests\n",
    "import time\n",
    "t1 = time.perf_counter()\n",
    "print(t1)\n",
    "@tf.function\n",
    "def benchmark(dataset, num_epochs=1, nbatch=32):\n",
    "    start_time = time.perf_counter()\n",
    "    for epoch_num in range(num_epochs):\n",
    "        ienum=0\n",
    "        for sample in dataset:\n",
    "            if ienum%10==0: print(ienum)\n",
    "            # Performing a training step\n",
    "            time.sleep(0.01)\n",
    "            ienum+=1\n",
    "            if ienum>nbatch: break\n",
    "    print(\"Execution time:\", time.perf_counter() - start_time, time.perf_counter())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"args_1:0\", shape=(), dtype=int32)\n",
      "Execution time: 0.04838154900016889 2586.00342969\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/t5/9xgccmv92hnfvjwd62mk8zqh0000gn/T/ipykernel_9839/3746369835.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbenchmark\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mArtificialDatasetNonAug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'valid'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# new paradigm, no prefetch 95.88\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/Paper1/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/Paper1/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    893\u001b[0m       \u001b[0;31m# If we did not create any variables the trace we have is good enough.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       return self._concrete_stateful_fn._call_flat(\n\u001b[0;32m--> 895\u001b[0;31m           filtered_flat_args, self._concrete_stateful_fn.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    896\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfn_with_cond\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minner_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minner_kwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minner_filtered_flat_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/Paper1/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1917\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1918\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1919\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1921\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/Paper1/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    558\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 560\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    561\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m~/anaconda3/envs/Paper1/lib/python3.7/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "benchmark(ArtificialDatasetNonAug(batch_size=32,split='valid')) # new paradigm, no prefetch 95.88"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "Execution time: 99.58878045300003\n"
     ]
    }
   ],
   "source": [
    "benchmark(ArtificialDatasetNonAug(batch_size=32,split='valid').prefetch(tf.data.AUTOTUNE)) # new paradigm, with prefetch, 99.58, but maybe better on other systems?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /var/folders/t5/9xgccmv92hnfvjwd62mk8zqh0000gn/T/ipykernel_9839/425625053.py:76: calling DatasetV2.from_generator (from tensorflow.python.data.ops.dataset_ops) with output_types is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use output_signature instead\n",
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "Execution time: 97.36569318700003\n"
     ]
    }
   ],
   "source": [
    "benchmark(\n",
    "    tf.data.Dataset.range(2)\n",
    "    .interleave(lambda _: ArtificialDatasetNonAug(batch_size=32,split='valid'))\n",
    ") # 97.365"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "Execution time: 73.844264458\n"
     ]
    }
   ],
   "source": [
    "benchmark(\n",
    "    tf.data.Dataset.range(2)\n",
    "    .interleave(\n",
    "        lambda _: ArtificialDatasetNonAug(batch_size=32,split='valid'),\n",
    "        num_parallel_calls=tf.data.AUTOTUNE\n",
    "    )\n",
    ") # 73.844"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "Execution time: 85.55944432500019\n"
     ]
    }
   ],
   "source": [
    "benchmark(\n",
    "    tf.data.Dataset.range(2)\n",
    "    .interleave(\n",
    "        lambda _: ArtificialDatasetNonAug(batch_size=32,split='valid'),\n",
    "        num_parallel_calls=tf.data.AUTOTUNE\n",
    "    ).shuffle(3)\n",
    ") # 85.59"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time: 27.734012822000068\n"
     ]
    }
   ],
   "source": [
    "benchmark(val_dataset) \n",
    "# naive approach gives: 19.24 sec\n",
    "# with prefetch(tf.data.AUTOTUNE): 27.73 sec (maybe its better with different systems?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "`map_func` must return a `Dataset` object. Got <class 'tensorflow.python.data.ops.iterator_ops.IteratorSpec'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/t5/9xgccmv92hnfvjwd62mk8zqh0000gn/T/ipykernel_9839/2554311210.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m     .interleave(\n\u001b[1;32m      4\u001b[0m         \u001b[0;32mlambda\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mval_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0mnum_parallel_calls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAUTOTUNE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     )\n\u001b[1;32m      7\u001b[0m )\n",
      "\u001b[0;32m~/anaconda3/envs/Paper1/lib/python3.7/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36minterleave\u001b[0;34m(self, map_func, cycle_length, block_length, num_parallel_calls, deterministic)\u001b[0m\n\u001b[1;32m   1947\u001b[0m           \u001b[0mblock_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1948\u001b[0m           \u001b[0mnum_parallel_calls\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1949\u001b[0;31m           deterministic=deterministic)\n\u001b[0m\u001b[1;32m   1950\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1951\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/Paper1/lib/python3.7/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, input_dataset, map_func, cycle_length, block_length, num_parallel_calls, buffer_output_elements, prefetch_input_elements, deterministic)\u001b[0m\n\u001b[1;32m   4365\u001b[0m       raise TypeError(\n\u001b[1;32m   4366\u001b[0m           \"`map_func` must return a `Dataset` object. Got {}\".format(\n\u001b[0;32m-> 4367\u001b[0;31m               type(self._map_func.output_structure)))\n\u001b[0m\u001b[1;32m   4368\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_structure\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_map_func\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_structure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_element_spec\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4369\u001b[0m     self._cycle_length = ops.convert_to_tensor(\n",
      "\u001b[0;31mTypeError\u001b[0m: `map_func` must return a `Dataset` object. Got <class 'tensorflow.python.data.ops.iterator_ops.IteratorSpec'>"
     ]
    }
   ],
   "source": [
    "benchmark(\n",
    "    tf.data.Dataset.range(2)\n",
    "    .interleave(\n",
    "        lambda _: val_dataset,\n",
    "        num_parallel_calls=tf.data.AUTOTUNE\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1638305360506,
     "user": {
      "displayName": "Jill Naiman",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "13328008118965652761"
     },
     "user_tz": 360
    },
    "id": "lsQQuI7dzZJw"
   },
   "outputs": [],
   "source": [
    "#next(valid_gen_csv)\n",
    "#next(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1638305360506,
     "user": {
      "displayName": "Jill Naiman",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "13328008118965652761"
     },
     "user_tz": 360
    },
    "id": "OfxcXCqPyXZj"
   },
   "outputs": [],
   "source": [
    "#val_dataset= get_dataset('valid', LABELS, VAL_BATCH_SIZE,use_aug=True)\n",
    "#next(val_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uvgdY6ly7jy4"
   },
   "source": [
    "Including Augmentation like a boss!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1638305360506,
     "user": {
      "displayName": "Jill Naiman",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "13328008118965652761"
     },
     "user_tz": 360
    },
    "id": "nm5833D_7jy5"
   },
   "outputs": [],
   "source": [
    "aug_train_dataset = augmentation_generator(train_dataset, anchors, CLASS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b5doQu9r7jy5"
   },
   "source": [
    "For calculating the loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1638305360507,
     "user": {
      "displayName": "Jill Naiman",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "13328008118965652761"
     },
     "user_tz": 360
    },
    "id": "oWeoMEdE7jy5"
   },
   "outputs": [],
   "source": [
    "class ComputeLoss(object):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    @staticmethod\n",
    "    def compute_loss(y_pred, y_true, anchors):\n",
    "        grid_size = tf.shape(y_pred)[1:3]\n",
    "        ratio = tf.cast(tf.constant([image_size, image_size]) / grid_size, tf.float32)\n",
    "        batch_size = tf.cast(tf.shape(y_pred)[0], tf.float32)\n",
    "\n",
    "        x_y_offset, pred_boxes, pred_conf, pred_prob = process_layer(y_pred, anchors,CLASS)\n",
    "\n",
    "        object_mask = y_true[..., 4:5]\n",
    "\n",
    "        def cond(idx, _):\n",
    "            return tf.less(idx, tf.cast(batch_size, tf.int32))\n",
    "\n",
    "        def body(idx, mask):\n",
    "            valid_true_boxes = tf.boolean_mask(y_true[idx, ..., 0:4],\n",
    "                                               tf.cast(object_mask[idx, ..., 0], 'bool'))\n",
    "            iou = box_iou(pred_boxes[idx], valid_true_boxes)\n",
    "            return idx + 1, mask.write(idx, tf.cast(tf.reduce_max(iou, axis=-1) < 0.2, tf.float32))\n",
    "\n",
    "        ignore_mask = tf.TensorArray(dtype=tf.float32, size=0, dynamic_size=True)\n",
    "        \n",
    "        #print('here1.1')\n",
    "        #print(cond, body, ignore_mask)\n",
    "\n",
    "        _, ignore_mask = tf.while_loop(cond=cond, body=body, loop_vars=[0, ignore_mask])\n",
    "        \n",
    "        #print('here1.2')\n",
    "        ignore_mask = ignore_mask.stack()\n",
    "        ignore_mask = tf.expand_dims(ignore_mask, -1)\n",
    "\n",
    "        true_xy = y_true[..., 0:2] / ratio[::-1] - x_y_offset\n",
    "        pred_xy = pred_boxes[..., 0:2] / ratio[::-1] - x_y_offset\n",
    "\n",
    "        true_tw_th = y_true[..., 2:4] / anchors\n",
    "        pred_tw_th = pred_boxes[..., 2:4] / anchors\n",
    "        true_tw_th = tf.where(tf.equal(true_tw_th, 0), tf.ones_like(true_tw_th), true_tw_th)\n",
    "        pred_tw_th = tf.where(tf.equal(pred_tw_th, 0), tf.ones_like(pred_tw_th), pred_tw_th)\n",
    "        true_tw_th = tf.math.log(tf.clip_by_value(true_tw_th, 1e-9, 1e+9))\n",
    "        pred_tw_th = tf.math.log(tf.clip_by_value(pred_tw_th, 1e-9, 1e+9))\n",
    "\n",
    "        box_loss_scale = y_true[..., 2:3] * y_true[..., 3:4]\n",
    "        box_loss_scale = 2. - box_loss_scale / tf.cast(image_size ** 2, tf.float32)\n",
    "\n",
    "        xy_loss = tf.reduce_sum(tf.square(true_xy - pred_xy) * object_mask * box_loss_scale)\n",
    "        wh_loss = tf.reduce_sum(tf.square(true_tw_th - pred_tw_th) * object_mask * box_loss_scale)\n",
    "\n",
    "        conf_pos_mask = object_mask\n",
    "        conf_neg_mask = (1 - object_mask) * ignore_mask\n",
    "        conf_loss_pos = conf_pos_mask * tf.nn.sigmoid_cross_entropy_with_logits(labels=object_mask, logits=pred_conf)\n",
    "        conf_loss_neg = conf_neg_mask * tf.nn.sigmoid_cross_entropy_with_logits(labels=object_mask, logits=pred_conf)\n",
    "        # try this\n",
    "        #conf_loss_pos = conf_pos_mask * -tf.reduce_sum(object_mask*tf.math.log(tf.clip_by_value(pred_conf,1e-10,1.0)))\n",
    "        #conf_loss_neg = conf_neg_mask * -tf.reduce_sum(object_mask*tf.math.log(tf.clip_by_value(pred_conf,1e-10,1.0)))\n",
    "\n",
    "\n",
    "        conf_loss = tf.reduce_sum((conf_loss_pos + conf_loss_neg))\n",
    "\n",
    "        true_conf = y_true[..., 5:]\n",
    "\n",
    "        class_loss = object_mask * tf.nn.sigmoid_cross_entropy_with_logits(true_conf, pred_prob)\n",
    "        #class_loss = object_mask * -tf.reduce_sum(true_conf*tf.math.log(tf.clip_by_value(pred_conf,1e-10,1.0)))\n",
    "        #class_loss = object_mask * tf.losses.categorical_crossentropy(true_conf, pred_prob)\n",
    "        #tf.losses.sparse_softmax_cross_entropy(y, logits)\n",
    "        class_loss = tf.reduce_sum(class_loss) # sum across all -- 1 number for loss\n",
    "\n",
    "        if np.isnan(xy_loss):\n",
    "          print('xy_loss is NaN')\n",
    "        if np.isnan(wh_loss):\n",
    "          print('wh_loss is NaN')\n",
    "        if np.isnan(conf_loss):\n",
    "          print('conf_loss is NaN')#, conf_loss_pos, conf_loss_neg)\n",
    "        if np.isnan(class_loss):\n",
    "          print('class_loss is NaN')\n",
    "\n",
    "        if np.isnan(xy_loss + wh_loss + conf_loss + class_loss):\n",
    "          print('--- object mask ---')\n",
    "          print(object_mask.numpy().shape, pred_conf.numpy().shape, true_conf.numpy().shape)\n",
    "          print(object_mask.numpy().max(), object_mask.numpy().min())\n",
    "          print(object_mask)\n",
    "          print(' ')\n",
    "          print('--------')\n",
    "        #else:\n",
    "        #  print(object_mask.numpy().max(), object_mask.numpy().min())\n",
    "\n",
    "        return xy_loss + wh_loss + conf_loss + class_loss\n",
    "\n",
    "    def __call__(self, y_pred, y_true):\n",
    "        loss = 0.\n",
    "        anchor_group = [anchors[6:9], anchors[3:6], anchors[0:3]]\n",
    "\n",
    "        for i in range(len(y_pred)):\n",
    "            loss += self.compute_loss(y_pred[i], y_true[i], anchor_group[i])\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1638305360507,
     "user": {
      "displayName": "Jill Naiman",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "13328008118965652761"
     },
     "user_tz": 360
    },
    "id": "XZZsr5247jy6"
   },
   "outputs": [],
   "source": [
    "loss_object = ComputeLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1638305360507,
     "user": {
      "displayName": "Jill Naiman",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "13328008118965652761"
     },
     "user_tz": 360
    },
    "id": "9lBB1U-k0T7-"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1638305360508,
     "user": {
      "displayName": "Jill Naiman",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "13328008118965652761"
     },
     "user_tz": 360
    },
    "id": "ZbiMja467jy6"
   },
   "outputs": [],
   "source": [
    "def compute_loss(y_true, y_pred):\n",
    "    total_loss = loss_object(y_pred, y_true)\n",
    "    return tf.reduce_sum(total_loss) / batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1638305360508,
     "user": {
      "displayName": "Jill Naiman",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "13328008118965652761"
     },
     "user_tz": 360
    },
    "id": "fbZopKn57jy6"
   },
   "outputs": [],
   "source": [
    "def train_step(image, y_true):\n",
    "    with tf.GradientTape() as tape:\n",
    "        y_pred = model(image, training=True)\n",
    "        loss = compute_loss(y_true, y_pred)\n",
    "    if not np.isnan(loss):\n",
    "        variables = model.trainable_variables\n",
    "        gradients = tape.gradient(loss, variables)\n",
    "        #if np.isnan(loss):\n",
    "        #  print('nan')\n",
    "        #  print\n",
    "        optimizer.apply_gradients(zip(gradients, variables))\n",
    "    else: # this will stop if we have non-convergence \n",
    "        print('is NaN -- probably want to lower your learning rate!!!!')\n",
    "        import sys; sys.exit()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1638305360508,
     "user": {
      "displayName": "Jill Naiman",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "13328008118965652761"
     },
     "user_tz": 360
    },
    "id": "l1ftaeHUvvab"
   },
   "outputs": [],
   "source": [
    "# save weights\n",
    "def save_best_weights(model, name, val_loss_avg):\n",
    "    # delete existing weights file\n",
    "    files = glob.glob(os.path.join(weightsDir + 'weights/', name + '*'))\n",
    "    for file in files:\n",
    "        os.remove(file)\n",
    "    # create new weights file\n",
    "    name = name + '_model_' +version + str(val_loss_avg) + '.h5'\n",
    "    path_name = os.path.join(weightsDir +'weights/', name)\n",
    "    model.save_weights(path_name)\n",
    "    \n",
    "    \n",
    "# log (tensorboard)\n",
    "def log_loss(loss, val_loss, step):\n",
    "    tf.summary.scalar('loss', loss, step)\n",
    "    tf.summary.scalar('val_loss', val_loss, step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1638305360509,
     "user": {
      "displayName": "Jill Naiman",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "13328008118965652761"
     },
     "user_tz": 360
    },
    "id": "C19AoZV9loKh",
    "outputId": "e1b08851-f8e4-42ce-da75-ec9bd3dfcfce"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'./classifications/yolo_512x512_ann/'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classDir_main_to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1638305360509,
     "user": {
      "displayName": "Jill Naiman",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "13328008118965652761"
     },
     "user_tz": 360
    },
    "id": "omK8Zjfc7jy6"
   },
   "outputs": [],
   "source": [
    "# training\n",
    "def train(epochs, model, train_dataset, val_dataset, steps_per_epoch_train, \n",
    "          steps_per_epoch_val, optimizer, train_name = 'train'):\n",
    "    '''\n",
    "    Train YOLO model for n epochs.\n",
    "    Eval loss on training and validation dataset.\n",
    "    Log training loss and validation loss for tensorboard.\n",
    "    Save best weights during training (according to validation loss).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    - epochs : integer, number of epochs to train the model.\n",
    "    - model : YOLO model.\n",
    "    - train_dataset : YOLO ground truth and image generator from training dataset.\n",
    "    - val_dataset : YOLO ground truth and image generator from validation dataset.\n",
    "    - steps_per_epoch_train : integer, number of batch to complete one epoch for train_dataset.\n",
    "    - steps_per_epoch_val : integer, number of batch to complete one epoch for val_dataset.\n",
    "    - train_name : string, training name used to log loss and save weights.\n",
    "    \n",
    "    Notes :\n",
    "    - train_dataset and val_dataset generate YOLO ground truth tensors : detector_mask,\n",
    "      matching_true_boxes, class_one_hot, true_boxes_grid. Shape of these tensors (batch size, tensor shape).\n",
    "    - steps per epoch = number of images in dataset // batch size of dataset\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    - loss history : [train_loss_history, val_loss_history] : list of average loss for each epoch.\n",
    "    '''\n",
    "    num_epochs1 = epochs\n",
    "    steps_per_epoch_train = steps_per_epoch_train\n",
    "    steps_per_epoch_val = steps_per_epoch_val\n",
    "    train_loss_history = []\n",
    "    val_loss_history = []\n",
    "    best_val_loss = 1e6\n",
    "    \n",
    "    # optimizer\n",
    "    #optimizer = tf.keras.optimizers.Adam(learning_rate=1e-5, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
    "    \n",
    "    # log (tensorboard)\n",
    "    summary_writer = tf.summary.create_file_writer(os.path.join(logsDir+'logs/', train_name), \n",
    "                                                   flush_millis=20000)\n",
    "    summary_writer.set_as_default()\n",
    "    \n",
    "    # training\n",
    "    for epoch in range(num_epochs1):\n",
    "        epoch_loss = []\n",
    "        epoch_val_loss = []\n",
    "        epoch_val_sub_loss = []\n",
    "        print('Epoch {} :'.format(epoch))\n",
    "        # train\n",
    "        for batch_idx in range(steps_per_epoch_train):        \n",
    "            image, y_true_1, y_true_2, y_true_3 = next(train_dataset)\n",
    "            y_true = (y_true_1, y_true_2, y_true_3)\n",
    "            loss = train_step(image, y_true)\n",
    "            # check for nans\n",
    "            optOrig = optimizer.learning_rate.lr\n",
    "            while np.isnan(loss):\n",
    "              print('loss nan')\n",
    "              optimizer.learning_rate.lr *= 0.5\n",
    "              loss = train_step(image, y_true)\n",
    "            epoch_loss.append(loss)\n",
    "            print('-', end='')\n",
    "        print(' | ', end='')\n",
    "        # val\n",
    "        for batch_idx in range(steps_per_epoch_val): \n",
    "            image, y_true_1, y_true_2, y_true_3 = next(val_dataset)\n",
    "            y_true = (y_true_1, y_true_2, y_true_3)\n",
    "            loss = train_step(image, y_true)\n",
    "            epoch_val_loss.append(loss)\n",
    "            print('-', end='')\n",
    "\n",
    "        loss_avg = np.mean(np.array(epoch_loss))\n",
    "        val_loss_avg = np.mean(np.array(epoch_val_loss))\n",
    "        train_loss_history.append(loss_avg)\n",
    "        val_loss_history.append(val_loss_avg)\n",
    "        \n",
    "        # log\n",
    "        log_loss(loss_avg, val_loss_avg, epoch)\n",
    "        \n",
    "        # save\n",
    "        if val_loss_avg < best_val_loss:\n",
    "            save_best_weights(model, train_name, val_loss_avg)\n",
    "            best_val_loss = val_loss_avg\n",
    "            #tf.saved_model.save(model, chksDir + 'checkpoints/'+today)        \n",
    "        print(' loss = {:.4f}, val_loss = {:.4f}'.format(loss_avg, val_loss_avg))\n",
    "        \n",
    "    return [train_loss_history, val_loss_history]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "l2FpED3Dvvac",
    "outputId": "53dfa0c1-8492-4858-d7c6-e9e5ae480a29"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 :\n",
      "-----------------------"
     ]
    }
   ],
   "source": [
    "results = train(num_epochs, model, aug_train_dataset, val_dataset, \n",
    "               steps_training, steps_val, optimizer,'training_1'+extraName)\n",
    "\n",
    "# debug\n",
    "#results = train(num_epochs, model, aug_train_dataset, val_dataset, \n",
    "#               1, 1, optimizer,'training_1'+extraName)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oqGPwg0R7jy6"
   },
   "source": [
    "Plot diagnostics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "68HC1xUF7jy6"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(30,14))\n",
    "ax.plot(results[0], label='Training Loss')\n",
    "ax.plot(results[1], label='Validation Loss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EjYMDlLo7jy7"
   },
   "outputs": [],
   "source": [
    "# imgs_name, bbox = parse_annotation([X_train[0]], LABELS, \n",
    "#                                    classDir_main_to_imgs=classDir_main_to_imgs,\n",
    "#                                        classDir_main_to_ann=classDir_main_to)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "theCs-467jy8"
   },
   "outputs": [],
   "source": [
    "# from importlib import reload\n",
    "# reload(parse_annotation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "orYNx10Y7jy8"
   },
   "outputs": [],
   "source": [
    "#import general_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1OXBhSDS7jy8"
   },
   "outputs": [],
   "source": [
    "#reload(general_utils)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tKhsRWNh7jy8"
   },
   "outputs": [],
   "source": [
    "#from general_utils import parse_annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8eIrQ2HH5C8Z"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wXSKvTs95Pp6"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ihigRX4m5QaB"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "mega_yolo_train.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
