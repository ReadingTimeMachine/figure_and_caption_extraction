{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iC4Y4O7bneag"
   },
   "source": [
    "# Mega Yolo -- train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kado6I35Dzr1"
   },
   "source": [
    "## Some toggles for if you want to re-start from weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 300,
     "status": "ok",
     "timestamp": 1638305345334,
     "user": {
      "displayName": "Jill Naiman",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "13328008118965652761"
     },
     "user_tz": 360
    },
    "id": "0bgHP_GnD2SN"
   },
   "outputs": [],
   "source": [
    "# Do you want to re-run from an already generated train/valid/test split?\n",
    "#  -- this is useful for feature testing and/or re-starting from weights\n",
    "re_run_from_splits = True\n",
    "\n",
    "# if restarting, how many previous log files do we want to look at?\n",
    "nRecent = 7 # for the 1st restart, this will be 1, for the 2nd, 2, etc\n",
    "\n",
    "# set to true if you are not re-running from the same dataset\n",
    "regenAnchors = False\n",
    " \n",
    "# use a saved weights file? Set to None if not and training will start anew\n",
    "#saved_weights_file = 'weights/savedWeights/training_1_model_l0.017813377.h5'\n",
    "saved_weights_file = None\n",
    "\n",
    "#fileStorage = 'binaries/' # binaries is where things are -- MAIN   \n",
    "#extraName = '' # append to training weights name\n",
    "\n",
    "# for feature collections\n",
    "#fileStorage = 'binaries_model1/' # binaries is where things are -- MAIN   \n",
    "#extraName = 'model1' # never use 8, this is our usual model?\n",
    "\n",
    "#fileStorage = 'binaries_model1_inverted/' # binaries is where things are -- MAIN   \n",
    "#extraName = 'model1_inverted'\n",
    "\n",
    "#fileStorage = 'binaries_model1_inverted_palletized/'\n",
    "#extraName = 'model1_inverted_palletized'\n",
    "\n",
    "# fileStorage = 'binaries_model2/'\n",
    "# extraName = 'model2'\n",
    "\n",
    "#fileStorage = 'binaries_model3/'\n",
    "#extraName = 'model3'\n",
    "\n",
    "#fileStorage = 'binaries_model4/'\n",
    "#extraName = 'model4'\n",
    "\n",
    "#fileStorage = 'binaries_model5/'\n",
    "#extraName = 'model5'\n",
    "\n",
    "#fileStorage = 'binaries_model5_maxTag125/'\n",
    "#extraName = 'model5_maxTag125'\n",
    "\n",
    "# fileStorage = 'binaries_model6/'\n",
    "# extraName = 'model6'\n",
    "\n",
    "# fileStorage = 'binaries_model8/'\n",
    "# extraName = 'model8'\n",
    "\n",
    "fileStorage = 'binaries_model8/'; method='npz'\n",
    "#fileStorage = 'binaries_model8_pickle/'; method='pickle'\n",
    "#fileStorage = 'binaries_model8_noncom/'; method = 'npy'\n",
    "#fileStorage = 'binaries_model8_noncomz/'; method = 'npz'\n",
    "\n",
    "extraName = 'model8'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 229,
     "status": "ok",
     "timestamp": 1638305346236,
     "user": {
      "displayName": "Jill Naiman",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "13328008118965652761"
     },
     "user_tz": 360
    },
    "id": "dLbPjO38y8NV"
   },
   "outputs": [],
   "source": [
    "# toggle for if on google collab or not\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1638305346557,
     "user": {
      "displayName": "Jill Naiman",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "13328008118965652761"
     },
     "user_tz": 360
    },
    "id": "gnmYH7-3neak",
    "outputId": "25237af9-ee3b-46e2-dfc8-09db00289dff"
   },
   "outputs": [],
   "source": [
    "from sys import path\n",
    "path.append('../')\n",
    "import config\n",
    "classDirMain = config.save_binary_dir #+ fileStorage\n",
    "# where are raw images?\n",
    "images_pulled_dir = config.images_jpeg_dir #'/Users/jillnaiman/Dropbox/wwt_image_extraction/FigureLocalization/Pages/RandomSingleFromPDFIndexed/' ## what about Dropbox though????\n",
    "classDirMainHOME = fileStorage \n",
    "splitsDir = config.tmp_storage_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1638305346558,
     "user": {
      "displayName": "Jill Naiman",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "13328008118965652761"
     },
     "user_tz": 360
    },
    "id": "H67BF_sLneah"
   },
   "outputs": [],
   "source": [
    "# # some parameters for different architectures of YOLO\n",
    "batch_size = 10\n",
    "num_epochs = 125 #150 #300\n",
    "\n",
    "#IMAGE_H, IMAGE_W = 512, 512\n",
    "image_size = config.IMAGE_H # assume width=height\n",
    "\n",
    "TRAIN_BATCH_SIZE = batch_size #10\n",
    "VAL_BATCH_SIZE   = batch_size #10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1638305346558,
     "user": {
      "displayName": "Jill Naiman",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "13328008118965652761"
     },
     "user_tz": 360
    },
    "id": "9GGT2mW9nean",
    "outputId": "3a140232-6fdc-44f7-e6b1-b17e841e2644"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/Users/jillnaiman/MegaYolo/yolo_512x512_ann/',\n",
       " '/Users/jillnaiman/MegaYolo/binaries_model8/')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# where annotations and features files\n",
    "#classDir_main_to = classDirMain + 'yolo_512x512_cap_ann/'\n",
    "#classDir_main_to_imgs = classDirMain + 'binaries/'#+ 'yolo_512x512/'\n",
    "classDir_main_to = classDirMain + config.ann_name + str(int(config.IMAGE_H)) + 'x' + str(int(config.IMAGE_W))  + '_ann/'\n",
    "\n",
    "classDir_main_to_imgs = classDirMain + fileStorage.split('/')[-2] + '/'\n",
    "classDir_main_to, classDir_main_to_imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1638305346559,
     "user": {
      "displayName": "Jill Naiman",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "13328008118965652761"
     },
     "user_tz": 360
    },
    "id": "IVGN-eMPe0-c"
   },
   "outputs": [],
   "source": [
    "#!conda install numba --yes\n",
    "#logsDir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 2276,
     "status": "ok",
     "timestamp": 1638305348829,
     "user": {
      "displayName": "Jill Naiman",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "13328008118965652761"
     },
     "user_tz": 360
    },
    "id": "vwhXD8HOneap"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import re\n",
    "import numpy as np\n",
    "import cv2 as cv\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import xml.etree.cElementTree as ET\n",
    "\n",
    "# make more better?\n",
    "#from numba import jit\n",
    "from time import perf_counter\n",
    "import sys\n",
    "\n",
    "# for v5\n",
    "import math\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 414,
     "status": "ok",
     "timestamp": 1638305349239,
     "user": {
      "displayName": "Jill Naiman",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "13328008118965652761"
     },
     "user_tz": 360
    },
    "id": "TpGXEVAfnear",
    "outputId": "e5d1ec46-a9b1-4262-c794-7b98f42f0c54"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow version : 2.4.1\n",
      "GPU : []\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print('Tensorflow version : {}'.format(tf.__version__))\n",
    "print('GPU : {}'.format(tf.config.list_physical_devices('GPU')))\n",
    "from tensorflow import keras\n",
    "import tensorflow.keras.backend as K\n",
    "import imgaug as ia\n",
    "from imgaug import augmenters as iaa\n",
    "\n",
    "# my imports\n",
    "import pickle\n",
    "#from classification_utils import make_get_csv\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import json\n",
    "#from classification_utils import train_test_valid_split\n",
    "from scipy import stats\n",
    "import shutil\n",
    "\n",
    "# for restart\n",
    "from tensorflow.python.summary.summary_iterator import summary_iterator\n",
    "import struct\n",
    "from datetime import date as DATE\n",
    "\n",
    "# get parse\n",
    "from mega_yolo_utils import build_model, train_test_valid_split, \\\n",
    "    process_box, process_layer, box_iou, compute_nms, iou, num_cluster, generator, \\\n",
    "    get_n_features\n",
    "from general_utils import parse_annotation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HUJLjHVTneav"
   },
   "source": [
    "## First, data setup\n",
    "\n",
    " * Define classes\n",
    " * Grab image location info, grab boxes\n",
    " * Remap images to new size for running through YOLO\n",
    " * get 9 anchors -- why 9? because that is the code we are grabbing, that is why :P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1638305349240,
     "user": {
      "displayName": "Jill Naiman",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "13328008118965652761"
     },
     "user_tz": 360
    },
    "id": "EoCivx-Jneav",
    "outputId": "313ea5ea-2424-4907-dcdb-1516e523950c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5515"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Grab labels\n",
    "annotations = glob.glob(classDir_main_to + '*')\n",
    "len(annotations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tFWzDaooBb6A"
   },
   "source": [
    "Parse annotations -- **NOTE: this can take a while!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7058,
     "status": "ok",
     "timestamp": 1638305356292,
     "user": {
      "displayName": "Jill Naiman",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "13328008118965652761"
     },
     "user_tz": 360
    },
    "id": "N34O8BwTneay",
    "outputId": "b48f751d-1367-4f32-8dcf-202b6b625254"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "on  0  of  5515\n",
      "on  200  of  5515\n",
      "on  400  of  5515\n",
      "on  600  of  5515\n",
      "on  800  of  5515\n",
      "on  1000  of  5515\n",
      "on  1200  of  5515\n",
      "on  1400  of  5515\n",
      "on  1600  of  5515\n",
      "on  1800  of  5515\n",
      "on  2000  of  5515\n",
      "on  2200  of  5515\n",
      "on  2400  of  5515\n",
      "on  2600  of  5515\n",
      "on  2800  of  5515\n",
      "on  3000  of  5515\n",
      "on  3200  of  5515\n",
      "on  3400  of  5515\n",
      "on  3600  of  5515\n",
      "on  3800  of  5515\n",
      "on  4000  of  5515\n",
      "on  4200  of  5515\n",
      "on  4400  of  5515\n",
      "on  4600  of  5515\n",
      "on  4800  of  5515\n",
      "on  5000  of  5515\n",
      "on  5200  of  5515\n",
      "on  5400  of  5515\n",
      "    Elapsed wall clock time = 4.98519 seconds.\n"
     ]
    }
   ],
   "source": [
    "##### what about anchors -- do we want to regenerate? Generally keep this as True...\n",
    "#####regenAnchors = True\n",
    "# ... unless we are re-running from a previous split\n",
    "if re_run_from_splits: regenAnchors = False\n",
    "#if regenAnchorsAnyway: regenAnchors = True\n",
    "\n",
    "if regenAnchors:\n",
    "    bboxes = []\n",
    "    \n",
    "\n",
    "# this parsing does some loading on collab that I'm not 100% sure about, but seems necessary\n",
    "#   to load into memory, so keep it and figure it out later\n",
    "def load_parse_data_split(X_full):\n",
    "    Y_full_str = np.array([]) # have to loop and give best guesses for the pages that have multiple images/classes in them\n",
    "    slabels = np.array([])\n",
    "    for ii, X in enumerate(X_full):\n",
    "        if ii%200 == 0: print('on ', ii, ' of ', len(X_full))\n",
    "        tree = ET.parse(X)\n",
    "        tags = np.array([])\n",
    "        for elem in tree.iter(): \n",
    "            if 'object' in elem.tag or 'part' in elem.tag:                  \n",
    "                box = np.zeros((5))\n",
    "                for attr in list(elem):\n",
    "                    if 'name' in attr.tag:\n",
    "                        if attr.text is not None:\n",
    "                            tags = np.append(tags,attr.text)\n",
    "                            slabels = np.append(slabels,attr.text)\n",
    "                        #print(tags, slabels)\n",
    "                    if regenAnchors:\n",
    "                        if 'bndbox' in attr.tag and 'bndboxOrig' not in attr.tag:\n",
    "                            for dim in list(attr):\n",
    "                                if 'xmin' in dim.tag:\n",
    "                                    box[0] = int(round(float(dim.text)))\n",
    "                                if 'ymin' in dim.tag:\n",
    "                                    box[1] = int(round(float(dim.text)))\n",
    "                                if 'xmax' in dim.tag:\n",
    "                                    box[2] = int(round(float(dim.text)))\n",
    "                                if 'ymax' in dim.tag:\n",
    "                                    box[3] = int(round(float(dim.text)))\n",
    "                if regenAnchors and len(box)>0: bboxes.append(np.asarray(box))\n",
    "        if len(tags) > 0:\n",
    "            #print(tags)\n",
    "            modeClass = stats.mode(tags).mode[0] # most frequent class that pops up on this page\n",
    "            Y_full_str = np.append(Y_full_str, modeClass) # class in string\n",
    "        else:\n",
    "            Y_full_str = np.append(Y_full_str, 'none')\n",
    "    return Y_full_str,slabels\n",
    "\n",
    "# NEXT: do a quick test run-through of the data generator for train/test splits\n",
    "X_full = np.array(annotations)\n",
    "\n",
    "start_time = perf_counter( )\n",
    "Y_full_str,slabels = load_parse_data_split(X_full)\n",
    "stop_time = perf_counter( )\n",
    "print('    Elapsed wall clock time = %g seconds.' % (stop_time - start_time) )\n",
    "    \n",
    "# also do regeneration of anchors\n",
    "if regenAnchors: bboxes = np.array(bboxes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yUWutM847jyy"
   },
   "source": [
    "Get anchors, if that is what you wanna do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 25,
     "status": "ok",
     "timestamp": 1638305356293,
     "user": {
      "displayName": "Jill Naiman",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "13328008118965652761"
     },
     "user_tz": 360
    },
    "id": "Dvtr6msG7jyy",
    "outputId": "d75a4d9f-85d2-4c87-aeae-5908282da486"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from saved:\n",
      "[[195.  21.]\n",
      " [203.   7.]\n",
      " [ 51.   5.]\n",
      " [313. 199.]\n",
      " [359. 391.]\n",
      " [435.  16.]\n",
      " [399. 307.]\n",
      " [204. 114.]\n",
      " [ 15. 355.]]\n"
     ]
    }
   ],
   "source": [
    "# assume location of saved anchors:\n",
    "saveFileAnchors = classDirMain + 'weights/anchors.pickle'\n",
    "# hack for local debugging\n",
    "#if '/Users/jillnaiman' in thisDir:\n",
    "saveFileAnchors = splitsDir + 'anchors.pickle'\n",
    "\n",
    "if regenAnchors:\n",
    "    boxes = []\n",
    "    for b in bboxes:\n",
    "        boxes.append([b[2]-b[0], b[3]-b[1]])\n",
    "    boxes = np.array(boxes)\n",
    "    \n",
    "    anchors = generator(boxes,k=num_cluster)\n",
    "    print('NEW ANCHORS:')\n",
    "    \n",
    "    # save!\n",
    "    with open(saveFileAnchors, 'wb') as ff:\n",
    "        pickle.dump(anchors, ff)\n",
    "else:\n",
    "    print('from saved:')\n",
    "    with open(saveFileAnchors, 'rb') as f:\n",
    "        anchors = pickle.load(f)    \n",
    "    \n",
    "print(anchors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 21,
     "status": "ok",
     "timestamp": 1638305356293,
     "user": {
      "displayName": "Jill Naiman",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "13328008118965652761"
     },
     "user_tz": 360
    },
    "id": "Z1G0r3vYnea0",
    "outputId": "75ac0211-6d6a-4e9a-a368-3b40bd104023"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['figure', 'figure caption', 'math formula', 'table']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LABELS = np.unique(slabels).tolist()\n",
    "CLASS = len(LABELS)\n",
    "##if use_only_one_class: CLASS = 1\n",
    "LABELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "executionInfo": {
     "elapsed": 20,
     "status": "ok",
     "timestamp": 1638305356294,
     "user": {
      "displayName": "Jill Naiman",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "13328008118965652761"
     },
     "user_tz": 360
    },
    "id": "yZJXzsfVnea2"
   },
   "outputs": [],
   "source": [
    "# strings to integers\n",
    "labels = np.arange(0,len(LABELS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 20,
     "status": "ok",
     "timestamp": 1638305356294,
     "user": {
      "displayName": "Jill Naiman",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "13328008118965652761"
     },
     "user_tz": 360
    },
    "id": "0R42fLapLPI-",
    "outputId": "88612ccb-2478-4462-dd44-710d55e35142"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1, 2, 3]), ['figure', 'figure caption', 'math formula', 'table'])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels, LABELS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6wKLOv6xBb6D"
   },
   "source": [
    "Create splits one way or the other.  Using the function makes sure the classes are evenly split as there can be un-even classes (for example, there might be way more figures/figure captions than tables).  Note: each instance is tagged as having one class but this is just the most frequent type on that page -- pages can sometimes have multiple types.\n",
    "\n",
    "Or, load from previous file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 438
    },
    "executionInfo": {
     "elapsed": 315,
     "status": "ok",
     "timestamp": 1638305356591,
     "user": {
      "displayName": "Jill Naiman",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "13328008118965652761"
     },
     "user_tz": 360
    },
    "id": "nIG98zeHnea6",
    "outputId": "faf8c608-e844-4503-92da-12d5c5cc2b52"
   },
   "outputs": [],
   "source": [
    "# # splits\n",
    "# X_train = np.loadtxt(splitsDir + 'train.csv', dtype=str, delimiter=',')\n",
    "# X_test = np.loadtxt(splitsDir + 'test.csv', dtype=str, delimiter=',')\n",
    "# X_valid = np.loadtxt(splitsDir + 'valid.csv', dtype=str, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mega_yolo_utils import augmentation_generator, csv_gen\n",
    "onGoogle=False\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import general_utils\n",
    "from importlib import reload\n",
    "reload(general_utils)\n",
    "from general_utils import parse_annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitsDirIn = None\n",
    "if onGoogle: splitsDirIn = classDirMain\n",
    "train_gen_csv = csv_gen('train', splitsDir=splitsDirIn)\n",
    "valid_gen_csv = csv_gen('valid',splitsDir=splitsDirIn)\n",
    "test_gen_csv = csv_gen('test',splitsDir=splitsDirIn)\n",
    "\n",
    "class ArtificialDatasetNonAug(tf.data.Dataset):\n",
    "\n",
    "    #@tf.function\n",
    "    def _dataset_gen(split, batch_size, method='npz'):\n",
    "        while True:\n",
    "            true_boxes = []; imgs = []; files = []\n",
    "            #time1 = time.perf_counter()\n",
    "\n",
    "            while len(files) < batch_size:\n",
    "                if type(split) == str:\n",
    "                    if b'train' in bytes(split, encoding='utf8'): # NOTE, must be bytes here!!!\n",
    "                        line = next(train_gen_csv)\n",
    "                    elif b'valid' in bytes(split, encoding='utf8'):\n",
    "                        line = next(valid_gen_csv)\n",
    "                    elif b'test' in bytes(split, encoding='utf8'):\n",
    "                        line = next(test_gen_csv)\n",
    "                else:\n",
    "                    if b'train' in split: # NOTE, must be bytes here!!!\n",
    "                        line = next(train_gen_csv)\n",
    "                    elif b'valid' in split:\n",
    "                        line = next(valid_gen_csv)\n",
    "                    elif b'test' in split:\n",
    "                        line = next(test_gen_csv)\n",
    "\n",
    "                files.append(line.strip())\n",
    "\n",
    "            # parse and get full names\n",
    "            #time2 = time.perf_counter()\n",
    "            #print('parse CSV:', time2-time1)\n",
    "            try:\n",
    "                imgs_name, bbox = parse_annotation(files, LABELS, \n",
    "                                                   feature_dir=classDir_main_to_imgs,\n",
    "                                                   annotation_dir=classDir_main_to)\n",
    "            except:\n",
    "                print('error parsing:', imgs_name)\n",
    "                \n",
    "            #print('method is ', method)\n",
    "            if method == bytes('npz', encoding='utf8'): ender = '.npz'\n",
    "            if method == bytes('npy', encoding='utf8'): ender = '.npy'\n",
    "            if method == bytes('pickle', encoding='utf8'): ender = '.pickle'\n",
    "            #print(imgs_name)\n",
    "            # do a debug check\n",
    "            for im in imgs_name:\n",
    "                #print(im)\n",
    "                if ender not in im:\n",
    "                    print('no np file, no pickle file, no npy file')\n",
    "                    import sys; sys.exit()\n",
    "                    \n",
    "            #time3 = time.perf_counter()\n",
    "            #print('Parse annotation:',time3-time2)\n",
    "\n",
    "            # read in and keep images -- npy files/npz\n",
    "            if method == bytes('npz', encoding='utf8'):\n",
    "                for im in imgs_name:\n",
    "                    with np.load(im) as b:\n",
    "                        #b = b['arr_0']/255.0\n",
    "                        imgs.append(b['arr_0']/255.0)\n",
    "            elif method == bytes('npy', encoding='utf8'):\n",
    "                for im in imgs_name:\n",
    "                    b=np.load(im)\n",
    "                    if type(b) != np.ndarray:\n",
    "                        b = b['arr_0'] # WHY????\n",
    "                    imgs.append(b/255.0)\n",
    "            elif method == bytes('pickle', encoding='utf8'):\n",
    "                for im in imgs_name:\n",
    "                    with open(im, 'rb') as f:\n",
    "                        b = pickle.load(f) \n",
    "                        imgs.append(np.array(b)/255.0)\n",
    "            else:\n",
    "                print('no idea what this method is')\n",
    "                        \n",
    "                \n",
    "            #time4 = time.perf_counter()\n",
    "            #print('Load data:', time4-time3)\n",
    "\n",
    "            # finally, format for output\n",
    "            y_true1, y_true2, y_true3 = [],[],[]\n",
    "            for b in bbox:\n",
    "                y1,y2,y3= process_box(b[:,:4], b[:,4].astype('int'),anchors,CLASS)\n",
    "                y_true1.append(y1); y_true2.append(y2); y_true3.append(y3)\n",
    "            # if there is no box, do something different\n",
    "            if len(bbox) == 0:\n",
    "                # fake a box\n",
    "                b = np.array([[111.,  59., 403., 364. ,  4.]])\n",
    "                y1,y2,y3= process_box(b[:,:4], b[:,4].astype('int'),anchors,CLASS)\n",
    "                y1[:] = 0; y2[:]=0;y3[:]=0\n",
    "                y_true1.append(y1); y_true2.append(y2); y_true3.append(y3)        \n",
    "            img = tf.cast(np.array(imgs), tf.float32)     \n",
    "            #time5 = time.perf_counter()\n",
    "            #print('process blocks:', time5-time4)\n",
    "            del imgs\n",
    "            yield img, tf.cast(y_true1, tf.float32), tf.cast(y_true2, tf.float32), tf.cast(y_true3, tf.float32)\n",
    "\n",
    "\n",
    "\n",
    "    def __new__(cls,batch_size=32,split='valid', method='npz'):\n",
    "        dataset = tf.data.Dataset.from_generator(cls._dataset_gen, args=[split, batch_size,method],\n",
    "                                             output_types = (tf.float32, tf.float32, \n",
    "                                                             tf.float32, tf.float32))\n",
    "        return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs_name, bbox = parse_annotation([annotations[0]], LABELS, \n",
    "                                                   feature_dir=classDir_main_to_imgs,\n",
    "                                                   annotation_dir=classDir_main_to)\n",
    "\n",
    "for im in imgs_name:\n",
    "    arr = np.load(im)['arr_0']\n",
    "    \n",
    "#features_dataset = tf.data.Dataset.from_tensor_slices((feature0, feature1, feature2, feature3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bbox, arr\n",
    "arr.shape\n",
    "arr.reshape(1,arr.shape[0]*arr.shape[1]*arr.shape[2]).shape\n",
    "bbox[0].shape\n",
    "\n",
    "# maxboxes stuff\n",
    "maxboxes = 20\n",
    "\n",
    "boxout = np.zeros([maxboxes+1,5])\n",
    "boxout[:bbox[0].shape[0],:] = bbox[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "## start here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mInit signature:\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatList\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m/\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m      A ProtocolMessage\n",
       "\u001b[0;31mFile:\u001b[0m           ~/anaconda3/envs/Paper1/lib/python3.7/site-packages/tensorflow/core/example/feature_pb2.py\n",
       "\u001b[0;31mType:\u001b[0m           GeneratedProtocolMessageType\n",
       "\u001b[0;31mSubclasses:\u001b[0m     \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tf.train.FloatList?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _bytes_feature(value):\n",
    "    \"\"\"Returns a bytes_list from a string / byte.\"\"\"\n",
    "    if isinstance(value, type(tf.constant(0))):\n",
    "        value = value.numpy() # BytesList won't unpack a string from an EagerTensor.\n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "\n",
    "def _float_feature(value):\n",
    "    \"\"\"Returns a float_list from a float / double.\"\"\"\n",
    "    return tf.train.Feature(float_list=tf.train.FloatList(value=[value]))\n",
    "\n",
    "def _int64_feature(value):\n",
    "    \"\"\"Returns an int64_list from a bool / enum / int / uint.\"\"\"\n",
    "    return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n",
    "\n",
    "# def _float32_feature(value):\n",
    "#     \"\"\"Returns an int64_list from a bool / enum / int / uint.\"\"\"\n",
    "#     return tf.train.Feature(float_list=tf.train.Float32List(value=[value]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary with features that may be relevant.\n",
    "def image_example(image, boxes,img_name):\n",
    "    #image_shape = tf.io.decode_jpeg(image_string).shape\n",
    "    image_string = image.astype('float32')/255.0\n",
    "    image_string = image.reshape(image.shape[0]*image.shape[1]*image.shape[2])\n",
    "    \n",
    "    nfeatures = image.shape[2]\n",
    "    nboxes = boxes.shape[0]\n",
    "    if nboxes>0:\n",
    "        boxout = boxes.reshape(boxes.shape[0]*boxes.shape[1])\n",
    "    else:\n",
    "        boxout = np.array([])\n",
    "    \n",
    "    feature = {\n",
    "      #'nbox': _int64_feature(np.int64(nboxes)),\n",
    "      #'nfeatures': _int64_feature(np.int64(nfeatures)),\n",
    "      'nbox': _float_feature(np.float32(nboxes)),\n",
    "      'nfeatures': _float_feature(np.float32(nfeatures)),\n",
    "      'boxes': _bytes_feature(boxout.astype('float32').tobytes()),\n",
    "      'image_raw': _bytes_feature(image_string.astype('float32').tobytes()),\n",
    "      'image_name': _bytes_feature(img_name.astype('float32').tobytes()),\n",
    "    }\n",
    "\n",
    "    return tf.train.Example(features=tf.train.Features(feature=feature))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many TFRecord files to have? So that each are $\\sim$100Mb: https://docs.w3cub.com/tensorflow~guide/performance/performance_guide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['/Users/jillnaiman/tmpMegaYolo/binaries/yolo_512x512_cap_ann/1952ApJ___115___82K_p1.xml',\n",
       "       '/Users/jillnaiman/tmpMegaYolo/binaries/yolo_512x512_cap_ann/1994ApJ___427L__43F_p4.xml',\n",
       "       '/Users/jillnaiman/tmpMegaYolo/binaries/yolo_512x512_cap_ann/1995ApJ___445__246C_p6.xml',\n",
       "       ...,\n",
       "       '/Users/jillnaiman/tmpMegaYolo/binaries/yolo_512x512_cap_ann/1978ApJ___220___14H_p1.xml',\n",
       "       '/Users/jillnaiman/tmpMegaYolo/binaries/yolo_512x512_cap_ann/1982ApJ___252__269Y_p2.xml',\n",
       "       '/Users/jillnaiman/tmpMegaYolo/binaries/yolo_512x512_cap_ann/1982ApJ___258__467Y_p2.xml'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filelist = pd.read_csv(config.tmp_storage_dir+'train.csv',names=['filename'])['filename'].values\n",
    "filelist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/Users/jillnaiman/MegaYolo/binaries_model8/',\n",
       " '/Users/jillnaiman/MegaYolo/yolo_512x512_ann/')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classDir_main_to_imgs, classDir_main_to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "random(size=None)\n",
       "\n",
       "Return random floats in the half-open interval [0.0, 1.0). Alias for\n",
       "`random_sample` to ease forward-porting to the new random API.\n",
       "\u001b[0;31mType:\u001b[0m      builtin_function_or_method\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "np.random.random?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 5)"
      ]
     },
     "execution_count": 312,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.random([maxboxes,5]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['figure', 'figure caption', 'math formula', 'table']"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#np.savetxt(splitsDir + 'LABELS.csv', X_train, fmt='%s', delimiter=',')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no file /Users/jillnaiman/MegaYolo/yolo_512x512_ann/1992AJ____103_1151C_p9.xml\n",
      "no file /Users/jillnaiman/MegaYolo/yolo_512x512_ann/1991ApJS___76__455E_p6.xml\n",
      "no file /Users/jillnaiman/MegaYolo/yolo_512x512_ann/1990ApJ___365__510C_p2.xml\n",
      "no file /Users/jillnaiman/MegaYolo/yolo_512x512_ann/1996ApJ___467__887H_p4.xml\n",
      "no file /Users/jillnaiman/MegaYolo/yolo_512x512_ann/1994ApJS___95__457W_p15.xml\n"
     ]
    }
   ],
   "source": [
    "# write one image file and see how big it is\n",
    "record_file = config.tmp_storage_dir+'test.tfrecords'\n",
    "# what is our max boxes\n",
    "maxboxes = -1\n",
    "for a in filelist:\n",
    "    a = classDir_main_to + a.split('/')[-1]\n",
    "    try:\n",
    "        imgs_name, bbox = parse_annotation([a], LABELS,\n",
    "                                           feature_dir=classDir_main_to_imgs,\n",
    "                                           annotation_dir=classDir_main_to) \n",
    "    except:\n",
    "        print('no file', a)\n",
    "    if len(bbox) > 0:\n",
    "        maxboxes = max([maxboxes,len(bbox[0])])\n",
    "    \n",
    "# print with maxboxes\n",
    "with tf.io.TFRecordWriter(record_file) as writer:\n",
    "    \n",
    "    a = classDir_main_to + filelist[0].split('/')[-1]\n",
    "    imgs_name, bbox = parse_annotation([a], LABELS,\n",
    "                                           feature_dir=classDir_main_to_imgs,\n",
    "                                           annotation_dir=classDir_main_to) \n",
    "    # fake boxes\n",
    "    fakebox = np.random.random([maxboxes,5])\n",
    "    tf_example = image_example(arr,fakebox,imgs_name[0])\n",
    "    writer.write(tf_example.SerializeToString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/t5/9xgccmv92hnfvjwd62mk8zqh0000gn/T/ipykernel_45883/1373489018.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfilesize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetsize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecord_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mfilesize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilesize\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100.\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m1e6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "filesize = os.path.getsize(record_file)\n",
    "filesize, filesize/(100.*1e6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.0"
      ]
     },
     "execution_count": 452,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#i.e we want:\n",
    "nfiles_per_file = 100*1e6//filesize\n",
    "nfiles_per_file # per recordio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "592"
      ]
     },
     "execution_count": 453,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nfiles = int(np.ceil(len(filelist)*1.0/nfiles_per_file))\n",
    "nfiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "on 0 of 592\n",
      "on 50 of 592\n",
      "on 100 of 592\n",
      "on 150 of 592\n",
      "cant find /Users/jillnaiman/MegaYolo/yolo_512x512_ann/1992AJ____103_1151C_p9.xml , moving on...\n",
      "on 200 of 592\n",
      "on 250 of 592\n",
      "cant find /Users/jillnaiman/MegaYolo/yolo_512x512_ann/1991ApJS___76__455E_p6.xml , moving on...\n",
      "cant find /Users/jillnaiman/MegaYolo/yolo_512x512_ann/1990ApJ___365__510C_p2.xml , moving on...\n",
      "on 300 of 592\n",
      "on 350 of 592\n",
      "cant find /Users/jillnaiman/MegaYolo/yolo_512x512_ann/1996ApJ___467__887H_p4.xml , moving on...\n",
      "on 400 of 592\n",
      "cant find /Users/jillnaiman/MegaYolo/yolo_512x512_ann/1994ApJS___95__457W_p15.xml , moving on...\n",
      "on 450 of 592\n",
      "on 500 of 592\n",
      "on 550 of 592\n"
     ]
    }
   ],
   "source": [
    "# write some training\n",
    "\n",
    "itrain = 0\n",
    "#iloop = 0\n",
    "#record_file = '/Users/jillnaiman/Downloads/tmp/rio/train'\n",
    "#record_file = '/Users/jillnaiman/Downloads/tmp/rio/train_{}.tfrecords'\n",
    "record_file = '/Users/jillnaiman/MegaYolo/binaries_model8_tfrecordz/train_{}.tfrecords'\n",
    "\n",
    "itotalLoop = 0\n",
    "for index in range(nfiles):\n",
    "    if index%50 == 0: print('on', index,'of',nfiles)\n",
    "    with tf.io.TFRecordWriter(record_file.format(index)) as writer:\n",
    "        #print(index*int(nfiles_per_file),min([(index+1)*int(nfiles_per_file),len(filelist)]))\n",
    "        for iloop,a in enumerate(filelist[index*int(nfiles_per_file):min([(index+1)*int(nfiles_per_file),len(filelist)])]):\n",
    "            #print(iloop,a)\n",
    "            a = classDir_main_to + a.split('/')[-1]\n",
    "            \n",
    "            try:\n",
    "                imgs_name, bbox = parse_annotation([a], LABELS,\n",
    "                                                   feature_dir=classDir_main_to_imgs,\n",
    "                                                   annotation_dir=classDir_main_to)\n",
    "            except:\n",
    "                print('cant find', a, ', moving on...')\n",
    "                continue\n",
    "            arr = np.load(imgs_name[0])['arr_0']\n",
    "\n",
    "            if len(bbox) > 0: \n",
    "                bbox = np.array(bbox[0])\n",
    "            else:\n",
    "                bbox = np.array([])\n",
    "            tf_example = image_example(arr,bbox,imgs_name[0])\n",
    "            writer.write(tf_example.SerializeToString())\n",
    "\n",
    "            # if itotalLoop%int(nfiles_per_file)==0: \n",
    "            #     print(index,iloop,itrain)\n",
    "            #     itrain+=1\n",
    "            # itotalLoop += 1\n",
    "\n",
    "    #if index == 2: import sys; sys.exit()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "***HERE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list_of_records = glob.glob('/Users/jillnaiman/Downloads/tmp/rio/train_*.tfrecords')\n",
    "#list_of_records = glob.glob('/Users/jillnaiman/Downloads/tmp/rio/train_*.tfrecords')\n",
    "list_of_records = glob.glob('/Users/jillnaiman/MegaYolo/binaries_model8_tfrecord/train_*.tfrecords')\n",
    "#list_of_records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_image_dataset = tf.data.TFRecordDataset(list_of_records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary describing the features.\n",
    "image_feature_description = {\n",
    "    #'nbox': tf.io.FixedLenFeature([], tf.int64),\n",
    "    #'nfeatures': tf.io.FixedLenFeature([], tf.int64),\n",
    "    'nbox': tf.io.FixedLenFeature([], tf.float32),\n",
    "    'nfeatures': tf.io.FixedLenFeature([], tf.float32),\n",
    "    'boxes': tf.io.FixedLenFeature([], tf.string),\n",
    "    'image_raw': tf.io.FixedLenFeature([], tf.string),\n",
    "    #'image_name': tf.io.FixedLenFeature([], tf.string),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[195.,  21.],\n",
       "       [203.,   7.],\n",
       "       [ 51.,   5.],\n",
       "       [313., 199.],\n",
       "       [359., 391.],\n",
       "       [435.,  16.],\n",
       "       [399., 307.],\n",
       "       [204., 114.],\n",
       "       [ 15., 355.]])"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anchors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [],
   "source": [
    "#image_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 589,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#@tf.function\n",
    "def process_box2(boxes, labels,anchors,CLASS,image_size=config.IMAGE_H):\n",
    "    '''\n",
    "    labels: integer numbers associated with labels, JPN: does this start at 0 or 1??\n",
    "    boxes: [number of boxes on page, xmin, ymin, xmax, ymax]\n",
    "    returns: the correctly formatted 3 y-trues\n",
    "    '''\n",
    "    anchors_mask = [[6, 7, 8], [3, 4, 5], [0, 1, 2]]\n",
    "    #anchors = anchors\n",
    "    box_centers = (boxes[:, 0:2] + boxes[:, 2:4]) / 2\n",
    "    box_size = boxes[:, 2:4] - boxes[:, 0:2]\n",
    "    #print(box_size)\n",
    "    \n",
    "    # y_true_1 = tf.zeros((tf.math.floordiv(image_size,32),\n",
    "    #                         tf.math.floordiv(image_size,32),\n",
    "    #                         3, 5 + CLASS), tf.float32)\n",
    "    # y_true_2 = tf.zeros((tf.math.floordiv(image_size,16),\n",
    "    #                         tf.math.floordiv(image_size,16),\n",
    "    #                         3, 5 + CLASS), tf.float32)\n",
    "    # y_true_3 = tf.zeros((tf.math.floordiv(image_size,8),\n",
    "    #                         tf.math.floordiv(image_size,8),\n",
    "    #                         3, 5 + CLASS), tf.float32)\n",
    "    #print('hi')\n",
    "\n",
    "    y_true_1 = np.zeros((image_size // 32,\n",
    "                            image_size // 32,\n",
    "                            3, 5 + CLASS.numpy()), np.float32)\n",
    "    y_true_2 = np.zeros((image_size // 16,\n",
    "                            image_size // 16,\n",
    "                            3, 5 + CLASS.numpy()), np.float32)\n",
    "    y_true_3 = np.zeros((image_size // 8,\n",
    "                            image_size // 8,\n",
    "                            3, 5 + CLASS.numpy()), np.float32)\n",
    "    # y_true_1 = np.zeros((tf.math.floordiv(image_size,32),\n",
    "    #                         tf.math.floordiv(image_size,32),\n",
    "    #                         3, 5 + CLASS.numpy()), np.float32)\n",
    "    # y_true_2 = np.zeros((tf.math.floordiv(image_size,16),\n",
    "    #                         tf.math.floordiv(image_size,16),\n",
    "    #                         3, 5 + CLASS), np.float32)\n",
    "    # y_true_3 = np.zeros((tf.math.floordiv(image_size,8),\n",
    "    #                         tf.math.floordiv(image_size,8),\n",
    "    #                         3, 5 + CLASS), np.float32)\n",
    "    y_true = [y_true_1, y_true_2, y_true_3]\n",
    "\n",
    "    box_size = tf.expand_dims(box_size, 1)\n",
    "\n",
    "    min_np = tf.maximum(- box_size / 2, - anchors / 2)\n",
    "    max_np = tf.minimum(box_size / 2, anchors / 2)\n",
    "\n",
    "    whs = max_np - min_np\n",
    "\n",
    "    overlap = whs[:, :, 0] * whs[:, :, 1]\n",
    "    union = box_size[:, :, 0] * box_size[:, :, 1] + anchors[:, 0] * anchors[:, 1] - whs[:, :, 0] * whs[:, :, 1] + 1e-10\n",
    "\n",
    "    iou = overlap / union\n",
    "    best_match_idx = tf.argmax(iou, axis=1)\n",
    "\n",
    "    #ratio_dict = {1.: 8., 2.: 16., 3.: 32.}\n",
    "    ratio_dict = tf.constant([8,16,32],tf.int32)\n",
    "    #ratio = -1\n",
    "    #for i, idx in enumerate(best_match_idx):\n",
    "    for i in tf.range(tf.shape(best_match_idx)[0]):\n",
    "        idx = best_match_idx[i]\n",
    "        feature_map_group = 2 - idx // 3\n",
    "        ind = tf.cast(tf.math.ceil((tf.cast(idx + 1,tf.float32)) / 3.),tf.int32)\n",
    "        #ratio = ratio_dict[np.ceil((idx + 1) / 3.)]\n",
    "        ratio = tf.slice(ratio_dict,[ind],[1])\n",
    "        x = int(tf.math.floor(box_centers[i, 0] / tf.cast(ratio,tf.float32)))\n",
    "        y = int(tf.math.floor(box_centers[i, 1] / tf.cast(ratio,tf.float32)))\n",
    "        #k = anchors_mask[feature_map_group].index(idx)\n",
    "        m1 = tf.gather(anchors_mask,feature_map_group)\n",
    "        k = tf.gather(m1,idx)\n",
    "        #k = tf.gather(anchors_mask[feature_map_group],idx)\n",
    "        c = labels[i]\n",
    "\n",
    "        y_true[feature_map_group][y, x, k, :2] = box_centers[i]\n",
    "        # y_true = y_true[feature_map_group][y, x, k, :2].assign(box_centers[i])\n",
    "        y_true[feature_map_group][y, x, k, 2:4] = box_size[i]\n",
    "        y_true[feature_map_group][y, x, k, 4] = 1.\n",
    "        # if feature_map_group == 0:\n",
    "        #     yt2 = np\n",
    "        #     y_true_11[y,x,k,:2] = box_centers[i]\n",
    "        try:\n",
    "            #y_true[feature_map_group][y, x, k, 5 + c] = 1.\n",
    "            y_true[feature_map_group][y, x, k, 5 + c -1] = 1. # labels start at 0\n",
    "        except:\n",
    "            print('in parse')\n",
    "            print(y,x,k,c, 5+c)\n",
    "            print(labels)\n",
    "\n",
    "    return y_true_1, y_true_2, y_true_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#@tf.function\n",
    "def process_box3(boxes, labels,anchors,CLASS):\n",
    "    '''\n",
    "    labels: integer numbers associated with labels, JPN: does this start at 0 or 1??\n",
    "    boxes: [number of boxes on page, xmin, ymin, xmax, ymax]\n",
    "    returns: the correctly formatted 3 y-trues\n",
    "    '''\n",
    "    anchors_mask = [[6, 7, 8], [3, 4, 5], [0, 1, 2]]\n",
    "    #anchors = anchors\n",
    "    box_centers = (boxes[:, 0:2] + boxes[:, 2:4]) / 2\n",
    "    box_size = boxes[:, 2:4] - boxes[:, 0:2]\n",
    "\n",
    "    y_true_1 = np.zeros((image_size // 32,\n",
    "                            image_size // 32,\n",
    "                            3, 5 + CLASS), np.float32)\n",
    "    y_true_2 = np.zeros((image_size // 16,\n",
    "                            image_size // 16,\n",
    "                            3, 5 + CLASS), np.float32)\n",
    "    y_true_3 = np.zeros((image_size // 8,\n",
    "                            image_size // 8,\n",
    "                            3, 5 + CLASS), np.float32)\n",
    "\n",
    "    y_true = [y_true_1, y_true_2, y_true_3]\n",
    "\n",
    "    box_size = np.expand_dims(box_size, 1)\n",
    "\n",
    "    min_np = np.maximum(- box_size / 2, - anchors / 2)\n",
    "    max_np = np.minimum(box_size / 2, anchors / 2)\n",
    "\n",
    "    whs = max_np - min_np\n",
    "\n",
    "    overlap = whs[:, :, 0] * whs[:, :, 1]\n",
    "    union = box_size[:, :, 0] * box_size[:, :, 1] + anchors[:, 0] * anchors[:, 1] - whs[:, :, 0] * whs[:, :, 1] + 1e-10\n",
    "\n",
    "    iou = overlap / union\n",
    "    best_match_idx = np.argmax(iou, axis=1)\n",
    "\n",
    "    ratio_dict = {1.: 8., 2.: 16., 3.: 32.}\n",
    "    for i, idx in enumerate(best_match_idx):\n",
    "        feature_map_group = 2 - idx // 3\n",
    "        ratio = ratio_dict[np.ceil((idx + 1) / 3.)]\n",
    "        x = int(np.floor(box_centers[i, 0] / ratio))\n",
    "        y = int(np.floor(box_centers[i, 1] / ratio))\n",
    "        k = anchors_mask[feature_map_group].index(idx)\n",
    "        c = labels[i].numpy().astype('int')\n",
    "\n",
    "        y_true[feature_map_group][y, x, k, :2] = box_centers[i]\n",
    "        y_true[feature_map_group][y, x, k, 2:4] = box_size[i]\n",
    "        y_true[feature_map_group][y, x, k, 4] = 1.\n",
    "        try:\n",
    "            y_true[feature_map_group][y, x, k, 5 + c -1] = 1. # labels start at 0\n",
    "        except:\n",
    "            print('in parse')\n",
    "            print('y','x','k','c')\n",
    "            print(y,x,k,c)\n",
    "            print(labels.numpy())\n",
    "    #print(y_true_1)\n",
    "\n",
    "    return y_true_1, y_true_2, y_true_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# debug\n",
    "import mega_yolo_utils\n",
    "from importlib import reload\n",
    "reload(mega_yolo_utils)\n",
    "from mega_yolo_utils import process_box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _parse_image_function(example_proto,anchors,CLASS):\n",
    "    image_features = tf.io.parse_single_example(example_proto, image_feature_description)\n",
    "    #print(image_features)\n",
    "\n",
    "    nboxes = image_features['nbox']\n",
    "    nfeatures = image_features['nfeatures']\n",
    "    boxes = tf.io.decode_raw(image_features['boxes'],tf.float32)\n",
    "    boxes = tf.reshape(boxes,[nboxes,5])\n",
    "    images_raw = image_features['image_raw']\n",
    "    image = tf.io.decode_raw(images_raw,tf.float32)\n",
    "    image = tf.reshape(image,[config.IMAGE_H,config.IMAGE_W,nfeatures])\n",
    "\n",
    "    # process boxes\n",
    "    y1,y2,y3 = tf.py_function(process_box,\n",
    "                              (boxes[:,:4], boxes[:,4],anchors,CLASS),\n",
    "                              (tf.float32,tf.float32,tf.float32))   \n",
    "    \n",
    "    #return image, nboxes, nfeatures, boxes,y1,y2,y3\n",
    "    return image, y1,y2,y3\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<MapDataset shapes: ((512, 512, None), <unknown>, <unknown>, <unknown>), types: (tf.float32, tf.float32, tf.float32, tf.float32)>"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#parsed_image_dataset = raw_image_dataset.map(_parse_image_function)\n",
    "anchors_in = tf.constant(anchors,tf.float32)\n",
    "CLASS_in = tf.constant(CLASS,tf.int32)\n",
    "parsed_image_dataset = raw_image_dataset.map(lambda example_proto:_parse_image_function(example_proto,anchors,CLASS))\n",
    "parsed_image_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "#boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for checking:\n",
    "import IPython.display as display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for image,y1,y2,y3 in parsed_image_dataset.take(1):\n",
    "    # nboxes = image_features['nbox']\n",
    "    # nfeatures = image_features['nfeatures']\n",
    "    # boxes = tf.io.decode_raw(image_features['boxes'],tf.float32)\n",
    "    # boxes = tf.reshape(boxes,[nboxes,5])\n",
    "    # images_raw = image_features['image_raw']\n",
    "    # image = tf.io.decode_raw(images_raw,tf.float32)\n",
    "    # image = tf.reshape(image,[config.IMAGE_H,config.IMAGE_W,nfeatures])\n",
    "    imgPlot = image.numpy()\n",
    "    plt.imshow(imgPlot[:,:,0],cmap='gray')\n",
    "    #print(nboxes,nfeatures,boxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# benchmark tests\n",
    "import time\n",
    "def benchmark(dataset, num_epochs=1, nbatch=32):\n",
    "    start_time = time.perf_counter()\n",
    "    for epoch_num in range(num_epochs):\n",
    "        ienum=0\n",
    "        for sample in dataset:\n",
    "            #if ienum%10==0: print(ienum)\n",
    "            #print(ienum)\n",
    "            # Performing a training step\n",
    "            time.sleep(0.01)\n",
    "            ienum+=1\n",
    "            if ienum>nbatch: break\n",
    "    print(\"Execution time:\", time.perf_counter() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time: 0.810500438004965\n",
      "Execution time: 0.8195978469884722\n",
      "Execution time: 0.8190564879914746\n",
      "Execution time: 0.8738473739940673\n"
     ]
    }
   ],
   "source": [
    "# try different methods and time\n",
    "#nbatch = 10; nloop = 10\n",
    "nbatch = 25; nloop = 4\n",
    "th = []\n",
    "\n",
    "for n in range(nloop):\n",
    "    t1 = time.perf_counter()\n",
    "    benchmark(parsed_image_dataset,\n",
    "              nbatch=nbatch)\n",
    "    t2 = time.perf_counter()\n",
    "    th.append(t2-t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.819500102494203, 0.02516570435485579)"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.median(th), np.std(th)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a description of the features.\n",
    "feature_description = {\n",
    "    'feature0': tf.io.FixedLenFeature([], tf.int64, default_value=0),\n",
    "    'feature1': tf.io.FixedLenFeature([], tf.int64, default_value=0),\n",
    "    'feature2': tf.io.FixedLenFeature([], tf.string, default_value=''),\n",
    "    'feature3': tf.io.FixedLenFeature([], tf.float32, default_value=0.0),\n",
    "}\n",
    "\n",
    "def _parse_function(example_proto):\n",
    "  # Parse the input `tf.train.Example` proto using the dictionary above.\n",
    "  return tf.io.parse_single_example(example_proto, feature_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [],
   "source": [
    "def serialize_example(nbox,nfeatures,boxes,image_raw):\n",
    "    \"\"\"\n",
    "    Creates a tf.train.Example message ready to be written to a file.\n",
    "    \"\"\"\n",
    "    # Create a dictionary mapping the feature name to the tf.train.Example-compatible\n",
    "    # data type.\n",
    "    feature = {\n",
    "      'nbox': _int64_feature(nbox),\n",
    "      'nfeatures': _int64_feature(nfeatures),\n",
    "      'boxes': _bytes_feature(boxes),\n",
    "      'image_raw': _bytes_feature(image_raw),\n",
    "    }\n",
    "\n",
    "    # Create a Features message using tf.train.Example.\n",
    "\n",
    "    example_proto = tf.train.Example(features=tf.train.Features(feature=feature))\n",
    "    return example_proto.SerializeToString()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bytes_list {\n",
       "  value: \"\"\n",
       "}"
      ]
     },
     "execution_count": 322,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to read\n",
    "def parse_proto(example_proto):\n",
    "  features = {\n",
    "    'X': tf.FixedLenFeature((345,), tf.float32),\n",
    "    'y': tf.FixedLenFeature((5,), tf.float32),\n",
    "  }\n",
    "  parsed_features = tf.parse_single_example(example_proto, features)\n",
    "  return parsed_features['X'], parsed_features['y']\n",
    "\n",
    "def read_tfrecords(file_names=(\"file1.tfrecord\", \"file2.tfrecord\", \"file3.tfrecord\"),\n",
    "                   buffer_size=10000,\n",
    "                   batch_size=100):\n",
    "    dataset = tf.contrib.data.TFRecordDataset(file_names)\n",
    "    dataset = dataset.map(parse_proto)\n",
    "    dataset = dataset.shuffle(buffer_size)\n",
    "    dataset = dataset.repeat()\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    return tf.contrib.data.Iterator.from_structure(dataset.output_types, dataset.output_shapes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'serialized_example' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/t5/9xgccmv92hnfvjwd62mk8zqh0000gn/T/ipykernel_25922/2583518312.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mexample_proto\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExample\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFromString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mserialized_example\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'serialized_example' is not defined"
     ]
    }
   ],
   "source": [
    "example_proto = tf.train.Example.FromString(serialized_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5,)"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bbox[0].reshape(1*5).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 5)"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bbox[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_dataset = tf.data.Dataset.from_tensor_slices((arr.reshape(1,arr.shape[0]*arr.shape[1]*arr.shape[2]),\n",
    "                                                       bbox[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TensorSliceDataset shapes: ((3145728,), (5,)), types: (tf.uint8, tf.float64)>"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([0 0 0 ... 0 0 0], shape=(3145728,), dtype=uint8)\n",
      "tf.Tensor([ 46. 451. 257. 473.   2.], shape=(5,), dtype=float64)\n"
     ]
    }
   ],
   "source": [
    "for f0,f1 in features_dataset.take(1):\n",
    "    print(f0)\n",
    "    print(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _float_feature(value):\n",
    "    \"\"\"Returns a float_list from a float / double.\"\"\"\n",
    "    return tf.train.Feature(float_list=tf.train.FloatList(value=[value]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "def serialize_example(feature0, feature1):\n",
    "    \"\"\"\n",
    "    Creates a tf.train.Example message ready to be written to a file.\n",
    "    \"\"\"\n",
    "    # Create a dictionary mapping the feature name to the tf.train.Example-compatible\n",
    "    # data type.\n",
    "    feature = {\n",
    "      'feature0': _float_feature(feature0),\n",
    "      'feature1': _float_feature(feature1),\n",
    "    }\n",
    "\n",
    "    # Create a Features message using tf.train.Example.\n",
    "\n",
    "    example_proto = tf.train.Example(features=tf.train.Features(feature=feature))\n",
    "    return example_proto.SerializeToString()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_serialize_example(f0,f1):\n",
    "    tf_string = tf.py_function(\n",
    "        serialize_example,\n",
    "        (f0, f1),  # Pass these args to the above function.\n",
    "        tf.string)      # The return type is `tf.string`.\n",
    "    return tf.reshape(tf_string, ()) # The result is a scalar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([0. 0. 0. ... 0. 0. 0.], shape=(3145728,), dtype=float64) tf.Tensor(\n",
      "[ 46. 451. 257. 473.   2.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.], shape=(105,), dtype=float64)\n"
     ]
    }
   ],
   "source": [
    "#tf.cast(f0,tf.float32)\n",
    "features_dataset\n",
    "for f0,f1 in features_dataset.take(1):\n",
    "    print(f0,f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "array([[0., 0., 0., ..., 0., 0., 0.]]) has type numpy.ndarray, but expected one of: int, long, float",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/t5/9xgccmv92hnfvjwd62mk8zqh0000gn/T/ipykernel_25922/4247669254.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#tf_serialize_example(f0,f1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mserialize_example\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf0np\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mf1np\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/t5/9xgccmv92hnfvjwd62mk8zqh0000gn/T/ipykernel_25922/1118630037.py\u001b[0m in \u001b[0;36mserialize_example\u001b[0;34m(feature0, feature1)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;31m# data type.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     feature = {\n\u001b[0;32m----> 8\u001b[0;31m       \u001b[0;34m'feature0'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0m_float_feature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m       \u001b[0;34m'feature1'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0m_float_feature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     }\n",
      "\u001b[0;32m/var/folders/t5/9xgccmv92hnfvjwd62mk8zqh0000gn/T/ipykernel_25922/2948262194.py\u001b[0m in \u001b[0;36m_float_feature\u001b[0;34m(value)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_float_feature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;34m\"\"\"Returns a float_list from a float / double.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFeature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfloat_list\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatList\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: array([[0., 0., 0., ..., 0., 0., 0.]]) has type numpy.ndarray, but expected one of: int, long, float"
     ]
    }
   ],
   "source": [
    "#tf_serialize_example(f0,f1)\n",
    "\n",
    "serialize_example(f0np,f1np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Dimensions 3145728 and 105 are not compatible",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/t5/9xgccmv92hnfvjwd62mk8zqh0000gn/T/ipykernel_25922/3470775508.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mf1np\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mboxout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mboxout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mboxout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mfeatures_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_tensor_slices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf0np\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mf1np\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mf1\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfeatures_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/Paper1/lib/python3.7/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36mfrom_tensor_slices\u001b[0;34m(tensors)\u001b[0m\n\u001b[1;32m    689\u001b[0m       \u001b[0mDataset\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mA\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m     \"\"\"\n\u001b[0;32m--> 691\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mTensorSliceDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    692\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    693\u001b[0m   \u001b[0;32mclass\u001b[0m \u001b[0m_GeneratorState\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/Paper1/lib/python3.7/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, element)\u001b[0m\n\u001b[1;32m   3163\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tensors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3164\u001b[0m       batch_dim.assert_is_compatible_with(tensor_shape.Dimension(\n\u001b[0;32m-> 3165\u001b[0;31m           tensor_shape.dimension_value(t.get_shape()[0])))\n\u001b[0m\u001b[1;32m   3166\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3167\u001b[0m     variant_tensor = gen_dataset_ops.tensor_slice_dataset(\n",
      "\u001b[0;32m~/anaconda3/envs/Paper1/lib/python3.7/site-packages/tensorflow/python/framework/tensor_shape.py\u001b[0m in \u001b[0;36massert_is_compatible_with\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    280\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_compatible_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m       raise ValueError(\"Dimensions %s and %s are not compatible\" %\n\u001b[0;32m--> 282\u001b[0;31m                        (self, other))\n\u001b[0m\u001b[1;32m    283\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mmerge_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Dimensions 3145728 and 105 are not compatible"
     ]
    }
   ],
   "source": [
    "\n",
    "# for each annotation\n",
    "boxout = np.zeros([maxboxes+1,5])\n",
    "boxout[:bbox[0].shape[0],:] = bbox[0]\n",
    "\n",
    "f0np = arr.reshape(arr.shape[0]*arr.shape[1]*arr.shape[2])\n",
    "f0np = f0np.astype('float')/255.\n",
    "f1np = boxout.reshape(boxout.shape[0]*boxout.shape[1])\n",
    "\n",
    "features_dataset = tf.data.Dataset.from_tensor_slices((f0np,f1np))\n",
    "\n",
    "for f0,f1 in features_dataset.take(1):\n",
    "    #print(f0)\n",
    "    #print(f1)\n",
    "    tf_serialize_example(f0,f1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<TensorSliceDataset shapes: (3145728,), types: tf.float64>,\n",
       " <TensorSliceDataset shapes: (105,), types: tf.float64>)"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f0,f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TensorSliceDataset shapes: (21, 5), types: tf.float64>"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # load 1\n",
    "# imgs_name, bbox = parse_annotation([annotations[0]], LABELS, \n",
    "#                                                    feature_dir=classDir_main_to_imgs,\n",
    "#                                                    annotation_dir=classDir_main_to)\n",
    "\n",
    "\n",
    "# # read in and keep images -- npy files\n",
    "# for im in imgs_name:\n",
    "#     arr = np.load(im)#['arr_0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.load(im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "#arr.astype('uint8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nbatch = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #dataset = ArtificialDatasetNonAug(batch_size=32,split='valid')\n",
    "# b = np.load('/Users/jillnaiman/MegaYolo/binaries_model8_noncom/1988ApJ___334__144K_p8.npy')\n",
    "# b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "#b['arr_0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# benchmark tests\n",
    "import time\n",
    "def benchmark(dataset, num_epochs=1, nbatch=32):\n",
    "    start_time = time.perf_counter()\n",
    "    for epoch_num in range(num_epochs):\n",
    "        ienum=0\n",
    "        for sample in dataset:\n",
    "            #if ienum%10==0: print(ienum)\n",
    "            #print(ienum)\n",
    "            # Performing a training step\n",
    "            time.sleep(0.01)\n",
    "            ienum+=1\n",
    "            if ienum>nbatch: break\n",
    "    print(\"Execution time:\", time.perf_counter() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple\n",
    "#benchmark(ArtificialDatasetNonAug(batch_size=10,split='valid'),nbatch=nbatch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try different methods and time\n",
    "#nbatch = 10; nloop = 10\n",
    "nbatch = 25; nloop = 4\n",
    "th = []\n",
    "\n",
    "for n in range(nloop):\n",
    "    t1 = time.perf_counter()\n",
    "    benchmark(ArtificialDatasetNonAug(batch_size=nbatch,split='valid',method=method),\n",
    "              nbatch=nbatch)\n",
    "    t2 = time.perf_counter()\n",
    "    th.append(t2-t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40.807152897499996, 0.9870036160257788)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.median(th), np.std(th)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/jillnaiman/MegaYolo/binaries_model8_pickle/'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classDir_main_to_imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for tfrecords files\n",
    "def parse_proto(example_proto):\n",
    "    features = {\n",
    "        'X': tf.io.FixedLenFeature([], tf.float32),\n",
    "        'x1': tf.io.FixedLenSequenceFeature([10], tf.float32,allow_missing = True,default_value=0),\n",
    "        'x2': tf.io.FixedLenSequenceFeature([10], tf.float32,allow_missing = True,default_value=0),\n",
    "        'y1': tf.io.FixedLenSequenceFeature([10], tf.float32,allow_missing = True,default_value=0),\n",
    "        'y2': tf.io.FixedLenSequenceFeature([10], tf.float32,allow_missing = True,default_value=0),\n",
    "        'class': tf.io.FixedLenSequenceFeature([10], tf.float32,allow_missing = True,default_value=0),\n",
    "    }\n",
    "    parsed_features = tf.io.parse_single_example(example_proto, features)\n",
    "    print(parsed_features)\n",
    "    return parsed_features['X']\n",
    "#     y_true1, y_true2, y_true3 = [],[],[]\n",
    "#     for b in bbox:\n",
    "#         y1,y2,y3= process_box(b[:,:4], b[:,4].astype('int'),anchors,CLASS)\n",
    "#         y_true1.append(y1); y_true2.append(y2); y_true3.append(y3)\n",
    "#     # if there is no box, do something different\n",
    "#     if len(bbox) == 0:\n",
    "#         # fake a box\n",
    "#         b = np.array([[111.,  59., 403., 364. ,  4.]])\n",
    "#         y1,y2,y3= process_box(b[:,:4], b[:,4].astype('int'),anchors,CLASS)\n",
    "#         y1[:] = 0; y2[:]=0;y3[:]=0\n",
    "#         y_true1.append(y1); y_true2.append(y2); y_true3.append(y3)        \n",
    "#     img = tf.cast(np.array(imgs), tf.float32)     \n",
    "#     #time5 = time.perf_counter()\n",
    "#     #print('process blocks:', time5-time4)\n",
    "#     del imgs\n",
    "#     yield img, tf.cast(y_true1, tf.float32), tf.cast(y_true2, tf.float32), tf.cast(y_true3, tf.float32)\n",
    "\n",
    "    \n",
    "#     return parsed_features['X'], parsed_features['y']\n",
    "\n",
    "def read_tfrecords(file_names=(\"file1.tfrecord\", \"file2.tfrecord\", \"file3.tfrecord\"),\n",
    "                   buffer_size=10000,\n",
    "                   batch_size=100):\n",
    "    dataset = tf.data.TFRecordDataset(file_names)\n",
    "    dataset = dataset.map(parse_proto)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'X': <tf.Tensor 'ParseSingleExample/ParseExample/ParseExampleV2:0' shape=() dtype=float32>, 'class': <tf.Tensor 'ParseSingleExample/ParseExample/ParseExampleV2:1' shape=(None, 10) dtype=float32>, 'x1': <tf.Tensor 'ParseSingleExample/ParseExample/ParseExampleV2:2' shape=(None, 10) dtype=float32>, 'x2': <tf.Tensor 'ParseSingleExample/ParseExample/ParseExampleV2:3' shape=(None, 10) dtype=float32>, 'y1': <tf.Tensor 'ParseSingleExample/ParseExample/ParseExampleV2:4' shape=(None, 10) dtype=float32>, 'y2': <tf.Tensor 'ParseSingleExample/ParseExample/ParseExampleV2:5' shape=(None, 10) dtype=float32>}\n"
     ]
    }
   ],
   "source": [
    "dirtf = '/Users/jillnaiman/MegaYolo/binaries_model8_tfrecord/'\n",
    "tffiles = [dirtf+'1981AJ_____86__206V_p1.tfrecord',\n",
    "           dirtf+'1992AJ____104_2161O_p5.tfrecord',\n",
    "           dirtf+'1997AJ____114__913N_p13.tfrecord']\n",
    "\n",
    "\n",
    "dataset = read_tfrecords(file_names=tffiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-01 18:31:54.646748: W tensorflow/core/framework/op_kernel.cc:1763] OP_REQUIRES failed at example_parsing_ops.cc:94 : Invalid argument: Key: x2.  Can't parse serialized Example.\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Key: x2.  Can't parse serialized Example.\n\t [[{{node ParseSingleExample/ParseExample/ParseExampleV2}}]]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/Paper1/lib/python3.7/site-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36mexecution_mode\u001b[0;34m(mode)\u001b[0m\n\u001b[1;32m   2112\u001b[0m       \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecutor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexecutor_new\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2113\u001b[0;31m       \u001b[0;32myield\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2114\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/Paper1/lib/python3.7/site-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m_next_internal\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    732\u001b[0m           \u001b[0moutput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flat_output_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 733\u001b[0;31m           output_shapes=self._flat_output_shapes)\n\u001b[0m\u001b[1;32m    734\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/Paper1/lib/python3.7/site-packages/tensorflow/python/ops/gen_dataset_ops.py\u001b[0m in \u001b[0;36miterator_get_next\u001b[0;34m(iterator, output_types, output_shapes, name)\u001b[0m\n\u001b[1;32m   2578\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2579\u001b[0;31m       \u001b[0m_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2580\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/Paper1/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   6861\u001b[0m   \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6862\u001b[0;31m   \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6863\u001b[0m   \u001b[0;31m# pylint: enable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/Paper1/lib/python3.7/site-packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Key: x2.  Can't parse serialized Example.\n\t [[{{node ParseSingleExample/ParseExample/ParseExampleV2}}]] [Op:IteratorGetNext]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/t5/9xgccmv92hnfvjwd62mk8zqh0000gn/T/ipykernel_25922/3134862119.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mparsed_record\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparsed_record\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/Paper1/lib/python3.7/site-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    745\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    746\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 747\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    748\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    749\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/Paper1/lib/python3.7/site-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m_next_internal\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    737\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_element_spec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_from_compatible_tensor_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    738\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 739\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mstructure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_compatible_tensor_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_element_spec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    741\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/Paper1/lib/python3.7/contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type, value, traceback)\u001b[0m\n\u001b[1;32m    128\u001b[0m                 \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthrow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraceback\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m                 \u001b[0;31m# Suppress StopIteration *unless* it's the same exception that\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/Paper1/lib/python3.7/site-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36mexecution_mode\u001b[0;34m(mode)\u001b[0m\n\u001b[1;32m   2114\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2115\u001b[0m       \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecutor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexecutor_old\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2116\u001b[0;31m       \u001b[0mexecutor_new\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/Paper1/lib/python3.7/site-packages/tensorflow/python/eager/executor.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     67\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;34m\"\"\"Waits for ops dispatched in this executor to finish.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m     \u001b[0mpywrap_tfe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTFE_ExecutorWaitForAllPendingNodes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mclear_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Key: x2.  Can't parse serialized Example.\n\t [[{{node ParseSingleExample/ParseExample/ParseExampleV2}}]]"
     ]
    }
   ],
   "source": [
    "for parsed_record in dataset.take(2):\n",
    "    print(parsed_record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now, following: https://stackoverflow.com/questions/48889482/feeding-npy-numpy-files-into-tensorflow-data-pipeline\n",
    "# try with io decode?\n",
    "def npy_header_offset(npy_path):\n",
    "    with open(str(npy_path), 'rb') as f:\n",
    "        print(f.read(6).decode('utf-8'))\n",
    "        if f.read(6) != b'\\x93NUMPY':\n",
    "            raise ValueError('Invalid NPY file.')\n",
    "        version_major, version_minor = f.read(2)\n",
    "        if version_major == 1:\n",
    "            header_len_size = 2\n",
    "        elif version_major == 2:\n",
    "            header_len_size = 4\n",
    "        else:\n",
    "            raise ValueError('Unknown NPY file version {}.{}.'.format(version_major, version_minor))\n",
    "        header_len = sum(b << (8 * i) for i, b in enumerate(f.read(header_len_size)))\n",
    "        header = f.read(header_len)\n",
    "        if not header.endswith(b'\\n'):\n",
    "            raise ValueError('Invalid NPY file.')\n",
    "        return f.tell()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PK\u0003\u0004\u0014\u0000\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Invalid NPY file.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/t5/9xgccmv92hnfvjwd62mk8zqh0000gn/T/ipykernel_25922/704836339.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m                 \u001b[0mnfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'arr_0'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m                 \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'arr_0'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m             \u001b[0mheader_offset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnpy_header_offset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mmod\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'npy'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0mb\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/t5/9xgccmv92hnfvjwd62mk8zqh0000gn/T/ipykernel_25922/838241765.py\u001b[0m in \u001b[0;36mnpy_header_offset\u001b[0;34m(npy_path)\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34mb'\\x93NUMPY'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Invalid NPY file.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0mversion_major\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mversion_minor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mversion_major\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Invalid NPY file."
     ]
    }
   ],
   "source": [
    "file_list1 = pd.read_csv(splitsDir+'train.csv',names=['filename'])['filename'].values.astype('str')\n",
    "file_list = []; nfeatures = -1; dtype = -1\n",
    "for iff,f in enumerate(file_list1):\n",
    "    f = f.rstrip('.xml').split('/')[-1]\n",
    "    if iff==0: # if first one, fill in the gaps\n",
    "        fg = glob.glob(classDir_main_to_imgs + f + '*')[0]\n",
    "        mod = fg.split('/')[-1]\n",
    "        mod = mod[mod.rfind('.')+1:]\n",
    "        # read in and keep images -- npy files/npz\n",
    "        if mod == 'npz':\n",
    "            with np.load(fg) as b:\n",
    "                nfeatures = b['arr_0'].shape[-1]\n",
    "                dtype = b['arr_0'].dtype\n",
    "            header_offset = npy_header_offset(fg)\n",
    "        elif mod == 'npy':\n",
    "            b=np.load(fg)\n",
    "            if type(b) != np.ndarray:\n",
    "                b = b['arr_0'] # WHY????\n",
    "                nfeatures = b.shape[-1]\n",
    "                dtype = b.dtype\n",
    "        elif mod == 'pickle':\n",
    "            print('not implemented!')\n",
    "            import sys; sys.exit()\n",
    "            with open(fg, 'rb') as ff:\n",
    "                b = pickle.load(ff) \n",
    "                nfeatures = np.array(b).shape[-1]\n",
    "                dtype = np.array(b).dtype\n",
    "        else:\n",
    "            print('no idea what this method is')\n",
    "        # what is dtype?\n",
    "        if dtype == np.dtype('uint8'): dtype = tf.uint8\n",
    "        if dtype == np.dtype('float64'): dtype = tf.float64\n",
    "    file_list.append(classDir_main_to_imgs+f+'.'+mod)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([], 12, dtype('uint8'))"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# alright maybe try recordio afterall?\n",
    "#https://github.com/tensorflow/tensorflow/blob/r1.3/tensorflow/examples/how_tos/reading_data/convert_to_records.py\n",
    "#https://stackoverflow.com/questions/50665144/create-tfrecord-for-object-detection-task\n",
    "#https://stackoverflow.com/questions/46820500/how-to-handle-large-amouts-of-data-in-tensorflow/47040165#47040165"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "recordIOdir = config.tmp_storage_dir + 'rio/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "def array_to_tfrecords(X, boxes, output_file):\n",
    "    if len(boxes)>0:\n",
    "        x1 = boxes[0][:,0]; y1 = boxes[0][:,1]; x2 = boxes[0][:,2]; y2 = boxes[0][:,3]\n",
    "        classes = boxes[0][:,4]\n",
    "    else:\n",
    "        x1=np.array([]);y1=np.array([]);x2=np.array([]);y2=np.array([]);classes=np.array([])\n",
    "    # do division already\n",
    "    X = X/255.\n",
    "    feature = {\n",
    "        'X': tf.train.Feature(float_list=tf.train.FloatList(value=X.flatten())),\n",
    "        'x1': tf.train.Feature(float_list=tf.train.FloatList(value=x1.flatten())),\n",
    "        'y1': tf.train.Feature(float_list=tf.train.FloatList(value=y1.flatten())),\n",
    "        'x2': tf.train.Feature(float_list=tf.train.FloatList(value=x2.flatten())),\n",
    "        'y2': tf.train.Feature(float_list=tf.train.FloatList(value=y2.flatten())),\n",
    "        'class': tf.train.Feature(float_list=tf.train.FloatList(value=classes.flatten()))\n",
    "    }\n",
    "    example = tf.train.Example(features=tf.train.Features(feature=feature))\n",
    "    serialized = example.SerializeToString()\n",
    "\n",
    "    writer = tf.io.TFRecordWriter(output_file)\n",
    "    writer.write(serialized)\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imgs_name, bbox = parse_annotation([classDir_main_to+f+'.xml'], LABELS, \n",
    "#                                                    feature_dir=classDir_main_to_imgs,\n",
    "#                                                    annotation_dir=classDir_main_to)\n",
    "\n",
    "# X = np.load(imgs_name[0])['arr_0']\n",
    "# array_to_tfrecords(X, bbox, recordIOdir+imgs_name[0].rstrip('.npz').split('/')[-1]+'.tfrecord')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 of 5515\n",
      "100 of 5515\n",
      "200 of 5515\n",
      "300 of 5515\n",
      "400 of 5515\n",
      "500 of 5515\n",
      "600 of 5515\n",
      "700 of 5515\n",
      "800 of 5515\n",
      "900 of 5515\n",
      "1000 of 5515\n",
      "1100 of 5515\n",
      "1200 of 5515\n",
      "1300 of 5515\n",
      "1400 of 5515\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/t5/9xgccmv92hnfvjwd62mk8zqh0000gn/T/ipykernel_25922/913782921.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimgs_name\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'arr_0'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0marray_to_tfrecords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbbox\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecordIOdir\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mimgs_name\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.npz'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'.tfrecord'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0;32mdel\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/t5/9xgccmv92hnfvjwd62mk8zqh0000gn/T/ipykernel_25922/1937598323.py\u001b[0m in \u001b[0;36marray_to_tfrecords\u001b[0;34m(X, boxes, output_file)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m255.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     feature = {\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0;34m'X'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFeature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfloat_list\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatList\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0;34m'x1'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFeature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfloat_list\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatList\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;34m'y1'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFeature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfloat_list\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatList\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# convert all\n",
    "for iff,f in enumerate(annotations):\n",
    "    imgs_name, bbox = parse_annotation([classDir_main_to+f.split('/')[-1]], LABELS, \n",
    "                                                       feature_dir=classDir_main_to_imgs,\n",
    "                                                       annotation_dir=classDir_main_to)\n",
    "\n",
    "    X = np.load(imgs_name[0])['arr_0']\n",
    "    array_to_tfrecords(X, bbox, recordIOdir+imgs_name[0].rstrip('.npz').split('/')[-1]+'.tfrecord')\n",
    "    del X\n",
    "    \n",
    "    if iff%100==0: print(iff,'of',len(annotations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5515"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(annotations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['/Users/jillnaiman/tmpMegaYolo/binaries/yolo_512x512_cap_ann/1952ApJ___115___82K_p1.xml',\n",
       "       '/Users/jillnaiman/tmpMegaYolo/binaries/yolo_512x512_cap_ann/1994ApJ___427L__43F_p4.xml',\n",
       "       '/Users/jillnaiman/tmpMegaYolo/binaries/yolo_512x512_cap_ann/1995ApJ___445__246C_p6.xml',\n",
       "       ...,\n",
       "       '/Users/jillnaiman/tmpMegaYolo/binaries/yolo_512x512_cap_ann/1978ApJ___220___14H_p1.xml',\n",
       "       '/Users/jillnaiman/tmpMegaYolo/binaries/yolo_512x512_cap_ann/1982ApJ___252__269Y_p2.xml',\n",
       "       '/Users/jillnaiman/tmpMegaYolo/binaries/yolo_512x512_cap_ann/1982ApJ___258__467Y_p2.xml'],\n",
       "      dtype='<U88')"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_list1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parse CSV: 6.593299985979684e-05\n",
      "parse CSV: 7.796400132065173e-05\n",
      "Parse annotation: 0.14502690299923415\n",
      "Parse annotation: 0.14677225699961127\n",
      "Load data: 0.12156924800001434\n",
      "Load data: 0.11779988200032676\n",
      "process blocks:process blocks: 0.1032422560001578\n",
      " 0.10012778300006175\n",
      "0\n",
      "parse CSV:parse CSV: 0.00013102500088280067\n",
      " 5.655000131810084e-05\n",
      "1\n",
      "Parse annotation: 0.15298127499954717\n",
      "Parse annotation: 0.15567094499965606\n",
      "Load data: 0.12399737299892877\n",
      "Load data: 0.12808158799998637\n",
      "process blocks: process blocks: 0.12697629399917787\n",
      "0.1203326869999728\n",
      "2parse CSV:parse CSV: 5.4875999921932817e-05\n",
      " 6.429899985960219e-05\n",
      "\n",
      "3\n",
      "Parse annotation: 0.14686419899953762\n",
      "Parse annotation: 0.16171898799984774\n",
      "Load data: 0.1252131900000677\n",
      "Load data: 0.12484830600078567\n",
      "process blocks:process blocks:  0.13700858399897697\n",
      "0.12357732499913254\n",
      "4parse CSV:parse CSV: 6.186500104377046e-05\n",
      "\n",
      " 8.753600013733376e-05\n",
      "5\n",
      "Parse annotation: 0.14494591199945717\n",
      "Parse annotation: 0.14763279299950227\n",
      "Load data: 0.1285129509997205\n",
      "Load data: 0.1277994949996355\n",
      "process blocks:process blocks: 0.12277748500127927\n",
      " 0.12455471900102566\n",
      "parse CSV: 6.833099905634299e-05\n",
      "parse CSV:6 \n",
      "7.652000022062566e-05\n",
      "7\n",
      "Parse annotation: 0.4687751359997492\n",
      "Parse annotation: 0.172946922999472\n",
      "Load data: 0.13505229000111285\n",
      "Load data: 0.1318750820009882\n",
      "process blocks: process blocks: 0.1395990349992644\n",
      "0.1316464569990785\n",
      "8\n",
      "parse CSV: 7.178299892984796e-05\n",
      "9parse CSV:\n",
      " 0.0015495119987463113\n",
      "Parse annotation:Parse annotation:  0.18537510900023335\n",
      "0.18108187100006035\n",
      "Load data: 0.13149454800077365\n",
      "Load data: 0.12941067999963707\n",
      "process blocks:process blocks: 0.12608493399966392\n",
      " 0.1266717679991416\n",
      "parse CSV:10parse CSV: 6.294599916145671e-05\n",
      "\n",
      " 7.425900002999697e-05\n",
      "Parse annotation: 0.14292050900075992\n",
      "Parse annotation: 0.13754443200014066\n",
      "Load data: 0.18013043599967204\n",
      "Load data: 0.20016973999918264\n",
      "process blocks: 0.10201240500100539\n",
      "process blocks: 0.4326179309991858\n",
      "Execution time: 7.084822689001157\n"
     ]
    }
   ],
   "source": [
    "benchmark(\n",
    "    tf.data.Dataset.range(2)\n",
    "    .interleave(\n",
    "        lambda _: ArtificialDatasetNonAug(batch_size=nbatch,split='valid'),\n",
    "        num_parallel_calls=tf.data.AUTOTUNE\n",
    "    )#.shuffle(3)\n",
    "    , nbatch=nbatch\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m\n",
       "\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterleave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmap_func\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mcycle_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mblock_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mnum_parallel_calls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mdeterministic\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "Maps `map_func` across this dataset, and interleaves the results.\n",
       "\n",
       "For example, you can use `Dataset.interleave()` to process many input files\n",
       "concurrently:\n",
       "\n",
       ">>> # Preprocess 4 files concurrently, and interleave blocks of 16 records\n",
       ">>> # from each file.\n",
       ">>> filenames = [\"/var/data/file1.txt\", \"/var/data/file2.txt\",\n",
       "...              \"/var/data/file3.txt\", \"/var/data/file4.txt\"]\n",
       ">>> dataset = tf.data.Dataset.from_tensor_slices(filenames)\n",
       ">>> def parse_fn(filename):\n",
       "...   return tf.data.Dataset.range(10)\n",
       ">>> dataset = dataset.interleave(lambda x:\n",
       "...     tf.data.TextLineDataset(x).map(parse_fn, num_parallel_calls=1),\n",
       "...     cycle_length=4, block_length=16)\n",
       "\n",
       "The `cycle_length` and `block_length` arguments control the order in which\n",
       "elements are produced. `cycle_length` controls the number of input elements\n",
       "that are processed concurrently. If you set `cycle_length` to 1, this\n",
       "transformation will handle one input element at a time, and will produce\n",
       "identical results to `tf.data.Dataset.flat_map`. In general,\n",
       "this transformation will apply `map_func` to `cycle_length` input elements,\n",
       "open iterators on the returned `Dataset` objects, and cycle through them\n",
       "producing `block_length` consecutive elements from each iterator, and\n",
       "consuming the next input element each time it reaches the end of an\n",
       "iterator.\n",
       "\n",
       "For example:\n",
       "\n",
       ">>> dataset = Dataset.range(1, 6)  # ==> [ 1, 2, 3, 4, 5 ]\n",
       ">>> # NOTE: New lines indicate \"block\" boundaries.\n",
       ">>> dataset = dataset.interleave(\n",
       "...     lambda x: Dataset.from_tensors(x).repeat(6),\n",
       "...     cycle_length=2, block_length=4)\n",
       ">>> list(dataset.as_numpy_iterator())\n",
       "[1, 1, 1, 1,\n",
       " 2, 2, 2, 2,\n",
       " 1, 1,\n",
       " 2, 2,\n",
       " 3, 3, 3, 3,\n",
       " 4, 4, 4, 4,\n",
       " 3, 3,\n",
       " 4, 4,\n",
       " 5, 5, 5, 5,\n",
       " 5, 5]\n",
       "\n",
       "Note: The order of elements yielded by this transformation is\n",
       "deterministic, as long as `map_func` is a pure function and\n",
       "`deterministic=True`. If `map_func` contains any stateful operations, the\n",
       "order in which that state is accessed is undefined.\n",
       "\n",
       "Performance can often be improved by setting `num_parallel_calls` so that\n",
       "`interleave` will use multiple threads to fetch elements. If determinism\n",
       "isn't required, it can also improve performance to set\n",
       "`deterministic=False`.\n",
       "\n",
       ">>> filenames = [\"/var/data/file1.txt\", \"/var/data/file2.txt\",\n",
       "...              \"/var/data/file3.txt\", \"/var/data/file4.txt\"]\n",
       ">>> dataset = tf.data.Dataset.from_tensor_slices(filenames)\n",
       ">>> dataset = dataset.interleave(lambda x: tf.data.TFRecordDataset(x),\n",
       "...     cycle_length=4, num_parallel_calls=tf.data.AUTOTUNE,\n",
       "...     deterministic=False)\n",
       "\n",
       "Args:\n",
       "  map_func: A function mapping a dataset element to a dataset.\n",
       "  cycle_length: (Optional.) The number of input elements that will be\n",
       "    processed concurrently. If not set, the tf.data runtime decides what it\n",
       "    should be based on available CPU. If `num_parallel_calls` is set to\n",
       "    `tf.data.AUTOTUNE`, the `cycle_length` argument identifies\n",
       "    the maximum degree of parallelism.\n",
       "  block_length: (Optional.) The number of consecutive elements to produce\n",
       "    from each input element before cycling to another input element. If not\n",
       "    set, defaults to 1.\n",
       "  num_parallel_calls: (Optional.) If specified, the implementation creates a\n",
       "    threadpool, which is used to fetch inputs from cycle elements\n",
       "    asynchronously and in parallel. The default behavior is to fetch inputs\n",
       "    from cycle elements synchronously with no parallelism. If the value\n",
       "    `tf.data.AUTOTUNE` is used, then the number of parallel\n",
       "    calls is set dynamically based on available CPU.\n",
       "  deterministic: (Optional.) A boolean controlling whether determinism\n",
       "    should be traded for performance by allowing elements to be produced out\n",
       "    of order.  If `deterministic` is `None`, the\n",
       "    `tf.data.Options.experimental_deterministic` dataset option (`True` by\n",
       "    default) is used to decide whether to produce elements\n",
       "    deterministically.\n",
       "\n",
       "Returns:\n",
       "  Dataset: A `Dataset`.\n",
       "\u001b[0;31mFile:\u001b[0m      ~/anaconda3/envs/Paper1/lib/python3.7/site-packages/tensorflow/python/data/ops/dataset_ops.py\n",
       "\u001b[0;31mType:\u001b[0m      function\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tf.data.Dataset.interleave?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RRyluqn6vvaW"
   },
   "source": [
    "Steps per epoch -- training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1638305356592,
     "user": {
      "displayName": "Jill Naiman",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "13328008118965652761"
     },
     "user_tz": 360
    },
    "id": "SdEYPW-tvvaW",
    "outputId": "6c038327-6fe5-439a-c5be-9cedf4f87cec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps per epoch = training: 828 , validation: 164\n"
     ]
    }
   ],
   "source": [
    "#steps = len(X_train) // batch_size\n",
    "#print(len(X_train)//batch_size)\n",
    "# can do larger with augmentation\n",
    "\n",
    "aug_fac = 2 # 2 or 3\n",
    "\n",
    "steps_training = (len(X_train)//batch_size)*aug_fac\n",
    "# factor of 2 from: https://stackoverflow.com/questions/49922252/choosing-number-of-steps-per-epoch\n",
    "\n",
    "steps_val = (len(X_valid)//batch_size)*aug_fac\n",
    "\n",
    "print('Steps per epoch = training:', steps_training, ', validation:', steps_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sExlEUTwIYBy"
   },
   "source": [
    "Save also the names of the test instances to use later:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1638305356592,
     "user": {
      "displayName": "Jill Naiman",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "13328008118965652761"
     },
     "user_tz": 360
    },
    "id": "y4LLclWnIQ49",
    "outputId": "958a13f5-ddcb-4b0f-f31f-4f7dc9795857"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hey, saved tests!\n"
     ]
    }
   ],
   "source": [
    "# save test list in an extra place... this is a bit redundant since its saved another place too\n",
    "#if not re_run_from_splits:\n",
    "np.savetxt(saveFile, X_test, delimiter=',',fmt='%s')\n",
    "print('Hey, saved tests!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7gQi6kS8nea8"
   },
   "source": [
    "# 1. Define YOLO model\n",
    "\n",
    "For v5, see: https://github.com/jahongir7174/YOLOv5-tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zoBEY7n67jy1"
   },
   "source": [
    "For creating the model -- how many features are we using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1638305356593,
     "user": {
      "displayName": "Jill Naiman",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "13328008118965652761"
     },
     "user_tz": 360
    },
    "id": "ymfe7jyL7jy1",
    "outputId": "eb9c56e9-a90b-4bec-cd53-f2747631615b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_features = get_n_features(classDir_main_to_imgs)\n",
    "n_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "executionInfo": {
     "elapsed": 2663,
     "status": "ok",
     "timestamp": 1638305359251,
     "user": {
      "displayName": "Jill Naiman",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "13328008118965652761"
     },
     "user_tz": 360
    },
    "id": "KRQsM_X7XC-V"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-30 14:53:47.147006: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "version = 'l' # large version\n",
    "model = build_model(n_features, anchors, version, len(LABELS))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yXdkSQK9Emky"
   },
   "source": [
    "Build YOLOv5:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 276,
     "status": "ok",
     "timestamp": 1638305359522,
     "user": {
      "displayName": "Jill Naiman",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "13328008118965652761"
     },
     "user_tz": 360
    },
    "id": "0Vnf5mjn7jy2",
    "outputId": "6a1d77e9-e8ef-41d8-b1b3-d11ccad3c0fa",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 512, 512, 12 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.space_to_depth (TFOpLambd (None, 256, 256, 48) 0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 256, 256, 64) 27648       tf.nn.space_to_depth[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 256, 256, 64) 256         conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 256, 256, 64) 0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d (ZeroPadding2D)  (None, 257, 257, 64) 0           activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 128, 128, 128 73728       zero_padding2d[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 128, 128, 128 512         conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 128, 128, 128 0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 128, 128, 64) 8192        activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 128, 128, 64) 256         conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 128, 128, 64) 0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 128, 128, 64) 4096        activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 128, 128, 64) 256         conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 128, 128, 64) 0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 128, 128, 64) 36864       activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 128, 128, 64) 256         conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 128, 128, 64) 0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add (TFOpLambd (None, 128, 128, 64) 0           activation_2[0][0]               \n",
      "                                                                 activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 128, 128, 64) 4096        tf.__operators__.add[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 128, 128, 64) 256         conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 128, 128, 64) 0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 128, 128, 64) 36864       activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 128, 128, 64) 256         conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 128, 128, 64) 0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_1 (TFOpLam (None, 128, 128, 64) 0           tf.__operators__.add[0][0]       \n",
      "                                                                 activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 128, 128, 64) 4096        tf.__operators__.add_1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 128, 128, 64) 256         conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 128, 128, 64) 0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 128, 128, 64) 36864       activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 128, 128, 64) 8192        activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 128, 128, 64) 256         conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 128, 128, 64) 256         conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 128, 128, 64) 0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 128, 128, 64) 0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_2 (TFOpLam (None, 128, 128, 64) 0           tf.__operators__.add_1[0][0]     \n",
      "                                                                 activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 128, 128, 128 0           activation_9[0][0]               \n",
      "                                                                 tf.__operators__.add_2[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 128, 128, 128 16384       concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 128, 128, 128 512         conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 128, 128, 128 0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_1 (ZeroPadding2D (None, 129, 129, 128 0           activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 64, 64, 256)  294912      zero_padding2d_1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 64, 64, 256)  1024        conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 64, 64, 256)  0           batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 64, 64, 128)  32768       activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 64, 64, 128)  512         conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 64, 64, 128)  0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 64, 64, 128)  16384       activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 64, 64, 128)  512         conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 64, 64, 128)  0           batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 64, 64, 128)  147456      activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 64, 64, 128)  512         conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 64, 64, 128)  0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_3 (TFOpLam (None, 64, 64, 128)  0           activation_12[0][0]              \n",
      "                                                                 activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 64, 64, 128)  16384       tf.__operators__.add_3[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 64, 64, 128)  512         conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 64, 64, 128)  0           batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 64, 64, 128)  147456      activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 64, 64, 128)  512         conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 64, 64, 128)  0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_4 (TFOpLam (None, 64, 64, 128)  0           tf.__operators__.add_3[0][0]     \n",
      "                                                                 activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 64, 64, 128)  16384       tf.__operators__.add_4[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 64, 64, 128)  512         conv2d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 64, 64, 128)  0           batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, 64, 64, 128)  147456      activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNo (None, 64, 64, 128)  512         conv2d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 64, 64, 128)  0           batch_normalization_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_5 (TFOpLam (None, 64, 64, 128)  0           tf.__operators__.add_4[0][0]     \n",
      "                                                                 activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_19 (Conv2D)              (None, 64, 64, 128)  16384       tf.__operators__.add_5[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_19 (BatchNo (None, 64, 64, 128)  512         conv2d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, 64, 64, 128)  0           batch_normalization_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_20 (Conv2D)              (None, 64, 64, 128)  147456      activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_20 (BatchNo (None, 64, 64, 128)  512         conv2d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_20 (Activation)      (None, 64, 64, 128)  0           batch_normalization_20[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_6 (TFOpLam (None, 64, 64, 128)  0           tf.__operators__.add_5[0][0]     \n",
      "                                                                 activation_20[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_21 (Conv2D)              (None, 64, 64, 128)  16384       tf.__operators__.add_6[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_21 (BatchNo (None, 64, 64, 128)  512         conv2d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_21 (Activation)      (None, 64, 64, 128)  0           batch_normalization_21[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_22 (Conv2D)              (None, 64, 64, 128)  147456      activation_21[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_22 (BatchNo (None, 64, 64, 128)  512         conv2d_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_22 (Activation)      (None, 64, 64, 128)  0           batch_normalization_22[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_7 (TFOpLam (None, 64, 64, 128)  0           tf.__operators__.add_6[0][0]     \n",
      "                                                                 activation_22[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_23 (Conv2D)              (None, 64, 64, 128)  16384       tf.__operators__.add_7[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_23 (BatchNo (None, 64, 64, 128)  512         conv2d_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_23 (Activation)      (None, 64, 64, 128)  0           batch_normalization_23[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_24 (Conv2D)              (None, 64, 64, 128)  147456      activation_23[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_24 (BatchNo (None, 64, 64, 128)  512         conv2d_24[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_24 (Activation)      (None, 64, 64, 128)  0           batch_normalization_24[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_8 (TFOpLam (None, 64, 64, 128)  0           tf.__operators__.add_7[0][0]     \n",
      "                                                                 activation_24[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_25 (Conv2D)              (None, 64, 64, 128)  16384       tf.__operators__.add_8[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_25 (BatchNo (None, 64, 64, 128)  512         conv2d_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_25 (Activation)      (None, 64, 64, 128)  0           batch_normalization_25[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_26 (Conv2D)              (None, 64, 64, 128)  147456      activation_25[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_26 (BatchNo (None, 64, 64, 128)  512         conv2d_26[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_26 (Activation)      (None, 64, 64, 128)  0           batch_normalization_26[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_9 (TFOpLam (None, 64, 64, 128)  0           tf.__operators__.add_8[0][0]     \n",
      "                                                                 activation_26[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_27 (Conv2D)              (None, 64, 64, 128)  16384       tf.__operators__.add_9[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_27 (BatchNo (None, 64, 64, 128)  512         conv2d_27[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_27 (Activation)      (None, 64, 64, 128)  0           batch_normalization_27[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_28 (Conv2D)              (None, 64, 64, 128)  147456      activation_27[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_28 (BatchNo (None, 64, 64, 128)  512         conv2d_28[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_28 (Activation)      (None, 64, 64, 128)  0           batch_normalization_28[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_10 (TFOpLa (None, 64, 64, 128)  0           tf.__operators__.add_9[0][0]     \n",
      "                                                                 activation_28[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_29 (Conv2D)              (None, 64, 64, 128)  16384       tf.__operators__.add_10[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_29 (BatchNo (None, 64, 64, 128)  512         conv2d_29[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_29 (Activation)      (None, 64, 64, 128)  0           batch_normalization_29[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_30 (Conv2D)              (None, 64, 64, 128)  147456      activation_29[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_31 (Conv2D)              (None, 64, 64, 128)  32768       activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_30 (BatchNo (None, 64, 64, 128)  512         conv2d_30[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_31 (BatchNo (None, 64, 64, 128)  512         conv2d_31[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_30 (Activation)      (None, 64, 64, 128)  0           batch_normalization_30[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_31 (Activation)      (None, 64, 64, 128)  0           batch_normalization_31[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_11 (TFOpLa (None, 64, 64, 128)  0           tf.__operators__.add_10[0][0]    \n",
      "                                                                 activation_30[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 64, 64, 256)  0           activation_31[0][0]              \n",
      "                                                                 tf.__operators__.add_11[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_32 (Conv2D)              (None, 64, 64, 256)  65536       concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_32 (BatchNo (None, 64, 64, 256)  1024        conv2d_32[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_32 (Activation)      (None, 64, 64, 256)  0           batch_normalization_32[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_2 (ZeroPadding2D (None, 65, 65, 256)  0           activation_32[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_33 (Conv2D)              (None, 32, 32, 512)  1179648     zero_padding2d_2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_33 (BatchNo (None, 32, 32, 512)  2048        conv2d_33[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_33 (Activation)      (None, 32, 32, 512)  0           batch_normalization_33[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_34 (Conv2D)              (None, 32, 32, 256)  131072      activation_33[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_34 (BatchNo (None, 32, 32, 256)  1024        conv2d_34[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_34 (Activation)      (None, 32, 32, 256)  0           batch_normalization_34[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_35 (Conv2D)              (None, 32, 32, 256)  65536       activation_34[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_35 (BatchNo (None, 32, 32, 256)  1024        conv2d_35[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_35 (Activation)      (None, 32, 32, 256)  0           batch_normalization_35[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_36 (Conv2D)              (None, 32, 32, 256)  589824      activation_35[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_36 (BatchNo (None, 32, 32, 256)  1024        conv2d_36[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_36 (Activation)      (None, 32, 32, 256)  0           batch_normalization_36[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_12 (TFOpLa (None, 32, 32, 256)  0           activation_34[0][0]              \n",
      "                                                                 activation_36[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_37 (Conv2D)              (None, 32, 32, 256)  65536       tf.__operators__.add_12[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_37 (BatchNo (None, 32, 32, 256)  1024        conv2d_37[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_37 (Activation)      (None, 32, 32, 256)  0           batch_normalization_37[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_38 (Conv2D)              (None, 32, 32, 256)  589824      activation_37[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_38 (BatchNo (None, 32, 32, 256)  1024        conv2d_38[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_38 (Activation)      (None, 32, 32, 256)  0           batch_normalization_38[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_13 (TFOpLa (None, 32, 32, 256)  0           tf.__operators__.add_12[0][0]    \n",
      "                                                                 activation_38[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_39 (Conv2D)              (None, 32, 32, 256)  65536       tf.__operators__.add_13[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_39 (BatchNo (None, 32, 32, 256)  1024        conv2d_39[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_39 (Activation)      (None, 32, 32, 256)  0           batch_normalization_39[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_40 (Conv2D)              (None, 32, 32, 256)  589824      activation_39[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_40 (BatchNo (None, 32, 32, 256)  1024        conv2d_40[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_40 (Activation)      (None, 32, 32, 256)  0           batch_normalization_40[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_14 (TFOpLa (None, 32, 32, 256)  0           tf.__operators__.add_13[0][0]    \n",
      "                                                                 activation_40[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_41 (Conv2D)              (None, 32, 32, 256)  65536       tf.__operators__.add_14[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_41 (BatchNo (None, 32, 32, 256)  1024        conv2d_41[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_41 (Activation)      (None, 32, 32, 256)  0           batch_normalization_41[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_42 (Conv2D)              (None, 32, 32, 256)  589824      activation_41[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_42 (BatchNo (None, 32, 32, 256)  1024        conv2d_42[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_42 (Activation)      (None, 32, 32, 256)  0           batch_normalization_42[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_15 (TFOpLa (None, 32, 32, 256)  0           tf.__operators__.add_14[0][0]    \n",
      "                                                                 activation_42[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_43 (Conv2D)              (None, 32, 32, 256)  65536       tf.__operators__.add_15[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_43 (BatchNo (None, 32, 32, 256)  1024        conv2d_43[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_43 (Activation)      (None, 32, 32, 256)  0           batch_normalization_43[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_44 (Conv2D)              (None, 32, 32, 256)  589824      activation_43[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_44 (BatchNo (None, 32, 32, 256)  1024        conv2d_44[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_44 (Activation)      (None, 32, 32, 256)  0           batch_normalization_44[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_16 (TFOpLa (None, 32, 32, 256)  0           tf.__operators__.add_15[0][0]    \n",
      "                                                                 activation_44[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_45 (Conv2D)              (None, 32, 32, 256)  65536       tf.__operators__.add_16[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_45 (BatchNo (None, 32, 32, 256)  1024        conv2d_45[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_45 (Activation)      (None, 32, 32, 256)  0           batch_normalization_45[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_46 (Conv2D)              (None, 32, 32, 256)  589824      activation_45[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_46 (BatchNo (None, 32, 32, 256)  1024        conv2d_46[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_46 (Activation)      (None, 32, 32, 256)  0           batch_normalization_46[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_17 (TFOpLa (None, 32, 32, 256)  0           tf.__operators__.add_16[0][0]    \n",
      "                                                                 activation_46[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_47 (Conv2D)              (None, 32, 32, 256)  65536       tf.__operators__.add_17[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_47 (BatchNo (None, 32, 32, 256)  1024        conv2d_47[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_47 (Activation)      (None, 32, 32, 256)  0           batch_normalization_47[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_48 (Conv2D)              (None, 32, 32, 256)  589824      activation_47[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_48 (BatchNo (None, 32, 32, 256)  1024        conv2d_48[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_48 (Activation)      (None, 32, 32, 256)  0           batch_normalization_48[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_18 (TFOpLa (None, 32, 32, 256)  0           tf.__operators__.add_17[0][0]    \n",
      "                                                                 activation_48[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_49 (Conv2D)              (None, 32, 32, 256)  65536       tf.__operators__.add_18[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_49 (BatchNo (None, 32, 32, 256)  1024        conv2d_49[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_49 (Activation)      (None, 32, 32, 256)  0           batch_normalization_49[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_50 (Conv2D)              (None, 32, 32, 256)  589824      activation_49[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_50 (BatchNo (None, 32, 32, 256)  1024        conv2d_50[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_50 (Activation)      (None, 32, 32, 256)  0           batch_normalization_50[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_19 (TFOpLa (None, 32, 32, 256)  0           tf.__operators__.add_18[0][0]    \n",
      "                                                                 activation_50[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_51 (Conv2D)              (None, 32, 32, 256)  65536       tf.__operators__.add_19[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_51 (BatchNo (None, 32, 32, 256)  1024        conv2d_51[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_51 (Activation)      (None, 32, 32, 256)  0           batch_normalization_51[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_52 (Conv2D)              (None, 32, 32, 256)  589824      activation_51[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_53 (Conv2D)              (None, 32, 32, 256)  131072      activation_33[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_52 (BatchNo (None, 32, 32, 256)  1024        conv2d_52[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_53 (BatchNo (None, 32, 32, 256)  1024        conv2d_53[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_52 (Activation)      (None, 32, 32, 256)  0           batch_normalization_52[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_53 (Activation)      (None, 32, 32, 256)  0           batch_normalization_53[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_20 (TFOpLa (None, 32, 32, 256)  0           tf.__operators__.add_19[0][0]    \n",
      "                                                                 activation_52[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 32, 32, 512)  0           activation_53[0][0]              \n",
      "                                                                 tf.__operators__.add_20[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_54 (Conv2D)              (None, 32, 32, 512)  262144      concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_54 (BatchNo (None, 32, 32, 512)  2048        conv2d_54[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_54 (Activation)      (None, 32, 32, 512)  0           batch_normalization_54[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_3 (ZeroPadding2D (None, 33, 33, 512)  0           activation_54[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_55 (Conv2D)              (None, 16, 16, 1024) 4718592     zero_padding2d_3[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_55 (BatchNo (None, 16, 16, 1024) 4096        conv2d_55[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_55 (Activation)      (None, 16, 16, 1024) 0           batch_normalization_55[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_56 (Conv2D)              (None, 16, 16, 512)  524288      activation_55[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_56 (BatchNo (None, 16, 16, 512)  2048        conv2d_56[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_56 (Activation)      (None, 16, 16, 512)  0           batch_normalization_56[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.max_pool2d (TFOpLambda)   (None, 16, 16, 512)  0           activation_56[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.max_pool2d_1 (TFOpLambda) (None, 16, 16, 512)  0           activation_56[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.max_pool2d_2 (TFOpLambda) (None, 16, 16, 512)  0           activation_56[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 16, 16, 2048) 0           activation_56[0][0]              \n",
      "                                                                 tf.nn.max_pool2d[0][0]           \n",
      "                                                                 tf.nn.max_pool2d_1[0][0]         \n",
      "                                                                 tf.nn.max_pool2d_2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_57 (Conv2D)              (None, 16, 16, 1024) 2097152     concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_57 (BatchNo (None, 16, 16, 1024) 4096        conv2d_57[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_57 (Activation)      (None, 16, 16, 1024) 0           batch_normalization_57[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_58 (Conv2D)              (None, 16, 16, 512)  524288      activation_57[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_58 (BatchNo (None, 16, 16, 512)  2048        conv2d_58[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_58 (Activation)      (None, 16, 16, 512)  0           batch_normalization_58[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_59 (Conv2D)              (None, 16, 16, 512)  262144      activation_58[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_59 (BatchNo (None, 16, 16, 512)  2048        conv2d_59[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_59 (Activation)      (None, 16, 16, 512)  0           batch_normalization_59[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_60 (Conv2D)              (None, 16, 16, 512)  2359296     activation_59[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_60 (BatchNo (None, 16, 16, 512)  2048        conv2d_60[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_60 (Activation)      (None, 16, 16, 512)  0           batch_normalization_60[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_61 (Conv2D)              (None, 16, 16, 512)  262144      activation_60[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_61 (BatchNo (None, 16, 16, 512)  2048        conv2d_61[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_61 (Activation)      (None, 16, 16, 512)  0           batch_normalization_61[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_62 (Conv2D)              (None, 16, 16, 512)  2359296     activation_61[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_62 (BatchNo (None, 16, 16, 512)  2048        conv2d_62[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_62 (Activation)      (None, 16, 16, 512)  0           batch_normalization_62[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_63 (Conv2D)              (None, 16, 16, 512)  262144      activation_62[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_63 (BatchNo (None, 16, 16, 512)  2048        conv2d_63[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_63 (Activation)      (None, 16, 16, 512)  0           batch_normalization_63[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_65 (Conv2D)              (None, 16, 16, 512)  524288      activation_57[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_64 (Conv2D)              (None, 16, 16, 512)  2359296     activation_63[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_65 (BatchNo (None, 16, 16, 512)  2048        conv2d_65[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_64 (BatchNo (None, 16, 16, 512)  2048        conv2d_64[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_65 (Activation)      (None, 16, 16, 512)  0           batch_normalization_65[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_64 (Activation)      (None, 16, 16, 512)  0           batch_normalization_64[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 16, 16, 1024) 0           activation_65[0][0]              \n",
      "                                                                 activation_64[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_66 (Conv2D)              (None, 16, 16, 1024) 1048576     concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_66 (BatchNo (None, 16, 16, 1024) 4096        conv2d_66[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_66 (Activation)      (None, 16, 16, 1024) 0           batch_normalization_66[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_67 (Conv2D)              (None, 16, 16, 512)  524288      activation_66[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_67 (BatchNo (None, 16, 16, 512)  2048        conv2d_67[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_67 (Activation)      (None, 16, 16, 512)  0           batch_normalization_67[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d (UpSampling2D)    (None, 32, 32, 512)  0           activation_67[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 32, 32, 1024) 0           up_sampling2d[0][0]              \n",
      "                                                                 activation_54[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_68 (Conv2D)              (None, 32, 32, 256)  262144      concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_68 (BatchNo (None, 32, 32, 256)  1024        conv2d_68[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_68 (Activation)      (None, 32, 32, 256)  0           batch_normalization_68[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_69 (Conv2D)              (None, 32, 32, 256)  65536       activation_68[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_69 (BatchNo (None, 32, 32, 256)  1024        conv2d_69[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_69 (Activation)      (None, 32, 32, 256)  0           batch_normalization_69[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_70 (Conv2D)              (None, 32, 32, 256)  589824      activation_69[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_70 (BatchNo (None, 32, 32, 256)  1024        conv2d_70[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_70 (Activation)      (None, 32, 32, 256)  0           batch_normalization_70[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_71 (Conv2D)              (None, 32, 32, 256)  65536       activation_70[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_71 (BatchNo (None, 32, 32, 256)  1024        conv2d_71[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_71 (Activation)      (None, 32, 32, 256)  0           batch_normalization_71[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_72 (Conv2D)              (None, 32, 32, 256)  589824      activation_71[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_72 (BatchNo (None, 32, 32, 256)  1024        conv2d_72[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_72 (Activation)      (None, 32, 32, 256)  0           batch_normalization_72[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_73 (Conv2D)              (None, 32, 32, 256)  65536       activation_72[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_73 (BatchNo (None, 32, 32, 256)  1024        conv2d_73[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_73 (Activation)      (None, 32, 32, 256)  0           batch_normalization_73[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_75 (Conv2D)              (None, 32, 32, 256)  262144      concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_74 (Conv2D)              (None, 32, 32, 256)  589824      activation_73[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_75 (BatchNo (None, 32, 32, 256)  1024        conv2d_75[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_74 (BatchNo (None, 32, 32, 256)  1024        conv2d_74[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_75 (Activation)      (None, 32, 32, 256)  0           batch_normalization_75[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_74 (Activation)      (None, 32, 32, 256)  0           batch_normalization_74[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 32, 32, 512)  0           activation_75[0][0]              \n",
      "                                                                 activation_74[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_76 (Conv2D)              (None, 32, 32, 512)  262144      concatenate_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_76 (BatchNo (None, 32, 32, 512)  2048        conv2d_76[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_76 (Activation)      (None, 32, 32, 512)  0           batch_normalization_76[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_77 (Conv2D)              (None, 32, 32, 256)  131072      activation_76[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_77 (BatchNo (None, 32, 32, 256)  1024        conv2d_77[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_77 (Activation)      (None, 32, 32, 256)  0           batch_normalization_77[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_1 (UpSampling2D)  (None, 64, 64, 256)  0           activation_77[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)     (None, 64, 64, 512)  0           up_sampling2d_1[0][0]            \n",
      "                                                                 activation_32[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_78 (Conv2D)              (None, 64, 64, 128)  65536       concatenate_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_78 (BatchNo (None, 64, 64, 128)  512         conv2d_78[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_78 (Activation)      (None, 64, 64, 128)  0           batch_normalization_78[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_79 (Conv2D)              (None, 64, 64, 128)  16384       activation_78[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_79 (BatchNo (None, 64, 64, 128)  512         conv2d_79[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_79 (Activation)      (None, 64, 64, 128)  0           batch_normalization_79[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_80 (Conv2D)              (None, 64, 64, 128)  147456      activation_79[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_80 (BatchNo (None, 64, 64, 128)  512         conv2d_80[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_80 (Activation)      (None, 64, 64, 128)  0           batch_normalization_80[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_81 (Conv2D)              (None, 64, 64, 128)  16384       activation_80[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_81 (BatchNo (None, 64, 64, 128)  512         conv2d_81[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_81 (Activation)      (None, 64, 64, 128)  0           batch_normalization_81[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_82 (Conv2D)              (None, 64, 64, 128)  147456      activation_81[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_82 (BatchNo (None, 64, 64, 128)  512         conv2d_82[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_82 (Activation)      (None, 64, 64, 128)  0           batch_normalization_82[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_83 (Conv2D)              (None, 64, 64, 128)  16384       activation_82[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_83 (BatchNo (None, 64, 64, 128)  512         conv2d_83[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_83 (Activation)      (None, 64, 64, 128)  0           batch_normalization_83[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_85 (Conv2D)              (None, 64, 64, 128)  65536       concatenate_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_84 (Conv2D)              (None, 64, 64, 128)  147456      activation_83[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_85 (BatchNo (None, 64, 64, 128)  512         conv2d_85[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_84 (BatchNo (None, 64, 64, 128)  512         conv2d_84[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_85 (Activation)      (None, 64, 64, 128)  0           batch_normalization_85[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_84 (Activation)      (None, 64, 64, 128)  0           batch_normalization_84[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_8 (Concatenate)     (None, 64, 64, 256)  0           activation_85[0][0]              \n",
      "                                                                 activation_84[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_86 (Conv2D)              (None, 64, 64, 256)  65536       concatenate_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_86 (BatchNo (None, 64, 64, 256)  1024        conv2d_86[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_86 (Activation)      (None, 64, 64, 256)  0           batch_normalization_86[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_4 (ZeroPadding2D (None, 65, 65, 256)  0           activation_86[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_87 (Conv2D)              (None, 32, 32, 256)  589824      zero_padding2d_4[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_87 (BatchNo (None, 32, 32, 256)  1024        conv2d_87[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_87 (Activation)      (None, 32, 32, 256)  0           batch_normalization_87[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_9 (Concatenate)     (None, 32, 32, 512)  0           activation_87[0][0]              \n",
      "                                                                 activation_77[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_88 (Conv2D)              (None, 32, 32, 256)  131072      concatenate_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_88 (BatchNo (None, 32, 32, 256)  1024        conv2d_88[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_88 (Activation)      (None, 32, 32, 256)  0           batch_normalization_88[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_89 (Conv2D)              (None, 32, 32, 256)  65536       activation_88[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_89 (BatchNo (None, 32, 32, 256)  1024        conv2d_89[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_89 (Activation)      (None, 32, 32, 256)  0           batch_normalization_89[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_90 (Conv2D)              (None, 32, 32, 256)  589824      activation_89[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_90 (BatchNo (None, 32, 32, 256)  1024        conv2d_90[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_90 (Activation)      (None, 32, 32, 256)  0           batch_normalization_90[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_91 (Conv2D)              (None, 32, 32, 256)  65536       activation_90[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_91 (BatchNo (None, 32, 32, 256)  1024        conv2d_91[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_91 (Activation)      (None, 32, 32, 256)  0           batch_normalization_91[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_92 (Conv2D)              (None, 32, 32, 256)  589824      activation_91[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_92 (BatchNo (None, 32, 32, 256)  1024        conv2d_92[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_92 (Activation)      (None, 32, 32, 256)  0           batch_normalization_92[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_93 (Conv2D)              (None, 32, 32, 256)  65536       activation_92[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_93 (BatchNo (None, 32, 32, 256)  1024        conv2d_93[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_93 (Activation)      (None, 32, 32, 256)  0           batch_normalization_93[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_95 (Conv2D)              (None, 32, 32, 256)  131072      concatenate_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_94 (Conv2D)              (None, 32, 32, 256)  589824      activation_93[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_95 (BatchNo (None, 32, 32, 256)  1024        conv2d_95[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_94 (BatchNo (None, 32, 32, 256)  1024        conv2d_94[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_95 (Activation)      (None, 32, 32, 256)  0           batch_normalization_95[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_94 (Activation)      (None, 32, 32, 256)  0           batch_normalization_94[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_10 (Concatenate)    (None, 32, 32, 512)  0           activation_95[0][0]              \n",
      "                                                                 activation_94[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_96 (Conv2D)              (None, 32, 32, 512)  262144      concatenate_10[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_96 (BatchNo (None, 32, 32, 512)  2048        conv2d_96[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_96 (Activation)      (None, 32, 32, 512)  0           batch_normalization_96[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_5 (ZeroPadding2D (None, 33, 33, 512)  0           activation_96[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_97 (Conv2D)              (None, 16, 16, 512)  2359296     zero_padding2d_5[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_97 (BatchNo (None, 16, 16, 512)  2048        conv2d_97[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_97 (Activation)      (None, 16, 16, 512)  0           batch_normalization_97[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_11 (Concatenate)    (None, 16, 16, 1024) 0           activation_97[0][0]              \n",
      "                                                                 activation_67[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_98 (Conv2D)              (None, 16, 16, 512)  524288      concatenate_11[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_98 (BatchNo (None, 16, 16, 512)  2048        conv2d_98[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_98 (Activation)      (None, 16, 16, 512)  0           batch_normalization_98[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_99 (Conv2D)              (None, 16, 16, 512)  262144      activation_98[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_99 (BatchNo (None, 16, 16, 512)  2048        conv2d_99[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_99 (Activation)      (None, 16, 16, 512)  0           batch_normalization_99[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_100 (Conv2D)             (None, 16, 16, 512)  2359296     activation_99[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_100 (BatchN (None, 16, 16, 512)  2048        conv2d_100[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_100 (Activation)     (None, 16, 16, 512)  0           batch_normalization_100[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_101 (Conv2D)             (None, 16, 16, 512)  262144      activation_100[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_101 (BatchN (None, 16, 16, 512)  2048        conv2d_101[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_101 (Activation)     (None, 16, 16, 512)  0           batch_normalization_101[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_102 (Conv2D)             (None, 16, 16, 512)  2359296     activation_101[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_102 (BatchN (None, 16, 16, 512)  2048        conv2d_102[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_102 (Activation)     (None, 16, 16, 512)  0           batch_normalization_102[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_103 (Conv2D)             (None, 16, 16, 512)  262144      activation_102[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_103 (BatchN (None, 16, 16, 512)  2048        conv2d_103[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_103 (Activation)     (None, 16, 16, 512)  0           batch_normalization_103[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_105 (Conv2D)             (None, 16, 16, 512)  524288      concatenate_11[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_104 (Conv2D)             (None, 16, 16, 512)  2359296     activation_103[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_105 (BatchN (None, 16, 16, 512)  2048        conv2d_105[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_104 (BatchN (None, 16, 16, 512)  2048        conv2d_104[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_105 (Activation)     (None, 16, 16, 512)  0           batch_normalization_105[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_104 (Activation)     (None, 16, 16, 512)  0           batch_normalization_104[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_12 (Concatenate)    (None, 16, 16, 1024) 0           activation_105[0][0]             \n",
      "                                                                 activation_104[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_106 (Conv2D)             (None, 16, 16, 1024) 1048576     concatenate_12[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_106 (BatchN (None, 16, 16, 1024) 4096        conv2d_106[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_106 (Activation)     (None, 16, 16, 1024) 0           batch_normalization_106[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "p5_4 (Conv2D)                   (None, 16, 16, 27)   27675       activation_106[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "p4_4 (Conv2D)                   (None, 32, 32, 27)   13851       activation_96[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "p3_4 (Conv2D)                   (None, 64, 64, 27)   6939        activation_86[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 46,729,809\n",
      "Trainable params: 46,668,241\n",
      "Non-trainable params: 61,568\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "executionInfo": {
     "elapsed": 34,
     "status": "ok",
     "timestamp": 1638305359523,
     "user": {
      "displayName": "Jill Naiman",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "13328008118965652761"
     },
     "user_tz": 360
    },
    "id": "6tXmyitL7jy2"
   },
   "outputs": [],
   "source": [
    "# plot if you wanna\n",
    "#tf.keras.utils.plot_model(model_v5, \"yolo_v5.png\", show_shapes=True, show_layer_names=True, expand_nested=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AS4ZcPQg7jy2"
   },
   "source": [
    "For optimizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "executionInfo": {
     "elapsed": 34,
     "status": "ok",
     "timestamp": 1638305359523,
     "user": {
      "displayName": "Jill Naiman",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "13328008118965652761"
     },
     "user_tz": 360
    },
    "id": "IWLTDSz77jy2"
   },
   "outputs": [],
   "source": [
    "#LRrate = 0.004\n",
    "LRrate = 0.002\n",
    "\n",
    "class CosineLR(tf.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self,steps):\n",
    "        super().__init__()\n",
    "        self.lr = LRrate * batch_size / 64\n",
    "        self.warmup_init = LRrate/10.\n",
    "        self.warmup_step = steps\n",
    "        self.decay_steps = tf.cast((num_epochs - 1) * self.warmup_step, tf.float32)\n",
    "\n",
    "    def __call__(self, step):\n",
    "        linear_warmup = tf.cast(step, dtype=tf.float32) / self.warmup_step * (self.lr - self.warmup_init)\n",
    "        cosine_lr = 0.5 * self.lr * (1 + tf.cos(math.pi * tf.cast(step, tf.float32) / self.decay_steps))\n",
    "        return tf.where(step < self.warmup_step, self.warmup_init + linear_warmup, cosine_lr)\n",
    "\n",
    "    def get_config(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "executionInfo": {
     "elapsed": 34,
     "status": "ok",
     "timestamp": 1638305359524,
     "user": {
      "displayName": "Jill Naiman",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "13328008118965652761"
     },
     "user_tz": 360
    },
    "id": "JpreDZDZvvaW"
   },
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(CosineLR(steps_training), 0.937)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "executionInfo": {
     "elapsed": 34,
     "status": "ok",
     "timestamp": 1638305359524,
     "user": {
      "displayName": "Jill Naiman",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "13328008118965652761"
     },
     "user_tz": 360
    },
    "id": "_l3BLK6XDRms"
   },
   "outputs": [],
   "source": [
    "# load weights if you wanna\n",
    "if saved_weights_file is not None:\n",
    "    weightsFiles = glob.glob(weightsDir + 'weights/' + '*h5')\n",
    "    # OR\n",
    "    if saved_weights_file is not None:\n",
    "      weightsFiles = [classDirMain+saved_weights_file]\n",
    "    model.load_weights(weightsFiles[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 34,
     "status": "ok",
     "timestamp": 1638305359524,
     "user": {
      "displayName": "Jill Naiman",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "13328008118965652761"
     },
     "user_tz": 360
    },
    "id": "3MAgw3DHD_nA",
    "outputId": "df181c03-e403-4537-e6fc-05fc98946489"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0003125"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print(weightsFiles)\n",
    "optimizer.learning_rate.lr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e6Y0CbKb0T76"
   },
   "source": [
    "For saving checkpoints:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1638305359525,
     "user": {
      "displayName": "Jill Naiman",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "13328008118965652761"
     },
     "user_tz": 360
    },
    "id": "REcMXF7W0T76"
   },
   "outputs": [],
   "source": [
    "# for saving model\n",
    "#save_model_name = chksDir + 'checkpoints/'+'model' + str(DATE.today().year).zfill(4) + str(DATE.today().month).zfill(2) + str(DATE.today().day).zfill(2) +'.h5'\n",
    "#checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(save_model_name, save_best_only=True)\n",
    "# today = str(DATE.today().year).zfill(4) + str(DATE.today().month).zfill(2) + str(DATE.today().day).zfill(2)\n",
    "# if not os.path.exists(chksDir + 'checkpoints/'+today):\n",
    "#     os.mkdir(chksDir + 'checkpoints/'+today)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1638305359525,
     "user": {
      "displayName": "Jill Naiman",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "13328008118965652761"
     },
     "user_tz": 360
    },
    "id": "h2o2DlWC0T77"
   },
   "outputs": [],
   "source": [
    "#model2 = tf.saved_model.load(save_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1638305359526,
     "user": {
      "displayName": "Jill Naiman",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "13328008118965652761"
     },
     "user_tz": 360
    },
    "id": "gBbSfeKP0T77"
   },
   "outputs": [],
   "source": [
    "# if restart_from_checkpoints:\n",
    "#     if saved_model_file is None:\n",
    "#         files = glob.glob(chksDir + 'checkpoints/*')\n",
    "#         files.sort()\n",
    "#         model = tf.saved_model.load(files[-1])\n",
    "#         fname = files[-1]\n",
    "#     else:\n",
    "#         model = tf.saved_model.load(chksDir + 'checkpoints/' +saved_model_file)\n",
    "#         fname = chksDir + 'checkpoints/' +saved_model_file\n",
    "#     print('Loading model from', fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RKk7HxgC7jy3"
   },
   "source": [
    "# For processing data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1638305359526,
     "user": {
      "displayName": "Jill Naiman",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "13328008118965652761"
     },
     "user_tz": 360
    },
    "id": "KFOuGj5l0T77",
    "outputId": "0060855f-3b00-47a4-b3bb-9f6eb4786ff2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/jillnaiman/MegaYolo/'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classDirMain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1638305359526,
     "user": {
      "displayName": "Jill Naiman",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "13328008118965652761"
     },
     "user_tz": 360
    },
    "id": "qqdxpzdoBb6H",
    "outputId": "04774847-7c14-4b6f-a99f-902d04403e71"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/Users/jillnaiman/MegaYolo/binaries_model8/',\n",
       " '/Users/jillnaiman/MegaYolo/yolo_512x512_ann/')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classDir_main_to_imgs, classDir_main_to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1638305359527,
     "user": {
      "displayName": "Jill Naiman",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "13328008118965652761"
     },
     "user_tz": 360
    },
    "id": "-L428Dy7VxBV"
   },
   "outputs": [],
   "source": [
    "#import mega_yolo_utils\n",
    "#reload(mega_yolo_utils)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1638305359527,
     "user": {
      "displayName": "Jill Naiman",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "13328008118965652761"
     },
     "user_tz": 360
    },
    "id": "RiM8WUcKGrFn"
   },
   "outputs": [],
   "source": [
    "from mega_yolo_utils import augmentation_generator, csv_gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1638305360505,
     "user": {
      "displayName": "Jill Naiman",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "13328008118965652761"
     },
     "user_tz": 360
    },
    "id": "ewFdU9-77jy3"
   },
   "outputs": [],
   "source": [
    "splitsDirIn = None\n",
    "if onGoogle: splitsDirIn = classDirMain\n",
    "train_gen_csv = csv_gen('train', splitsDir=splitsDirIn)\n",
    "valid_gen_csv = csv_gen('valid',splitsDir=splitsDirIn)\n",
    "test_gen_csv = csv_gen('test',splitsDir=splitsDirIn)\n",
    "\n",
    "\n",
    "def dataset_gen(split, batch_size):\n",
    "    while True:\n",
    "        true_boxes = []; imgs = []; files = []\n",
    "        \n",
    "        while len(files) < batch_size:\n",
    "            if type(split) == str:\n",
    "                if b'train' in bytes(split, encoding='utf8'): # NOTE, must be bytes here!!!\n",
    "                    line = next(train_gen_csv)\n",
    "                elif b'valid' in bytes(split, encoding='utf8'):\n",
    "                    line = next(valid_gen_csv)\n",
    "                elif b'test' in bytes(split, encoding='utf8'):\n",
    "                    line = next(test_gen_csv)\n",
    "            else:\n",
    "                if b'train' in split: # NOTE, must be bytes here!!!\n",
    "                    line = next(train_gen_csv)\n",
    "                elif b'valid' in split:\n",
    "                    line = next(valid_gen_csv)\n",
    "                elif b'test' in split:\n",
    "                    line = next(test_gen_csv)\n",
    "            \n",
    "            files.append(line.strip())\n",
    "\n",
    "        # parse and get full names\n",
    "        try:\n",
    "            imgs_name, bbox = parse_annotation(files, LABELS, \n",
    "                                               feature_dir=classDir_main_to_imgs,\n",
    "                                               annotation_dir=classDir_main_to)\n",
    "        except:\n",
    "            print('error parsing:', imgs_name)\n",
    "        # do a debug check\n",
    "        for im in imgs_name:\n",
    "            if '.npz' not in im:\n",
    "                print('no np file')\n",
    "                import sys; sys.exit()\n",
    "        \n",
    "        # read in and keep images -- npy files\n",
    "        for im in imgs_name:\n",
    "            b = np.load(im)['arr_0']\n",
    "            \n",
    "            ########### DEBUGGING ##########\n",
    "            #b = b[:,:,:3]\n",
    "            ################################\n",
    "            \n",
    "            # convert 0-1\n",
    "            b = b/255.0\n",
    "            imgs.append(b)\n",
    "        \n",
    "        # finally, format for output\n",
    "        y_true1, y_true2, y_true3 = [],[],[]\n",
    "        for b in bbox:\n",
    "            y1,y2,y3= process_box(b[:,:4], b[:,4].astype('int'),anchors,CLASS)\n",
    "            y_true1.append(y1); y_true2.append(y2); y_true3.append(y3)\n",
    "        # if there is no box, do something different\n",
    "        if len(bbox) == 0:\n",
    "            # fake a box\n",
    "            b = np.array([[111.,  59., 403., 364. ,  4.]])\n",
    "            y1,y2,y3= process_box(b[:,:4], b[:,4].astype('int'),anchors,CLASS)\n",
    "            y1[:] = 0; y2[:]=0;y3[:]=0\n",
    "            y_true1.append(y1); y_true2.append(y2); y_true3.append(y3)        \n",
    "        img = tf.cast(np.array(imgs), tf.float32)        \n",
    "        yield img, tf.cast(y_true1, tf.float32), tf.cast(y_true2, tf.float32), tf.cast(y_true3, tf.float32)\n",
    "        \n",
    "\n",
    "def dataset_gen_for_aug(split, batch_size): # for training/validation datasets\n",
    "    while True:\n",
    "        true_boxes = []; imgs = []; files = []\n",
    "        \n",
    "        while len(files) < batch_size:\n",
    "            if type(split) == str:\n",
    "                if b'train' in bytes(split, encoding='utf8'): # NOTE, must be bytes here!!!\n",
    "                    line = next(train_gen_csv)\n",
    "                elif b'valid' in bytes(split, encoding='utf8'):\n",
    "                    line = next(valid_gen_csv)\n",
    "                elif b'test' in bytes(split, encoding='utf8'):\n",
    "                    line = next(test_gen_csv)\n",
    "            else:\n",
    "                if b'train' in split: # NOTE, must be bytes here!!!\n",
    "                    line = next(train_gen_csv)\n",
    "                elif b'valid' in split:\n",
    "                    line = next(valid_gen_csv)\n",
    "                elif b'test' in split:\n",
    "                    line = next(test_gen_csv)\n",
    "            \n",
    "            files.append(line.strip())\n",
    "\n",
    "        # parse and get full names\n",
    "        imgs_name, bbox = parse_annotation(files, LABELS, \n",
    "                                           feature_dir=classDir_main_to_imgs,\n",
    "                                            annotation_dir=classDir_main_to)\n",
    "        # do a debug check\n",
    "        for im in imgs_name:\n",
    "            if '.npz' not in im:\n",
    "                print('no np file')\n",
    "                import sys; sys.exit()\n",
    "        \n",
    "        # read in and keep images -- npy files\n",
    "        for im in imgs_name:\n",
    "            b = np.load(im)['arr_0']\n",
    "            # convert 0-1\n",
    "            b = b/255.0\n",
    "            imgs.append(b)\n",
    "                \n",
    "        img = tf.cast(np.array(imgs), tf.float32)        \n",
    "        yield img, tf.cast(bbox, tf.float32)\n",
    "        \n",
    "\n",
    "def get_dataset(split, labels, batch_size, use_aug=True):\n",
    "    if use_aug and ('test' not in split.lower()):\n",
    "        dataset = tf.data.Dataset.from_generator(dataset_gen_for_aug, args=[split, batch_size],\n",
    "                                                 output_types = (tf.float32, tf.float32))\n",
    "    else:\n",
    "        dataset = tf.data.Dataset.from_generator(dataset_gen, args=[split, batch_size],\n",
    "                                             output_types = (tf.float32, tf.float32, \n",
    "                                                             tf.float32, tf.float32))\n",
    "                                             \n",
    "    #dataset = dataset.prefetch(10)\n",
    "    dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
    "    \n",
    "    # maybe?\n",
    "    iterator = iter(dataset)\n",
    "\n",
    "    #return dataset\n",
    "    return iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitsDirIn = None\n",
    "if onGoogle: splitsDirIn = classDirMain\n",
    "train_gen_csv = csv_gen('train', splitsDir=splitsDirIn)\n",
    "valid_gen_csv = csv_gen('valid',splitsDir=splitsDirIn)\n",
    "test_gen_csv = csv_gen('test',splitsDir=splitsDirIn)\n",
    "\n",
    "class ArtificialDatasetNonAug(tf.data.Dataset):\n",
    "\n",
    "    #@tf.function\n",
    "    def _dataset_gen(split, batch_size):\n",
    "        while True:\n",
    "            true_boxes = []; imgs = []; files = []\n",
    "\n",
    "            while len(files) < batch_size:\n",
    "                if type(split) == str:\n",
    "                    if b'train' in bytes(split, encoding='utf8'): # NOTE, must be bytes here!!!\n",
    "                        line = next(train_gen_csv)\n",
    "                    elif b'valid' in bytes(split, encoding='utf8'):\n",
    "                        line = next(valid_gen_csv)\n",
    "                    elif b'test' in bytes(split, encoding='utf8'):\n",
    "                        line = next(test_gen_csv)\n",
    "                else:\n",
    "                    if b'train' in split: # NOTE, must be bytes here!!!\n",
    "                        line = next(train_gen_csv)\n",
    "                    elif b'valid' in split:\n",
    "                        line = next(valid_gen_csv)\n",
    "                    elif b'test' in split:\n",
    "                        line = next(test_gen_csv)\n",
    "\n",
    "                files.append(line.strip())\n",
    "\n",
    "            # parse and get full names\n",
    "            try:\n",
    "                imgs_name, bbox = parse_annotation(files, LABELS, \n",
    "                                                   feature_dir=classDir_main_to_imgs,\n",
    "                                                   annotation_dir=classDir_main_to)\n",
    "            except:\n",
    "                print('error parsing:', imgs_name)\n",
    "            # do a debug check\n",
    "            for im in imgs_name:\n",
    "                if '.npz' not in im:\n",
    "                    print('no np file')\n",
    "                    import sys; sys.exit()\n",
    "\n",
    "            # read in and keep images -- npy files\n",
    "            for im in imgs_name:\n",
    "                b = np.load(im)['arr_0']\n",
    "\n",
    "                ########### DEBUGGING ##########\n",
    "                #b = b[:,:,:3]\n",
    "                ################################\n",
    "\n",
    "                # convert 0-1\n",
    "                b = b/255.0\n",
    "                imgs.append(b)\n",
    "\n",
    "            # finally, format for output\n",
    "            y_true1, y_true2, y_true3 = [],[],[]\n",
    "            for b in bbox:\n",
    "                y1,y2,y3= process_box(b[:,:4], b[:,4].astype('int'),anchors,CLASS)\n",
    "                y_true1.append(y1); y_true2.append(y2); y_true3.append(y3)\n",
    "            # if there is no box, do something different\n",
    "            if len(bbox) == 0:\n",
    "                # fake a box\n",
    "                b = np.array([[111.,  59., 403., 364. ,  4.]])\n",
    "                y1,y2,y3= process_box(b[:,:4], b[:,4].astype('int'),anchors,CLASS)\n",
    "                y1[:] = 0; y2[:]=0;y3[:]=0\n",
    "                y_true1.append(y1); y_true2.append(y2); y_true3.append(y3)        \n",
    "            img = tf.cast(np.array(imgs), tf.float32)        \n",
    "            yield img, tf.cast(y_true1, tf.float32), tf.cast(y_true2, tf.float32), tf.cast(y_true3, tf.float32)\n",
    "\n",
    "\n",
    "\n",
    "    def __new__(cls,batch_size=32,split='valid'):\n",
    "        dataset = tf.data.Dataset.from_generator(cls._dataset_gen, args=[split, batch_size],\n",
    "                                             output_types = (tf.float32, tf.float32, \n",
    "                                                             tf.float32, tf.float32))\n",
    "        return dataset\n",
    "\n",
    "#         #dataset = dataset.prefetch(10)\n",
    "#         dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "#         # maybe?\n",
    "#         iterator = iter(dataset)\n",
    "\n",
    "#         #return dataset\n",
    "#         return iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1638305360505,
     "user": {
      "displayName": "Jill Naiman",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "13328008118965652761"
     },
     "user_tz": 360
    },
    "id": "iCHTV-qu7jy4"
   },
   "outputs": [],
   "source": [
    "# # grab data!\n",
    "# train_dataset = None\n",
    "# # train_dataset= get_dataset('train', LABELS, TRAIN_BATCH_SIZE)\n",
    "\n",
    "# val_dataset = None\n",
    "# val_dataset= get_dataset('valid', LABELS,VAL_BATCH_SIZE,use_aug=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ArtificialDatasetNonAug(batch_size=32,split='valid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# benchmark tests\n",
    "import time\n",
    "t1 = time.perf_counter()\n",
    "print(t1)\n",
    "@tf.function\n",
    "def benchmark(dataset, num_epochs=1, nbatch=32):\n",
    "    start_time = time.perf_counter()\n",
    "    for epoch_num in range(num_epochs):\n",
    "        ienum=0\n",
    "        for sample in dataset:\n",
    "            if ienum%10==0: print(ienum)\n",
    "            # Performing a training step\n",
    "            time.sleep(0.01)\n",
    "            ienum+=1\n",
    "            if ienum>nbatch: break\n",
    "    print(\"Execution time:\", time.perf_counter() - start_time, time.perf_counter())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"args_1:0\", shape=(), dtype=int32)\n",
      "Execution time: 0.04838154900016889 2586.00342969\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/t5/9xgccmv92hnfvjwd62mk8zqh0000gn/T/ipykernel_9839/3746369835.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbenchmark\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mArtificialDatasetNonAug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'valid'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# new paradigm, no prefetch 95.88\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/Paper1/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/Paper1/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    893\u001b[0m       \u001b[0;31m# If we did not create any variables the trace we have is good enough.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       return self._concrete_stateful_fn._call_flat(\n\u001b[0;32m--> 895\u001b[0;31m           filtered_flat_args, self._concrete_stateful_fn.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    896\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfn_with_cond\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minner_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minner_kwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minner_filtered_flat_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/Paper1/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1917\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1918\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1919\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1921\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/Paper1/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    558\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 560\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    561\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m~/anaconda3/envs/Paper1/lib/python3.7/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "benchmark(ArtificialDatasetNonAug(batch_size=32,split='valid')) # new paradigm, no prefetch 95.88"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "Execution time: 99.58878045300003\n"
     ]
    }
   ],
   "source": [
    "benchmark(ArtificialDatasetNonAug(batch_size=32,split='valid').prefetch(tf.data.AUTOTUNE)) # new paradigm, with prefetch, 99.58, but maybe better on other systems?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /var/folders/t5/9xgccmv92hnfvjwd62mk8zqh0000gn/T/ipykernel_9839/425625053.py:76: calling DatasetV2.from_generator (from tensorflow.python.data.ops.dataset_ops) with output_types is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use output_signature instead\n",
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "Execution time: 97.36569318700003\n"
     ]
    }
   ],
   "source": [
    "benchmark(\n",
    "    tf.data.Dataset.range(2)\n",
    "    .interleave(lambda _: ArtificialDatasetNonAug(batch_size=32,split='valid'))\n",
    ") # 97.365"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "Execution time: 73.844264458\n"
     ]
    }
   ],
   "source": [
    "benchmark(\n",
    "    tf.data.Dataset.range(2)\n",
    "    .interleave(\n",
    "        lambda _: ArtificialDatasetNonAug(batch_size=32,split='valid'),\n",
    "        num_parallel_calls=tf.data.AUTOTUNE\n",
    "    )\n",
    ") # 73.844"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "Execution time: 85.55944432500019\n"
     ]
    }
   ],
   "source": [
    "benchmark(\n",
    "    tf.data.Dataset.range(2)\n",
    "    .interleave(\n",
    "        lambda _: ArtificialDatasetNonAug(batch_size=32,split='valid'),\n",
    "        num_parallel_calls=tf.data.AUTOTUNE\n",
    "    ).shuffle(3)\n",
    ") # 85.59"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time: 27.734012822000068\n"
     ]
    }
   ],
   "source": [
    "benchmark(val_dataset) \n",
    "# naive approach gives: 19.24 sec\n",
    "# with prefetch(tf.data.AUTOTUNE): 27.73 sec (maybe its better with different systems?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "`map_func` must return a `Dataset` object. Got <class 'tensorflow.python.data.ops.iterator_ops.IteratorSpec'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/t5/9xgccmv92hnfvjwd62mk8zqh0000gn/T/ipykernel_9839/2554311210.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m     .interleave(\n\u001b[1;32m      4\u001b[0m         \u001b[0;32mlambda\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mval_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0mnum_parallel_calls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAUTOTUNE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     )\n\u001b[1;32m      7\u001b[0m )\n",
      "\u001b[0;32m~/anaconda3/envs/Paper1/lib/python3.7/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36minterleave\u001b[0;34m(self, map_func, cycle_length, block_length, num_parallel_calls, deterministic)\u001b[0m\n\u001b[1;32m   1947\u001b[0m           \u001b[0mblock_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1948\u001b[0m           \u001b[0mnum_parallel_calls\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1949\u001b[0;31m           deterministic=deterministic)\n\u001b[0m\u001b[1;32m   1950\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1951\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/Paper1/lib/python3.7/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, input_dataset, map_func, cycle_length, block_length, num_parallel_calls, buffer_output_elements, prefetch_input_elements, deterministic)\u001b[0m\n\u001b[1;32m   4365\u001b[0m       raise TypeError(\n\u001b[1;32m   4366\u001b[0m           \"`map_func` must return a `Dataset` object. Got {}\".format(\n\u001b[0;32m-> 4367\u001b[0;31m               type(self._map_func.output_structure)))\n\u001b[0m\u001b[1;32m   4368\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_structure\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_map_func\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_structure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_element_spec\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4369\u001b[0m     self._cycle_length = ops.convert_to_tensor(\n",
      "\u001b[0;31mTypeError\u001b[0m: `map_func` must return a `Dataset` object. Got <class 'tensorflow.python.data.ops.iterator_ops.IteratorSpec'>"
     ]
    }
   ],
   "source": [
    "benchmark(\n",
    "    tf.data.Dataset.range(2)\n",
    "    .interleave(\n",
    "        lambda _: val_dataset,\n",
    "        num_parallel_calls=tf.data.AUTOTUNE\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1638305360506,
     "user": {
      "displayName": "Jill Naiman",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "13328008118965652761"
     },
     "user_tz": 360
    },
    "id": "lsQQuI7dzZJw"
   },
   "outputs": [],
   "source": [
    "#next(valid_gen_csv)\n",
    "#next(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1638305360506,
     "user": {
      "displayName": "Jill Naiman",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "13328008118965652761"
     },
     "user_tz": 360
    },
    "id": "OfxcXCqPyXZj"
   },
   "outputs": [],
   "source": [
    "#val_dataset= get_dataset('valid', LABELS, VAL_BATCH_SIZE,use_aug=True)\n",
    "#next(val_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uvgdY6ly7jy4"
   },
   "source": [
    "Including Augmentation like a boss!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1638305360506,
     "user": {
      "displayName": "Jill Naiman",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "13328008118965652761"
     },
     "user_tz": 360
    },
    "id": "nm5833D_7jy5"
   },
   "outputs": [],
   "source": [
    "aug_train_dataset = augmentation_generator(train_dataset, anchors, CLASS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b5doQu9r7jy5"
   },
   "source": [
    "For calculating the loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1638305360507,
     "user": {
      "displayName": "Jill Naiman",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "13328008118965652761"
     },
     "user_tz": 360
    },
    "id": "oWeoMEdE7jy5"
   },
   "outputs": [],
   "source": [
    "class ComputeLoss(object):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    @staticmethod\n",
    "    def compute_loss(y_pred, y_true, anchors):\n",
    "        grid_size = tf.shape(y_pred)[1:3]\n",
    "        ratio = tf.cast(tf.constant([image_size, image_size]) / grid_size, tf.float32)\n",
    "        batch_size = tf.cast(tf.shape(y_pred)[0], tf.float32)\n",
    "\n",
    "        x_y_offset, pred_boxes, pred_conf, pred_prob = process_layer(y_pred, anchors,CLASS)\n",
    "\n",
    "        object_mask = y_true[..., 4:5]\n",
    "\n",
    "        def cond(idx, _):\n",
    "            return tf.less(idx, tf.cast(batch_size, tf.int32))\n",
    "\n",
    "        def body(idx, mask):\n",
    "            valid_true_boxes = tf.boolean_mask(y_true[idx, ..., 0:4],\n",
    "                                               tf.cast(object_mask[idx, ..., 0], 'bool'))\n",
    "            iou = box_iou(pred_boxes[idx], valid_true_boxes)\n",
    "            return idx + 1, mask.write(idx, tf.cast(tf.reduce_max(iou, axis=-1) < 0.2, tf.float32))\n",
    "\n",
    "        ignore_mask = tf.TensorArray(dtype=tf.float32, size=0, dynamic_size=True)\n",
    "        \n",
    "        #print('here1.1')\n",
    "        #print(cond, body, ignore_mask)\n",
    "\n",
    "        _, ignore_mask = tf.while_loop(cond=cond, body=body, loop_vars=[0, ignore_mask])\n",
    "        \n",
    "        #print('here1.2')\n",
    "        ignore_mask = ignore_mask.stack()\n",
    "        ignore_mask = tf.expand_dims(ignore_mask, -1)\n",
    "\n",
    "        true_xy = y_true[..., 0:2] / ratio[::-1] - x_y_offset\n",
    "        pred_xy = pred_boxes[..., 0:2] / ratio[::-1] - x_y_offset\n",
    "\n",
    "        true_tw_th = y_true[..., 2:4] / anchors\n",
    "        pred_tw_th = pred_boxes[..., 2:4] / anchors\n",
    "        true_tw_th = tf.where(tf.equal(true_tw_th, 0), tf.ones_like(true_tw_th), true_tw_th)\n",
    "        pred_tw_th = tf.where(tf.equal(pred_tw_th, 0), tf.ones_like(pred_tw_th), pred_tw_th)\n",
    "        true_tw_th = tf.math.log(tf.clip_by_value(true_tw_th, 1e-9, 1e+9))\n",
    "        pred_tw_th = tf.math.log(tf.clip_by_value(pred_tw_th, 1e-9, 1e+9))\n",
    "\n",
    "        box_loss_scale = y_true[..., 2:3] * y_true[..., 3:4]\n",
    "        box_loss_scale = 2. - box_loss_scale / tf.cast(image_size ** 2, tf.float32)\n",
    "\n",
    "        xy_loss = tf.reduce_sum(tf.square(true_xy - pred_xy) * object_mask * box_loss_scale)\n",
    "        wh_loss = tf.reduce_sum(tf.square(true_tw_th - pred_tw_th) * object_mask * box_loss_scale)\n",
    "\n",
    "        conf_pos_mask = object_mask\n",
    "        conf_neg_mask = (1 - object_mask) * ignore_mask\n",
    "        conf_loss_pos = conf_pos_mask * tf.nn.sigmoid_cross_entropy_with_logits(labels=object_mask, logits=pred_conf)\n",
    "        conf_loss_neg = conf_neg_mask * tf.nn.sigmoid_cross_entropy_with_logits(labels=object_mask, logits=pred_conf)\n",
    "        # try this\n",
    "        #conf_loss_pos = conf_pos_mask * -tf.reduce_sum(object_mask*tf.math.log(tf.clip_by_value(pred_conf,1e-10,1.0)))\n",
    "        #conf_loss_neg = conf_neg_mask * -tf.reduce_sum(object_mask*tf.math.log(tf.clip_by_value(pred_conf,1e-10,1.0)))\n",
    "\n",
    "\n",
    "        conf_loss = tf.reduce_sum((conf_loss_pos + conf_loss_neg))\n",
    "\n",
    "        true_conf = y_true[..., 5:]\n",
    "\n",
    "        class_loss = object_mask * tf.nn.sigmoid_cross_entropy_with_logits(true_conf, pred_prob)\n",
    "        #class_loss = object_mask * -tf.reduce_sum(true_conf*tf.math.log(tf.clip_by_value(pred_conf,1e-10,1.0)))\n",
    "        #class_loss = object_mask * tf.losses.categorical_crossentropy(true_conf, pred_prob)\n",
    "        #tf.losses.sparse_softmax_cross_entropy(y, logits)\n",
    "        class_loss = tf.reduce_sum(class_loss) # sum across all -- 1 number for loss\n",
    "\n",
    "        if np.isnan(xy_loss):\n",
    "          print('xy_loss is NaN')\n",
    "        if np.isnan(wh_loss):\n",
    "          print('wh_loss is NaN')\n",
    "        if np.isnan(conf_loss):\n",
    "          print('conf_loss is NaN')#, conf_loss_pos, conf_loss_neg)\n",
    "        if np.isnan(class_loss):\n",
    "          print('class_loss is NaN')\n",
    "\n",
    "        if np.isnan(xy_loss + wh_loss + conf_loss + class_loss):\n",
    "          print('--- object mask ---')\n",
    "          print(object_mask.numpy().shape, pred_conf.numpy().shape, true_conf.numpy().shape)\n",
    "          print(object_mask.numpy().max(), object_mask.numpy().min())\n",
    "          print(object_mask)\n",
    "          print(' ')\n",
    "          print('--------')\n",
    "        #else:\n",
    "        #  print(object_mask.numpy().max(), object_mask.numpy().min())\n",
    "\n",
    "        return xy_loss + wh_loss + conf_loss + class_loss\n",
    "\n",
    "    def __call__(self, y_pred, y_true):\n",
    "        loss = 0.\n",
    "        anchor_group = [anchors[6:9], anchors[3:6], anchors[0:3]]\n",
    "\n",
    "        for i in range(len(y_pred)):\n",
    "            loss += self.compute_loss(y_pred[i], y_true[i], anchor_group[i])\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1638305360507,
     "user": {
      "displayName": "Jill Naiman",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "13328008118965652761"
     },
     "user_tz": 360
    },
    "id": "XZZsr5247jy6"
   },
   "outputs": [],
   "source": [
    "loss_object = ComputeLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1638305360507,
     "user": {
      "displayName": "Jill Naiman",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "13328008118965652761"
     },
     "user_tz": 360
    },
    "id": "9lBB1U-k0T7-"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1638305360508,
     "user": {
      "displayName": "Jill Naiman",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "13328008118965652761"
     },
     "user_tz": 360
    },
    "id": "ZbiMja467jy6"
   },
   "outputs": [],
   "source": [
    "def compute_loss(y_true, y_pred):\n",
    "    total_loss = loss_object(y_pred, y_true)\n",
    "    return tf.reduce_sum(total_loss) / batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1638305360508,
     "user": {
      "displayName": "Jill Naiman",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "13328008118965652761"
     },
     "user_tz": 360
    },
    "id": "fbZopKn57jy6"
   },
   "outputs": [],
   "source": [
    "def train_step(image, y_true):\n",
    "    with tf.GradientTape() as tape:\n",
    "        y_pred = model(image, training=True)\n",
    "        loss = compute_loss(y_true, y_pred)\n",
    "    if not np.isnan(loss):\n",
    "        variables = model.trainable_variables\n",
    "        gradients = tape.gradient(loss, variables)\n",
    "        #if np.isnan(loss):\n",
    "        #  print('nan')\n",
    "        #  print\n",
    "        optimizer.apply_gradients(zip(gradients, variables))\n",
    "    else: # this will stop if we have non-convergence \n",
    "        print('is NaN -- probably want to lower your learning rate!!!!')\n",
    "        import sys; sys.exit()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1638305360508,
     "user": {
      "displayName": "Jill Naiman",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "13328008118965652761"
     },
     "user_tz": 360
    },
    "id": "l1ftaeHUvvab"
   },
   "outputs": [],
   "source": [
    "# save weights\n",
    "def save_best_weights(model, name, val_loss_avg):\n",
    "    # delete existing weights file\n",
    "    files = glob.glob(os.path.join(weightsDir + 'weights/', name + '*'))\n",
    "    for file in files:\n",
    "        os.remove(file)\n",
    "    # create new weights file\n",
    "    name = name + '_model_' +version + str(val_loss_avg) + '.h5'\n",
    "    path_name = os.path.join(weightsDir +'weights/', name)\n",
    "    model.save_weights(path_name)\n",
    "    \n",
    "    \n",
    "# log (tensorboard)\n",
    "def log_loss(loss, val_loss, step):\n",
    "    tf.summary.scalar('loss', loss, step)\n",
    "    tf.summary.scalar('val_loss', val_loss, step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1638305360509,
     "user": {
      "displayName": "Jill Naiman",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "13328008118965652761"
     },
     "user_tz": 360
    },
    "id": "C19AoZV9loKh",
    "outputId": "e1b08851-f8e4-42ce-da75-ec9bd3dfcfce"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'./classifications/yolo_512x512_ann/'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classDir_main_to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1638305360509,
     "user": {
      "displayName": "Jill Naiman",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "13328008118965652761"
     },
     "user_tz": 360
    },
    "id": "omK8Zjfc7jy6"
   },
   "outputs": [],
   "source": [
    "# training\n",
    "def train(epochs, model, train_dataset, val_dataset, steps_per_epoch_train, \n",
    "          steps_per_epoch_val, optimizer, train_name = 'train'):\n",
    "    '''\n",
    "    Train YOLO model for n epochs.\n",
    "    Eval loss on training and validation dataset.\n",
    "    Log training loss and validation loss for tensorboard.\n",
    "    Save best weights during training (according to validation loss).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    - epochs : integer, number of epochs to train the model.\n",
    "    - model : YOLO model.\n",
    "    - train_dataset : YOLO ground truth and image generator from training dataset.\n",
    "    - val_dataset : YOLO ground truth and image generator from validation dataset.\n",
    "    - steps_per_epoch_train : integer, number of batch to complete one epoch for train_dataset.\n",
    "    - steps_per_epoch_val : integer, number of batch to complete one epoch for val_dataset.\n",
    "    - train_name : string, training name used to log loss and save weights.\n",
    "    \n",
    "    Notes :\n",
    "    - train_dataset and val_dataset generate YOLO ground truth tensors : detector_mask,\n",
    "      matching_true_boxes, class_one_hot, true_boxes_grid. Shape of these tensors (batch size, tensor shape).\n",
    "    - steps per epoch = number of images in dataset // batch size of dataset\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    - loss history : [train_loss_history, val_loss_history] : list of average loss for each epoch.\n",
    "    '''\n",
    "    num_epochs1 = epochs\n",
    "    steps_per_epoch_train = steps_per_epoch_train\n",
    "    steps_per_epoch_val = steps_per_epoch_val\n",
    "    train_loss_history = []\n",
    "    val_loss_history = []\n",
    "    best_val_loss = 1e6\n",
    "    \n",
    "    # optimizer\n",
    "    #optimizer = tf.keras.optimizers.Adam(learning_rate=1e-5, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
    "    \n",
    "    # log (tensorboard)\n",
    "    summary_writer = tf.summary.create_file_writer(os.path.join(logsDir+'logs/', train_name), \n",
    "                                                   flush_millis=20000)\n",
    "    summary_writer.set_as_default()\n",
    "    \n",
    "    # training\n",
    "    for epoch in range(num_epochs1):\n",
    "        epoch_loss = []\n",
    "        epoch_val_loss = []\n",
    "        epoch_val_sub_loss = []\n",
    "        print('Epoch {} :'.format(epoch))\n",
    "        # train\n",
    "        for batch_idx in range(steps_per_epoch_train):        \n",
    "            image, y_true_1, y_true_2, y_true_3 = next(train_dataset)\n",
    "            y_true = (y_true_1, y_true_2, y_true_3)\n",
    "            loss = train_step(image, y_true)\n",
    "            # check for nans\n",
    "            optOrig = optimizer.learning_rate.lr\n",
    "            while np.isnan(loss):\n",
    "              print('loss nan')\n",
    "              optimizer.learning_rate.lr *= 0.5\n",
    "              loss = train_step(image, y_true)\n",
    "            epoch_loss.append(loss)\n",
    "            print('-', end='')\n",
    "        print(' | ', end='')\n",
    "        # val\n",
    "        for batch_idx in range(steps_per_epoch_val): \n",
    "            image, y_true_1, y_true_2, y_true_3 = next(val_dataset)\n",
    "            y_true = (y_true_1, y_true_2, y_true_3)\n",
    "            loss = train_step(image, y_true)\n",
    "            epoch_val_loss.append(loss)\n",
    "            print('-', end='')\n",
    "\n",
    "        loss_avg = np.mean(np.array(epoch_loss))\n",
    "        val_loss_avg = np.mean(np.array(epoch_val_loss))\n",
    "        train_loss_history.append(loss_avg)\n",
    "        val_loss_history.append(val_loss_avg)\n",
    "        \n",
    "        # log\n",
    "        log_loss(loss_avg, val_loss_avg, epoch)\n",
    "        \n",
    "        # save\n",
    "        if val_loss_avg < best_val_loss:\n",
    "            save_best_weights(model, train_name, val_loss_avg)\n",
    "            best_val_loss = val_loss_avg\n",
    "            #tf.saved_model.save(model, chksDir + 'checkpoints/'+today)        \n",
    "        print(' loss = {:.4f}, val_loss = {:.4f}'.format(loss_avg, val_loss_avg))\n",
    "        \n",
    "    return [train_loss_history, val_loss_history]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "l2FpED3Dvvac",
    "outputId": "53dfa0c1-8492-4858-d7c6-e9e5ae480a29"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 :\n",
      "-----------------------"
     ]
    }
   ],
   "source": [
    "results = train(num_epochs, model, aug_train_dataset, val_dataset, \n",
    "               steps_training, steps_val, optimizer,'training_1'+extraName)\n",
    "\n",
    "# debug\n",
    "#results = train(num_epochs, model, aug_train_dataset, val_dataset, \n",
    "#               1, 1, optimizer,'training_1'+extraName)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oqGPwg0R7jy6"
   },
   "source": [
    "Plot diagnostics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "68HC1xUF7jy6"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(30,14))\n",
    "ax.plot(results[0], label='Training Loss')\n",
    "ax.plot(results[1], label='Validation Loss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EjYMDlLo7jy7"
   },
   "outputs": [],
   "source": [
    "# imgs_name, bbox = parse_annotation([X_train[0]], LABELS, \n",
    "#                                    classDir_main_to_imgs=classDir_main_to_imgs,\n",
    "#                                        classDir_main_to_ann=classDir_main_to)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "theCs-467jy8"
   },
   "outputs": [],
   "source": [
    "# from importlib import reload\n",
    "# reload(parse_annotation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "orYNx10Y7jy8"
   },
   "outputs": [],
   "source": [
    "#import general_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1OXBhSDS7jy8"
   },
   "outputs": [],
   "source": [
    "#reload(general_utils)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tKhsRWNh7jy8"
   },
   "outputs": [],
   "source": [
    "#from general_utils import parse_annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8eIrQ2HH5C8Z"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wXSKvTs95Pp6"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ihigRX4m5QaB"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "mega_yolo_train.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
