{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iC4Y4O7bneag"
   },
   "source": [
    "# Mega Yolo -- train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kado6I35Dzr1"
   },
   "source": [
    "## Some toggles for if you want to re-start from weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "executionInfo": {
     "elapsed": 300,
     "status": "ok",
     "timestamp": 1638305345334,
     "user": {
      "displayName": "Jill Naiman",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "13328008118965652761"
     },
     "user_tz": 360
    },
    "id": "0bgHP_GnD2SN"
   },
   "outputs": [],
   "source": [
    "# Do you want to re-run from an already generated train/valid/test split?\n",
    "#  -- this is useful for feature testing and/or re-starting from weights\n",
    "re_run_from_splits = True\n",
    "\n",
    "# if restarting, how many previous log files do we want to look at?\n",
    "nRecent = 7 # for the 1st restart, this will be 1, for the 2nd, 2, etc\n",
    "\n",
    "# set to true if you are not re-running from the same dataset\n",
    "regenAnchors = False\n",
    " \n",
    "# use a saved weights file? Set to None if not and training will start anew\n",
    "#saved_weights_file = 'weights/savedWeights/training_1_model_l0.017813377.h5'\n",
    "saved_weights_file = None\n",
    "\n",
    "#fileStorage = 'binaries/' # binaries is where things are -- MAIN   \n",
    "#extraName = '' # append to training weights name\n",
    "\n",
    "# for feature collections\n",
    "#fileStorage = 'binaries_model1/' # binaries is where things are -- MAIN   \n",
    "#extraName = 'model1' # never use 8, this is our usual model?\n",
    "\n",
    "#fileStorage = 'binaries_model1_inverted/' # binaries is where things are -- MAIN   \n",
    "#extraName = 'model1_inverted'\n",
    "\n",
    "#fileStorage = 'binaries_model1_inverted_palletized/'\n",
    "#extraName = 'model1_inverted_palletized'\n",
    "\n",
    "# fileStorage = 'binaries_model2/'\n",
    "# extraName = 'model2'\n",
    "\n",
    "#fileStorage = 'binaries_model3/'\n",
    "#extraName = 'model3'\n",
    "\n",
    "#fileStorage = 'binaries_model4/'\n",
    "#extraName = 'model4'\n",
    "\n",
    "#fileStorage = 'binaries_model5/'\n",
    "#extraName = 'model5'\n",
    "\n",
    "#fileStorage = 'binaries_model5_maxTag125/'\n",
    "#extraName = 'model5_maxTag125'\n",
    "\n",
    "# fileStorage = 'binaries_model6/'\n",
    "# extraName = 'model6'\n",
    "\n",
    "# fileStorage = 'binaries_model8/'\n",
    "# extraName = 'model8'\n",
    "\n",
    "fileStorage = 'binaries_model8/'; method='npz'\n",
    "#fileStorage = 'binaries_model8_pickle/'; method='pickle'\n",
    "#fileStorage = 'binaries_model8_noncom/'; method = 'npy'\n",
    "#fileStorage = 'binaries_model8_noncomz/'; method = 'npz'\n",
    "\n",
    "extraName = 'model8'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "executionInfo": {
     "elapsed": 229,
     "status": "ok",
     "timestamp": 1638305346236,
     "user": {
      "displayName": "Jill Naiman",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "13328008118965652761"
     },
     "user_tz": 360
    },
    "id": "dLbPjO38y8NV"
   },
   "outputs": [],
   "source": [
    "# toggle for if on google collab or not\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1638305346557,
     "user": {
      "displayName": "Jill Naiman",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "13328008118965652761"
     },
     "user_tz": 360
    },
    "id": "gnmYH7-3neak",
    "outputId": "25237af9-ee3b-46e2-dfc8-09db00289dff"
   },
   "outputs": [],
   "source": [
    "from sys import path\n",
    "path.append('../')\n",
    "import config\n",
    "classDirMain = config.save_binary_dir #+ fileStorage\n",
    "# where are raw images?\n",
    "images_pulled_dir = config.images_jpeg_dir #'/Users/jillnaiman/Dropbox/wwt_image_extraction/FigureLocalization/Pages/RandomSingleFromPDFIndexed/' ## what about Dropbox though????\n",
    "classDirMainHOME = fileStorage \n",
    "splitsDir = config.tmp_storage_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1638305346558,
     "user": {
      "displayName": "Jill Naiman",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "13328008118965652761"
     },
     "user_tz": 360
    },
    "id": "H67BF_sLneah"
   },
   "outputs": [],
   "source": [
    "# # some parameters for different architectures of YOLO\n",
    "batch_size = 10\n",
    "num_epochs = 125 #150 #300\n",
    "\n",
    "#IMAGE_H, IMAGE_W = 512, 512\n",
    "image_size = config.IMAGE_H # assume width=height\n",
    "\n",
    "TRAIN_BATCH_SIZE = batch_size #10\n",
    "VAL_BATCH_SIZE   = batch_size #10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1638305346558,
     "user": {
      "displayName": "Jill Naiman",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "13328008118965652761"
     },
     "user_tz": 360
    },
    "id": "9GGT2mW9nean",
    "outputId": "3a140232-6fdc-44f7-e6b1-b17e841e2644"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/Users/jillnaiman/MegaYolo/yolo_512x512_ann/',\n",
       " '/Users/jillnaiman/MegaYolo/binaries_model8/')"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# where annotations and features files\n",
    "#classDir_main_to = classDirMain + 'yolo_512x512_cap_ann/'\n",
    "#classDir_main_to_imgs = classDirMain + 'binaries/'#+ 'yolo_512x512/'\n",
    "classDir_main_to = classDirMain + config.ann_name + str(int(config.IMAGE_H)) + 'x' + str(int(config.IMAGE_W))  + '_ann/'\n",
    "\n",
    "classDir_main_to_imgs = classDirMain + fileStorage.split('/')[-2] + '/'\n",
    "classDir_main_to, classDir_main_to_imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1638305346559,
     "user": {
      "displayName": "Jill Naiman",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "13328008118965652761"
     },
     "user_tz": 360
    },
    "id": "IVGN-eMPe0-c"
   },
   "outputs": [],
   "source": [
    "#!conda install numba --yes\n",
    "#logsDir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "executionInfo": {
     "elapsed": 2276,
     "status": "ok",
     "timestamp": 1638305348829,
     "user": {
      "displayName": "Jill Naiman",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "13328008118965652761"
     },
     "user_tz": 360
    },
    "id": "vwhXD8HOneap"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import re\n",
    "import numpy as np\n",
    "import cv2 as cv\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import xml.etree.cElementTree as ET\n",
    "\n",
    "# make more better?\n",
    "#from numba import jit\n",
    "from time import perf_counter\n",
    "import sys\n",
    "\n",
    "# for v5\n",
    "import math\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 414,
     "status": "ok",
     "timestamp": 1638305349239,
     "user": {
      "displayName": "Jill Naiman",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "13328008118965652761"
     },
     "user_tz": 360
    },
    "id": "TpGXEVAfnear",
    "outputId": "e5d1ec46-a9b1-4262-c794-7b98f42f0c54"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow version : 2.4.1\n",
      "GPU : []\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print('Tensorflow version : {}'.format(tf.__version__))\n",
    "print('GPU : {}'.format(tf.config.list_physical_devices('GPU')))\n",
    "from tensorflow import keras\n",
    "import tensorflow.keras.backend as K\n",
    "import imgaug as ia\n",
    "from imgaug import augmenters as iaa\n",
    "\n",
    "# my imports\n",
    "import pickle\n",
    "#from classification_utils import make_get_csv\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import json\n",
    "#from classification_utils import train_test_valid_split\n",
    "from scipy import stats\n",
    "import shutil\n",
    "\n",
    "# for restart\n",
    "from tensorflow.python.summary.summary_iterator import summary_iterator\n",
    "import struct\n",
    "from datetime import date as DATE\n",
    "\n",
    "# get parse\n",
    "from mega_yolo_utils import build_model, train_test_valid_split, \\\n",
    "    process_box, process_layer, box_iou, compute_nms, iou, num_cluster, generator, \\\n",
    "    get_n_features\n",
    "from general_utils import parse_annotation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HUJLjHVTneav"
   },
   "source": [
    "## First, data setup\n",
    "\n",
    " * Define classes\n",
    " * Grab image location info, grab boxes\n",
    " * Remap images to new size for running through YOLO\n",
    " * get 9 anchors -- why 9? because that is the code we are grabbing, that is why :P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1638305349240,
     "user": {
      "displayName": "Jill Naiman",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "13328008118965652761"
     },
     "user_tz": 360
    },
    "id": "EoCivx-Jneav",
    "outputId": "313ea5ea-2424-4907-dcdb-1516e523950c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5515"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Grab labels\n",
    "annotations = glob.glob(classDir_main_to + '*')\n",
    "len(annotations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tFWzDaooBb6A"
   },
   "source": [
    "Parse annotations -- **NOTE: this can take a while!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7058,
     "status": "ok",
     "timestamp": 1638305356292,
     "user": {
      "displayName": "Jill Naiman",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "13328008118965652761"
     },
     "user_tz": 360
    },
    "id": "N34O8BwTneay",
    "outputId": "b48f751d-1367-4f32-8dcf-202b6b625254"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "on  0  of  5515\n",
      "on  200  of  5515\n",
      "on  400  of  5515\n",
      "on  600  of  5515\n",
      "on  800  of  5515\n",
      "on  1000  of  5515\n",
      "on  1200  of  5515\n",
      "on  1400  of  5515\n",
      "on  1600  of  5515\n",
      "on  1800  of  5515\n",
      "on  2000  of  5515\n",
      "on  2200  of  5515\n",
      "on  2400  of  5515\n",
      "on  2600  of  5515\n",
      "on  2800  of  5515\n",
      "on  3000  of  5515\n",
      "on  3200  of  5515\n",
      "on  3400  of  5515\n",
      "on  3600  of  5515\n",
      "on  3800  of  5515\n",
      "on  4000  of  5515\n",
      "on  4200  of  5515\n",
      "on  4400  of  5515\n",
      "on  4600  of  5515\n",
      "on  4800  of  5515\n",
      "on  5000  of  5515\n",
      "on  5200  of  5515\n",
      "on  5400  of  5515\n",
      "    Elapsed wall clock time = 4.31602 seconds.\n"
     ]
    }
   ],
   "source": [
    "##### what about anchors -- do we want to regenerate? Generally keep this as True...\n",
    "#####regenAnchors = True\n",
    "# ... unless we are re-running from a previous split\n",
    "if re_run_from_splits: regenAnchors = False\n",
    "#if regenAnchorsAnyway: regenAnchors = True\n",
    "\n",
    "if regenAnchors:\n",
    "    bboxes = []\n",
    "    \n",
    "\n",
    "# this parsing does some loading on collab that I'm not 100% sure about, but seems necessary\n",
    "#   to load into memory, so keep it and figure it out later\n",
    "def load_parse_data_split(X_full):\n",
    "    Y_full_str = np.array([]) # have to loop and give best guesses for the pages that have multiple images/classes in them\n",
    "    slabels = np.array([])\n",
    "    for ii, X in enumerate(X_full):\n",
    "        if ii%200 == 0: print('on ', ii, ' of ', len(X_full))\n",
    "        tree = ET.parse(X)\n",
    "        tags = np.array([])\n",
    "        for elem in tree.iter(): \n",
    "            if 'object' in elem.tag or 'part' in elem.tag:                  \n",
    "                box = np.zeros((5))\n",
    "                for attr in list(elem):\n",
    "                    if 'name' in attr.tag:\n",
    "                        if attr.text is not None:\n",
    "                            tags = np.append(tags,attr.text)\n",
    "                            slabels = np.append(slabels,attr.text)\n",
    "                        #print(tags, slabels)\n",
    "                    if regenAnchors:\n",
    "                        if 'bndbox' in attr.tag and 'bndboxOrig' not in attr.tag:\n",
    "                            for dim in list(attr):\n",
    "                                if 'xmin' in dim.tag:\n",
    "                                    box[0] = int(round(float(dim.text)))\n",
    "                                if 'ymin' in dim.tag:\n",
    "                                    box[1] = int(round(float(dim.text)))\n",
    "                                if 'xmax' in dim.tag:\n",
    "                                    box[2] = int(round(float(dim.text)))\n",
    "                                if 'ymax' in dim.tag:\n",
    "                                    box[3] = int(round(float(dim.text)))\n",
    "                if regenAnchors and len(box)>0: bboxes.append(np.asarray(box))\n",
    "        if len(tags) > 0:\n",
    "            #print(tags)\n",
    "            modeClass = stats.mode(tags).mode[0] # most frequent class that pops up on this page\n",
    "            Y_full_str = np.append(Y_full_str, modeClass) # class in string\n",
    "        else:\n",
    "            Y_full_str = np.append(Y_full_str, 'none')\n",
    "    return Y_full_str,slabels\n",
    "\n",
    "# NEXT: do a quick test run-through of the data generator for train/test splits\n",
    "X_full = np.array(annotations)\n",
    "\n",
    "start_time = perf_counter( )\n",
    "Y_full_str,slabels = load_parse_data_split(X_full)\n",
    "stop_time = perf_counter( )\n",
    "print('    Elapsed wall clock time = %g seconds.' % (stop_time - start_time) )\n",
    "    \n",
    "# also do regeneration of anchors\n",
    "if regenAnchors: bboxes = np.array(bboxes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yUWutM847jyy"
   },
   "source": [
    "Get anchors, if that is what you wanna do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 25,
     "status": "ok",
     "timestamp": 1638305356293,
     "user": {
      "displayName": "Jill Naiman",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "13328008118965652761"
     },
     "user_tz": 360
    },
    "id": "Dvtr6msG7jyy",
    "outputId": "d75a4d9f-85d2-4c87-aeae-5908282da486"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from saved:\n",
      "[[195.  21.]\n",
      " [203.   7.]\n",
      " [ 51.   5.]\n",
      " [313. 199.]\n",
      " [359. 391.]\n",
      " [435.  16.]\n",
      " [399. 307.]\n",
      " [204. 114.]\n",
      " [ 15. 355.]]\n"
     ]
    }
   ],
   "source": [
    "# assume location of saved anchors:\n",
    "saveFileAnchors = classDirMain + 'weights/anchors.pickle'\n",
    "# hack for local debugging\n",
    "#if '/Users/jillnaiman' in thisDir:\n",
    "saveFileAnchors = splitsDir + 'anchors.pickle'\n",
    "\n",
    "if regenAnchors:\n",
    "    boxes = []\n",
    "    for b in bboxes:\n",
    "        boxes.append([b[2]-b[0], b[3]-b[1]])\n",
    "    boxes = np.array(boxes)\n",
    "    \n",
    "    anchors = generator(boxes,k=num_cluster)\n",
    "    print('NEW ANCHORS:')\n",
    "    \n",
    "    # save!\n",
    "    with open(saveFileAnchors, 'wb') as ff:\n",
    "        pickle.dump(anchors, ff)\n",
    "else:\n",
    "    print('from saved:')\n",
    "    with open(saveFileAnchors, 'rb') as f:\n",
    "        anchors = pickle.load(f)    \n",
    "    \n",
    "print(anchors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 21,
     "status": "ok",
     "timestamp": 1638305356293,
     "user": {
      "displayName": "Jill Naiman",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "13328008118965652761"
     },
     "user_tz": 360
    },
    "id": "Z1G0r3vYnea0",
    "outputId": "75ac0211-6d6a-4e9a-a368-3b40bd104023"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['figure', 'figure caption', 'math formula', 'table']"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LABELS = np.unique(slabels).tolist()\n",
    "CLASS = len(LABELS)\n",
    "##if use_only_one_class: CLASS = 1\n",
    "LABELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "executionInfo": {
     "elapsed": 20,
     "status": "ok",
     "timestamp": 1638305356294,
     "user": {
      "displayName": "Jill Naiman",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "13328008118965652761"
     },
     "user_tz": 360
    },
    "id": "yZJXzsfVnea2"
   },
   "outputs": [],
   "source": [
    "# strings to integers\n",
    "labels = np.arange(0,len(LABELS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 20,
     "status": "ok",
     "timestamp": 1638305356294,
     "user": {
      "displayName": "Jill Naiman",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "13328008118965652761"
     },
     "user_tz": 360
    },
    "id": "0R42fLapLPI-",
    "outputId": "88612ccb-2478-4462-dd44-710d55e35142"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1, 2, 3]), ['figure', 'figure caption', 'math formula', 'table'])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels, LABELS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6wKLOv6xBb6D"
   },
   "source": [
    "Create splits one way or the other.  Using the function makes sure the classes are evenly split as there can be un-even classes (for example, there might be way more figures/figure captions than tables).  Note: each instance is tagged as having one class but this is just the most frequent type on that page -- pages can sometimes have multiple types.\n",
    "\n",
    "Or, load from previous file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 438
    },
    "executionInfo": {
     "elapsed": 315,
     "status": "ok",
     "timestamp": 1638305356591,
     "user": {
      "displayName": "Jill Naiman",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "13328008118965652761"
     },
     "user_tz": 360
    },
    "id": "nIG98zeHnea6",
    "outputId": "faf8c608-e844-4503-92da-12d5c5cc2b52"
   },
   "outputs": [],
   "source": [
    "# # splits\n",
    "# X_train = np.loadtxt(splitsDir + 'train.csv', dtype=str, delimiter=',')\n",
    "# X_test = np.loadtxt(splitsDir + 'test.csv', dtype=str, delimiter=',')\n",
    "# X_valid = np.loadtxt(splitsDir + 'valid.csv', dtype=str, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mega_yolo_utils import augmentation_generator, csv_gen\n",
    "onGoogle=False\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "import general_utils\n",
    "from importlib import reload\n",
    "reload(general_utils)\n",
    "from general_utils import parse_annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitsDirIn = None\n",
    "if onGoogle: splitsDirIn = classDirMain\n",
    "train_gen_csv = csv_gen('train', splitsDir=splitsDirIn)\n",
    "valid_gen_csv = csv_gen('valid',splitsDir=splitsDirIn)\n",
    "test_gen_csv = csv_gen('test',splitsDir=splitsDirIn)\n",
    "\n",
    "class ArtificialDatasetNonAug(tf.data.Dataset):\n",
    "\n",
    "    #@tf.function\n",
    "    def _dataset_gen(split, batch_size, method='npz'):\n",
    "        while True:\n",
    "            true_boxes = []; imgs = []; files = []\n",
    "            #time1 = time.perf_counter()\n",
    "\n",
    "            while len(files) < batch_size:\n",
    "                if type(split) == str:\n",
    "                    if b'train' in bytes(split, encoding='utf8'): # NOTE, must be bytes here!!!\n",
    "                        line = next(train_gen_csv)\n",
    "                    elif b'valid' in bytes(split, encoding='utf8'):\n",
    "                        line = next(valid_gen_csv)\n",
    "                    elif b'test' in bytes(split, encoding='utf8'):\n",
    "                        line = next(test_gen_csv)\n",
    "                else:\n",
    "                    if b'train' in split: # NOTE, must be bytes here!!!\n",
    "                        line = next(train_gen_csv)\n",
    "                    elif b'valid' in split:\n",
    "                        line = next(valid_gen_csv)\n",
    "                    elif b'test' in split:\n",
    "                        line = next(test_gen_csv)\n",
    "\n",
    "                files.append(line.strip())\n",
    "\n",
    "            # parse and get full names\n",
    "            #time2 = time.perf_counter()\n",
    "            #print('parse CSV:', time2-time1)\n",
    "            try:\n",
    "                imgs_name, bbox = parse_annotation(files, LABELS, \n",
    "                                                   feature_dir=classDir_main_to_imgs,\n",
    "                                                   annotation_dir=classDir_main_to)\n",
    "            except:\n",
    "                print('error parsing:', imgs_name)\n",
    "                \n",
    "            #print('method is ', method)\n",
    "            if method == bytes('npz', encoding='utf8'): ender = '.npz'\n",
    "            if method == bytes('npy', encoding='utf8'): ender = '.npy'\n",
    "            if method == bytes('pickle', encoding='utf8'): ender = '.pickle'\n",
    "            #print(imgs_name)\n",
    "            # do a debug check\n",
    "            for im in imgs_name:\n",
    "                #print(im)\n",
    "                if ender not in im:\n",
    "                    print('no np file, no pickle file, no npy file')\n",
    "                    import sys; sys.exit()\n",
    "                    \n",
    "            #time3 = time.perf_counter()\n",
    "            #print('Parse annotation:',time3-time2)\n",
    "\n",
    "            # read in and keep images -- npy files/npz\n",
    "            if method == bytes('npz', encoding='utf8'):\n",
    "                for im in imgs_name:\n",
    "                    with np.load(im) as b:\n",
    "                        #b = b['arr_0']/255.0\n",
    "                        imgs.append(b['arr_0']/255.0)\n",
    "            elif method == bytes('npy', encoding='utf8'):\n",
    "                for im in imgs_name:\n",
    "                    b=np.load(im)\n",
    "                    if type(b) != np.ndarray:\n",
    "                        b = b['arr_0'] # WHY????\n",
    "                    imgs.append(b/255.0)\n",
    "            elif method == bytes('pickle', encoding='utf8'):\n",
    "                for im in imgs_name:\n",
    "                    with open(im, 'rb') as f:\n",
    "                        b = pickle.load(f) \n",
    "                        imgs.append(np.array(b)/255.0)\n",
    "            else:\n",
    "                print('no idea what this method is')\n",
    "                        \n",
    "                \n",
    "            #time4 = time.perf_counter()\n",
    "            #print('Load data:', time4-time3)\n",
    "\n",
    "            # finally, format for output\n",
    "            y_true1, y_true2, y_true3 = [],[],[]\n",
    "            for b in bbox:\n",
    "                y1,y2,y3= process_box(b[:,:4], b[:,4].astype('int'),anchors,CLASS)\n",
    "                y_true1.append(y1); y_true2.append(y2); y_true3.append(y3)\n",
    "            # if there is no box, do something different\n",
    "            if len(bbox) == 0:\n",
    "                # fake a box\n",
    "                b = np.array([[111.,  59., 403., 364. ,  4.]])\n",
    "                y1,y2,y3= process_box(b[:,:4], b[:,4].astype('int'),anchors,CLASS)\n",
    "                y1[:] = 0; y2[:]=0;y3[:]=0\n",
    "                y_true1.append(y1); y_true2.append(y2); y_true3.append(y3)        \n",
    "            img = tf.cast(np.array(imgs), tf.float32)     \n",
    "            #time5 = time.perf_counter()\n",
    "            #print('process blocks:', time5-time4)\n",
    "            del imgs\n",
    "            yield img, tf.cast(y_true1, tf.float32), tf.cast(y_true2, tf.float32), tf.cast(y_true3, tf.float32)\n",
    "\n",
    "\n",
    "\n",
    "    def __new__(cls,batch_size=32,split='valid', method='npz'):\n",
    "        dataset = tf.data.Dataset.from_generator(cls._dataset_gen, args=[split, batch_size,method],\n",
    "                                             output_types = (tf.float32, tf.float32, \n",
    "                                                             tf.float32, tf.float32))\n",
    "        return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs_name, bbox = parse_annotation([annotations[0]], LABELS, \n",
    "                                                   feature_dir=classDir_main_to_imgs,\n",
    "                                                   annotation_dir=classDir_main_to)\n",
    "\n",
    "for im in imgs_name:\n",
    "    arr = np.load(im)['arr_0']\n",
    "    \n",
    "#features_dataset = tf.data.Dataset.from_tensor_slices((feature0, feature1, feature2, feature3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bbox, arr\n",
    "arr.shape\n",
    "arr.reshape(1,arr.shape[0]*arr.shape[1]*arr.shape[2]).shape\n",
    "bbox[0].shape\n",
    "\n",
    "# maxboxes stuff\n",
    "maxboxes = 20\n",
    "\n",
    "boxout = np.zeros([maxboxes+1,5])\n",
    "boxout[:bbox[0].shape[0],:] = bbox[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "## start here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mInit signature:\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatList\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m/\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m      A ProtocolMessage\n",
       "\u001b[0;31mFile:\u001b[0m           ~/anaconda3/envs/Paper1/lib/python3.7/site-packages/tensorflow/core/example/feature_pb2.py\n",
       "\u001b[0;31mType:\u001b[0m           GeneratedProtocolMessageType\n",
       "\u001b[0;31mSubclasses:\u001b[0m     \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tf.train.FloatList?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _bytes_feature(value):\n",
    "    \"\"\"Returns a bytes_list from a string / byte.\"\"\"\n",
    "    if isinstance(value, type(tf.constant(0))):\n",
    "        value = value.numpy() # BytesList won't unpack a string from an EagerTensor.\n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "\n",
    "def _float_feature(value):\n",
    "    \"\"\"Returns a float_list from a float / double.\"\"\"\n",
    "    return tf.train.Feature(float_list=tf.train.FloatList(value=[value]))\n",
    "\n",
    "def _int64_feature(value):\n",
    "    \"\"\"Returns an int64_list from a bool / enum / int / uint.\"\"\"\n",
    "    return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n",
    "\n",
    "# def _float32_feature(value):\n",
    "#     \"\"\"Returns an int64_list from a bool / enum / int / uint.\"\"\"\n",
    "#     return tf.train.Feature(float_list=tf.train.Float32List(value=[value]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary with features that may be relevant.\n",
    "def image_example(image, boxes):\n",
    "    #image_shape = tf.io.decode_jpeg(image_string).shape\n",
    "    image_string = image.astype('float32')/255.0\n",
    "    image_string = image.reshape(image.shape[0]*image.shape[1]*image.shape[2])\n",
    "    \n",
    "    nfeatures = image.shape[2]\n",
    "    nboxes = boxes.shape[0]\n",
    "    if nboxes>0:\n",
    "        boxout = boxes.reshape(boxes.shape[0]*boxes.shape[1])\n",
    "    else:\n",
    "        boxout = np.array([])\n",
    "    \n",
    "    feature = {\n",
    "      #'nbox': _int64_feature(np.int64(nboxes)),\n",
    "      #'nfeatures': _int64_feature(np.int64(nfeatures)),\n",
    "      'nbox': _float_feature(np.float32(nboxes)),\n",
    "      'nfeatures': _float_feature(np.float32(nfeatures)),\n",
    "      'boxes': _bytes_feature(boxout.astype('float32').tobytes()),\n",
    "      'image_raw': _bytes_feature(image_string.astype('float32').tobytes()),\n",
    "    }\n",
    "\n",
    "    return tf.train.Example(features=tf.train.Features(feature=feature))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many TFRecord files to have? So that each are $\\sim$100Mb: https://docs.w3cub.com/tensorflow~guide/performance/performance_guide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['/Users/jillnaiman/tmpMegaYolo/binaries/yolo_512x512_cap_ann/1952ApJ___115___82K_p1.xml',\n",
       "       '/Users/jillnaiman/tmpMegaYolo/binaries/yolo_512x512_cap_ann/1994ApJ___427L__43F_p4.xml',\n",
       "       '/Users/jillnaiman/tmpMegaYolo/binaries/yolo_512x512_cap_ann/1995ApJ___445__246C_p6.xml',\n",
       "       ...,\n",
       "       '/Users/jillnaiman/tmpMegaYolo/binaries/yolo_512x512_cap_ann/1978ApJ___220___14H_p1.xml',\n",
       "       '/Users/jillnaiman/tmpMegaYolo/binaries/yolo_512x512_cap_ann/1982ApJ___252__269Y_p2.xml',\n",
       "       '/Users/jillnaiman/tmpMegaYolo/binaries/yolo_512x512_cap_ann/1982ApJ___258__467Y_p2.xml'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 298,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filelist = pd.read_csv(config.tmp_storage_dir+'train.csv',names=['filename'])['filename'].values\n",
    "filelist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/Users/jillnaiman/MegaYolo/binaries_model8/',\n",
       " '/Users/jillnaiman/MegaYolo/yolo_512x512_ann/')"
      ]
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classDir_main_to_imgs, classDir_main_to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "random(size=None)\n",
       "\n",
       "Return random floats in the half-open interval [0.0, 1.0). Alias for\n",
       "`random_sample` to ease forward-porting to the new random API.\n",
       "\u001b[0;31mType:\u001b[0m      builtin_function_or_method\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "np.random.random?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 5)"
      ]
     },
     "execution_count": 312,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.random([maxboxes,5]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no file /Users/jillnaiman/MegaYolo/yolo_512x512_ann/1992AJ____103_1151C_p9.xml\n",
      "no file /Users/jillnaiman/MegaYolo/yolo_512x512_ann/1991ApJS___76__455E_p6.xml\n",
      "no file /Users/jillnaiman/MegaYolo/yolo_512x512_ann/1990ApJ___365__510C_p2.xml\n",
      "no file /Users/jillnaiman/MegaYolo/yolo_512x512_ann/1996ApJ___467__887H_p4.xml\n",
      "no file /Users/jillnaiman/MegaYolo/yolo_512x512_ann/1994ApJS___95__457W_p15.xml\n"
     ]
    }
   ],
   "source": [
    "# write one image file and see how big it is\n",
    "record_file = config.tmp_storage_dir+'test.tfrecords'\n",
    "# what is our max boxes\n",
    "maxboxes = -1\n",
    "for a in filelist:\n",
    "    a = classDir_main_to + a.split('/')[-1]\n",
    "    try:\n",
    "        imgs_name, bbox = parse_annotation([a], LABELS,\n",
    "                                           feature_dir=classDir_main_to_imgs,\n",
    "                                           annotation_dir=classDir_main_to) \n",
    "    except:\n",
    "        print('no file', a)\n",
    "    if len(bbox) > 0:\n",
    "        maxboxes = max([maxboxes,len(bbox[0])])\n",
    "    \n",
    "# print with maxboxes\n",
    "with tf.io.TFRecordWriter(record_file) as writer:\n",
    "    \n",
    "    a = classDir_main_to + filelist[0].split('/')[-1]\n",
    "    imgs_name, bbox = parse_annotation([a], LABELS,\n",
    "                                           feature_dir=classDir_main_to_imgs,\n",
    "                                           annotation_dir=classDir_main_to) \n",
    "    # fake boxes\n",
    "    fakebox = np.random.random([maxboxes,5])\n",
    "    tf_example = image_example(arr,fakebox)\n",
    "    writer.write(tf_example.SerializeToString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12583424, 0.12583424)"
      ]
     },
     "execution_count": 451,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filesize = os.path.getsize(record_file)\n",
    "filesize, filesize/(100.*1e6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.0"
      ]
     },
     "execution_count": 452,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#i.e we want:\n",
    "nfiles_per_file = 100*1e6//filesize\n",
    "nfiles_per_file # per recordio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "592"
      ]
     },
     "execution_count": 453,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nfiles = int(np.ceil(len(filelist)*1.0/nfiles_per_file))\n",
    "nfiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "on 0 of 592\n",
      "on 50 of 592\n",
      "on 100 of 592\n",
      "on 150 of 592\n",
      "cant find /Users/jillnaiman/MegaYolo/yolo_512x512_ann/1992AJ____103_1151C_p9.xml , moving on...\n",
      "on 200 of 592\n",
      "on 250 of 592\n",
      "cant find /Users/jillnaiman/MegaYolo/yolo_512x512_ann/1991ApJS___76__455E_p6.xml , moving on...\n",
      "cant find /Users/jillnaiman/MegaYolo/yolo_512x512_ann/1990ApJ___365__510C_p2.xml , moving on...\n",
      "on 300 of 592\n",
      "on 350 of 592\n",
      "cant find /Users/jillnaiman/MegaYolo/yolo_512x512_ann/1996ApJ___467__887H_p4.xml , moving on...\n",
      "on 400 of 592\n",
      "cant find /Users/jillnaiman/MegaYolo/yolo_512x512_ann/1994ApJS___95__457W_p15.xml , moving on...\n",
      "on 450 of 592\n",
      "on 500 of 592\n",
      "on 550 of 592\n"
     ]
    }
   ],
   "source": [
    "# write some training\n",
    "\n",
    "itrain = 0\n",
    "#iloop = 0\n",
    "#record_file = '/Users/jillnaiman/Downloads/tmp/rio/train'\n",
    "record_file = '/Users/jillnaiman/Downloads/tmp/rio/train_{}.tfrecords'\n",
    "\n",
    "itotalLoop = 0\n",
    "for index in range(nfiles):\n",
    "    if index%50 == 0: print('on', index,'of',nfiles)\n",
    "    with tf.io.TFRecordWriter(record_file.format(index)) as writer:\n",
    "        #print(index*int(nfiles_per_file),min([(index+1)*int(nfiles_per_file),len(filelist)]))\n",
    "        for iloop,a in enumerate(filelist[index*int(nfiles_per_file):min([(index+1)*int(nfiles_per_file),len(filelist)])]):\n",
    "            #print(iloop,a)\n",
    "            a = classDir_main_to + a.split('/')[-1]\n",
    "            \n",
    "            try:\n",
    "                imgs_name, bbox = parse_annotation([a], LABELS,\n",
    "                                                   feature_dir=classDir_main_to_imgs,\n",
    "                                                   annotation_dir=classDir_main_to)\n",
    "            except:\n",
    "                print('cant find', a, ', moving on...')\n",
    "                continue\n",
    "            arr = np.load(imgs_name[0])['arr_0']\n",
    "\n",
    "            if len(bbox) > 0: \n",
    "                bbox = np.array(bbox[0])\n",
    "            else:\n",
    "                bbox = np.array([])\n",
    "            tf_example = image_example(arr,bbox)\n",
    "            writer.write(tf_example.SerializeToString())\n",
    "\n",
    "            # if itotalLoop%int(nfiles_per_file)==0: \n",
    "            #     print(index,iloop,itrain)\n",
    "            #     itrain+=1\n",
    "            # itotalLoop += 1\n",
    "\n",
    "    #if index == 2: import sys; sys.exit()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_records = glob.glob('/Users/jillnaiman/Downloads/tmp/rio/train_*.tfrecords')\n",
    "#list_of_records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_image_dataset = tf.data.TFRecordDataset(list_of_records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary describing the features.\n",
    "image_feature_description = {\n",
    "    'nbox': tf.io.FixedLenFeature([], tf.int64),\n",
    "    'nfeatures': tf.io.FixedLenFeature([], tf.int64),\n",
    "    'boxes': tf.io.FixedLenFeature([], tf.string),\n",
    "    'image_raw': tf.io.FixedLenFeature([], tf.string),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[195.,  21.],\n",
       "       [203.,   7.],\n",
       "       [ 51.,   5.],\n",
       "       [313., 199.],\n",
       "       [359., 391.],\n",
       "       [435.,  16.],\n",
       "       [399., 307.],\n",
       "       [204., 114.],\n",
       "       [ 15., 355.]])"
      ]
     },
     "execution_count": 397,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anchors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [],
   "source": [
    "#image_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 589,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#@tf.function\n",
    "def process_box2(boxes, labels,anchors,CLASS,image_size=config.IMAGE_H):\n",
    "    '''\n",
    "    labels: integer numbers associated with labels, JPN: does this start at 0 or 1??\n",
    "    boxes: [number of boxes on page, xmin, ymin, xmax, ymax]\n",
    "    returns: the correctly formatted 3 y-trues\n",
    "    '''\n",
    "    anchors_mask = [[6, 7, 8], [3, 4, 5], [0, 1, 2]]\n",
    "    #anchors = anchors\n",
    "    box_centers = (boxes[:, 0:2] + boxes[:, 2:4]) / 2\n",
    "    box_size = boxes[:, 2:4] - boxes[:, 0:2]\n",
    "    #print(box_size)\n",
    "    \n",
    "    # y_true_1 = tf.zeros((tf.math.floordiv(image_size,32),\n",
    "    #                         tf.math.floordiv(image_size,32),\n",
    "    #                         3, 5 + CLASS), tf.float32)\n",
    "    # y_true_2 = tf.zeros((tf.math.floordiv(image_size,16),\n",
    "    #                         tf.math.floordiv(image_size,16),\n",
    "    #                         3, 5 + CLASS), tf.float32)\n",
    "    # y_true_3 = tf.zeros((tf.math.floordiv(image_size,8),\n",
    "    #                         tf.math.floordiv(image_size,8),\n",
    "    #                         3, 5 + CLASS), tf.float32)\n",
    "    #print('hi')\n",
    "\n",
    "    y_true_1 = np.zeros((image_size // 32,\n",
    "                            image_size // 32,\n",
    "                            3, 5 + CLASS.numpy()), np.float32)\n",
    "    y_true_2 = np.zeros((image_size // 16,\n",
    "                            image_size // 16,\n",
    "                            3, 5 + CLASS.numpy()), np.float32)\n",
    "    y_true_3 = np.zeros((image_size // 8,\n",
    "                            image_size // 8,\n",
    "                            3, 5 + CLASS.numpy()), np.float32)\n",
    "    # y_true_1 = np.zeros((tf.math.floordiv(image_size,32),\n",
    "    #                         tf.math.floordiv(image_size,32),\n",
    "    #                         3, 5 + CLASS.numpy()), np.float32)\n",
    "    # y_true_2 = np.zeros((tf.math.floordiv(image_size,16),\n",
    "    #                         tf.math.floordiv(image_size,16),\n",
    "    #                         3, 5 + CLASS), np.float32)\n",
    "    # y_true_3 = np.zeros((tf.math.floordiv(image_size,8),\n",
    "    #                         tf.math.floordiv(image_size,8),\n",
    "    #                         3, 5 + CLASS), np.float32)\n",
    "    y_true = [y_true_1, y_true_2, y_true_3]\n",
    "\n",
    "    box_size = tf.expand_dims(box_size, 1)\n",
    "\n",
    "    min_np = tf.maximum(- box_size / 2, - anchors / 2)\n",
    "    max_np = tf.minimum(box_size / 2, anchors / 2)\n",
    "\n",
    "    whs = max_np - min_np\n",
    "\n",
    "    overlap = whs[:, :, 0] * whs[:, :, 1]\n",
    "    union = box_size[:, :, 0] * box_size[:, :, 1] + anchors[:, 0] * anchors[:, 1] - whs[:, :, 0] * whs[:, :, 1] + 1e-10\n",
    "\n",
    "    iou = overlap / union\n",
    "    best_match_idx = tf.argmax(iou, axis=1)\n",
    "\n",
    "    #ratio_dict = {1.: 8., 2.: 16., 3.: 32.}\n",
    "    ratio_dict = tf.constant([8,16,32],tf.int32)\n",
    "    #ratio = -1\n",
    "    #for i, idx in enumerate(best_match_idx):\n",
    "    for i in tf.range(tf.shape(best_match_idx)[0]):\n",
    "        idx = best_match_idx[i]\n",
    "        feature_map_group = 2 - idx // 3\n",
    "        ind = tf.cast(tf.math.ceil((tf.cast(idx + 1,tf.float32)) / 3.),tf.int32)\n",
    "        #ratio = ratio_dict[np.ceil((idx + 1) / 3.)]\n",
    "        ratio = tf.slice(ratio_dict,[ind],[1])\n",
    "        x = int(tf.math.floor(box_centers[i, 0] / tf.cast(ratio,tf.float32)))\n",
    "        y = int(tf.math.floor(box_centers[i, 1] / tf.cast(ratio,tf.float32)))\n",
    "        #k = anchors_mask[feature_map_group].index(idx)\n",
    "        m1 = tf.gather(anchors_mask,feature_map_group)\n",
    "        k = tf.gather(m1,idx)\n",
    "        #k = tf.gather(anchors_mask[feature_map_group],idx)\n",
    "        c = labels[i]\n",
    "\n",
    "        y_true[feature_map_group][y, x, k, :2] = box_centers[i]\n",
    "        # y_true = y_true[feature_map_group][y, x, k, :2].assign(box_centers[i])\n",
    "        y_true[feature_map_group][y, x, k, 2:4] = box_size[i]\n",
    "        y_true[feature_map_group][y, x, k, 4] = 1.\n",
    "        # if feature_map_group == 0:\n",
    "        #     yt2 = np\n",
    "        #     y_true_11[y,x,k,:2] = box_centers[i]\n",
    "        try:\n",
    "            #y_true[feature_map_group][y, x, k, 5 + c] = 1.\n",
    "            y_true[feature_map_group][y, x, k, 5 + c -1] = 1. # labels start at 0\n",
    "        except:\n",
    "            print('in parse')\n",
    "            print(y,x,k,c, 5+c)\n",
    "            print(labels)\n",
    "\n",
    "    return y_true_1, y_true_2, y_true_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 658,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#@tf.function\n",
    "def process_box3(boxes, labels,anchors,CLASS):\n",
    "    '''\n",
    "    labels: integer numbers associated with labels, JPN: does this start at 0 or 1??\n",
    "    boxes: [number of boxes on page, xmin, ymin, xmax, ymax]\n",
    "    returns: the correctly formatted 3 y-trues\n",
    "    '''\n",
    "    anchors_mask = [[6, 7, 8], [3, 4, 5], [0, 1, 2]]\n",
    "    #anchors = anchors\n",
    "    box_centers = (boxes[:, 0:2] + boxes[:, 2:4]) / 2\n",
    "    box_size = boxes[:, 2:4] - boxes[:, 0:2]\n",
    "\n",
    "    y_true_1 = np.zeros((image_size // 32,\n",
    "                            image_size // 32,\n",
    "                            3, 5 + CLASS), np.float32)\n",
    "    y_true_2 = np.zeros((image_size // 16,\n",
    "                            image_size // 16,\n",
    "                            3, 5 + CLASS), np.float32)\n",
    "    y_true_3 = np.zeros((image_size // 8,\n",
    "                            image_size // 8,\n",
    "                            3, 5 + CLASS), np.float32)\n",
    "\n",
    "    y_true = [y_true_1, y_true_2, y_true_3]\n",
    "\n",
    "    box_size = np.expand_dims(box_size, 1)\n",
    "\n",
    "    min_np = np.maximum(- box_size / 2, - anchors / 2)\n",
    "    max_np = np.minimum(box_size / 2, anchors / 2)\n",
    "\n",
    "    whs = max_np - min_np\n",
    "\n",
    "    overlap = whs[:, :, 0] * whs[:, :, 1]\n",
    "    union = box_size[:, :, 0] * box_size[:, :, 1] + anchors[:, 0] * anchors[:, 1] - whs[:, :, 0] * whs[:, :, 1] + 1e-10\n",
    "\n",
    "    iou = overlap / union\n",
    "    best_match_idx = np.argmax(iou, axis=1)\n",
    "\n",
    "    ratio_dict = {1.: 8., 2.: 16., 3.: 32.}\n",
    "    for i, idx in enumerate(best_match_idx):\n",
    "        feature_map_group = 2 - idx // 3\n",
    "        ratio = ratio_dict[np.ceil((idx + 1) / 3.)]\n",
    "        x = int(np.floor(box_centers[i, 0] / ratio))\n",
    "        y = int(np.floor(box_centers[i, 1] / ratio))\n",
    "        k = anchors_mask[feature_map_group].index(idx)\n",
    "        c = labels[i].numpy()\n",
    "\n",
    "        y_true[feature_map_group][y, x, k, :2] = box_centers[i]\n",
    "        y_true[feature_map_group][y, x, k, 2:4] = box_size[i]\n",
    "        y_true[feature_map_group][y, x, k, 4] = 1.\n",
    "        try:\n",
    "            y_true[feature_map_group][y, x, k, 5 + c -1] = 1. # labels start at 0\n",
    "        except:\n",
    "            print('in parse')\n",
    "            print('y','x','k','c')\n",
    "            print(y,x,k,c, 5+c)\n",
    "            print(labels)\n",
    "\n",
    "    return y_true_1, y_true_2, y_true_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 665,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3])"
      ]
     },
     "execution_count": 665,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 659,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _parse_image_function(example_proto,anchors,CLASS):\n",
    "    image_features = tf.io.parse_single_example(example_proto, image_feature_description)\n",
    "\n",
    "    nboxes = image_features['nbox']\n",
    "    nfeatures = image_features['nfeatures']\n",
    "    boxes = tf.io.decode_raw(image_features['boxes'],tf.float32)\n",
    "    boxes = tf.reshape(boxes,[nboxes,5])\n",
    "    images_raw = image_features['image_raw']\n",
    "    image = tf.io.decode_raw(images_raw,tf.float32)\n",
    "    image = tf.reshape(image,[config.IMAGE_H,config.IMAGE_W,nfeatures])\n",
    "\n",
    "    # process boxes\n",
    "    y1,y2,y3 = tf.py_function(process_box3,\n",
    "                              (boxes[:,:4], boxes[:,4],anchors,CLASS),\n",
    "                              (tf.float32,tf.float32,tf.float32))   \n",
    "    #y1,y2,y3 = process_box3(boxes[:,:4], boxes[:,4],anchors,CLASS)\n",
    "    \n",
    "    return image, nboxes, nfeatures, boxes,y1,y2,y3\n",
    "    \n",
    "    #return tf.io.parse_single_example(example_proto, image_feature_description)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 664,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "in user code:\n\n    /var/folders/t5/9xgccmv92hnfvjwd62mk8zqh0000gn/T/ipykernel_25922/799275935.py:4 None  *\n        lambda example_proto:_parse_image_function(example_proto,LABELS,anchors,CLASS))\n\n    TypeError: tf___parse_image_function() takes 3 positional arguments but 4 were given\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/t5/9xgccmv92hnfvjwd62mk8zqh0000gn/T/ipykernel_25922/799275935.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0manchors_in\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manchors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mCLASS_in\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCLASS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mparsed_image_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mraw_image_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mexample_proto\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0m_parse_image_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexample_proto\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mLABELS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0manchors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mCLASS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mparsed_image_dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/Paper1/lib/python3.7/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36mmap\u001b[0;34m(self, map_func, num_parallel_calls, deterministic)\u001b[0m\n\u001b[1;32m   1803\u001b[0m     \"\"\"\n\u001b[1;32m   1804\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnum_parallel_calls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1805\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mMapDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreserve_cardinality\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1806\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1807\u001b[0m       return ParallelMapDataset(\n",
      "\u001b[0;32m~/anaconda3/envs/Paper1/lib/python3.7/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, input_dataset, map_func, use_inter_op_parallelism, preserve_cardinality, use_legacy_function)\u001b[0m\n\u001b[1;32m   4205\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transformation_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4206\u001b[0m         \u001b[0mdataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4207\u001b[0;31m         use_legacy_function=use_legacy_function)\n\u001b[0m\u001b[1;32m   4208\u001b[0m     variant_tensor = gen_dataset_ops.map_dataset(\n\u001b[1;32m   4209\u001b[0m         \u001b[0minput_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variant_tensor\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/Paper1/lib/python3.7/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, func, transformation_name, dataset, input_classes, input_shapes, input_types, input_structure, add_to_graph, use_legacy_function, defun_kwargs)\u001b[0m\n\u001b[1;32m   3523\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mtracking\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresource_tracker_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_tracker\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3524\u001b[0m         \u001b[0;31m# TODO(b/141462134): Switch to using garbage collection.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3525\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwrapper_fn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_concrete_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3526\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0madd_to_graph\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3527\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_to_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/Paper1/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mget_concrete_function\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3050\u001b[0m     \"\"\"\n\u001b[1;32m   3051\u001b[0m     graph_function = self._get_concrete_function_garbage_collected(\n\u001b[0;32m-> 3052\u001b[0;31m         *args, **kwargs)\n\u001b[0m\u001b[1;32m   3053\u001b[0m     \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_garbage_collector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3054\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/Paper1/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_get_concrete_function_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3017\u001b[0m       \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3018\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3019\u001b[0;31m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3020\u001b[0m       \u001b[0mseen_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3021\u001b[0m       captured = object_identity.ObjectIdentitySet(\n",
      "\u001b[0;32m~/anaconda3/envs/Paper1/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   3359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3360\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3361\u001b[0;31m           \u001b[0mgraph_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3362\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3363\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/Paper1/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   3204\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3205\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3206\u001b[0;31m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[1;32m   3207\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_attributes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3208\u001b[0m         \u001b[0mfunction_spec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_spec\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/Paper1/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m    988\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    989\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 990\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    991\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    992\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/Paper1/lib/python3.7/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36mwrapper_fn\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m   3516\u001b[0m           attributes=defun_kwargs)\n\u001b[1;32m   3517\u001b[0m       \u001b[0;32mdef\u001b[0m \u001b[0mwrapper_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=missing-docstring\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3518\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_wrapper_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3519\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstructure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tensor_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output_structure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3520\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/Paper1/lib/python3.7/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m_wrapper_helper\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m   3451\u001b[0m         \u001b[0mnested_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnested_args\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3452\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3453\u001b[0;31m       \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mautograph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf_convert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag_ctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnested_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3454\u001b[0m       \u001b[0;31m# If `func` returns a list of tensors, `nest.flatten()` and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3455\u001b[0m       \u001b[0;31m# `ops.convert_to_tensor()` would conspire to attempt to stack\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/Paper1/lib/python3.7/site-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    668\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    669\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ag_error_metadata'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 670\u001b[0;31m           \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    671\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    672\u001b[0m           \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: in user code:\n\n    /var/folders/t5/9xgccmv92hnfvjwd62mk8zqh0000gn/T/ipykernel_25922/799275935.py:4 None  *\n        lambda example_proto:_parse_image_function(example_proto,LABELS,anchors,CLASS))\n\n    TypeError: tf___parse_image_function() takes 3 positional arguments but 4 were given\n"
     ]
    }
   ],
   "source": [
    "#parsed_image_dataset = raw_image_dataset.map(_parse_image_function)\n",
    "anchors_in = tf.constant(anchors,tf.float32)\n",
    "CLASS_in = tf.constant(CLASS,tf.int32)\n",
    "parsed_image_dataset = raw_image_dataset.map(lambda example_proto:_parse_image_function(example_proto,anchors,CLASS))\n",
    "parsed_image_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 661,
   "metadata": {},
   "outputs": [],
   "source": [
    "#boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 662,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for checking:\n",
    "import IPython.display as display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 663,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in parse\n",
      "y x k c\n",
      "24 15 0 4.0 9.0\n",
      "tf.Tensor([4. 4. 4.], shape=(3,), dtype=float32)\n",
      "in parse\n",
      "y x k c\n",
      "2 7 1 4.0 9.0\n",
      "tf.Tensor([4. 4. 4.], shape=(3,), dtype=float32)\n",
      "in parse\n",
      "y x k c\n",
      "6 7 1 4.0 9.0\n",
      "tf.Tensor([4. 4. 4.], shape=(3,), dtype=float32)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for image,nboxes,nfeatures,boxes,y1,y2,y3 in parsed_image_dataset.take(1):\n",
    "    # nboxes = image_features['nbox']\n",
    "    # nfeatures = image_features['nfeatures']\n",
    "    # boxes = tf.io.decode_raw(image_features['boxes'],tf.float32)\n",
    "    # boxes = tf.reshape(boxes,[nboxes,5])\n",
    "    # images_raw = image_features['image_raw']\n",
    "    # image = tf.io.decode_raw(images_raw,tf.float32)\n",
    "    # image = tf.reshape(image,[config.IMAGE_H,config.IMAGE_W,nfeatures])\n",
    "    imgPlot = image.numpy()\n",
    "    plt.imshow(imgPlot[:,:,0],cmap='gray')\n",
    "    #print(nboxes,nfeatures,boxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for image_features in parsed_image_dataset.take(1):\n",
    "#     nboxes = image_features['nbox']\n",
    "#     nfeatures = image_features['nfeatures']\n",
    "#     boxes = tf.io.decode_raw(image_features['boxes'],tf.float32)\n",
    "#     boxes = tf.reshape(boxes,[nboxes,5])\n",
    "#     images_raw = image_features['image_raw']\n",
    "#     image = tf.io.decode_raw(images_raw,tf.float32)\n",
    "#     image = tf.reshape(image,[config.IMAGE_H,config.IMAGE_W,nfeatures])\n",
    "#     imgPlot = image.numpy()\n",
    "#     plt.imshow(imgPlot[:,:,0],cmap='gray')\n",
    "#     print(nboxes,nfeatures,boxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imgPlot[:,:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a description of the features.\n",
    "feature_description = {\n",
    "    'feature0': tf.io.FixedLenFeature([], tf.int64, default_value=0),\n",
    "    'feature1': tf.io.FixedLenFeature([], tf.int64, default_value=0),\n",
    "    'feature2': tf.io.FixedLenFeature([], tf.string, default_value=''),\n",
    "    'feature3': tf.io.FixedLenFeature([], tf.float32, default_value=0.0),\n",
    "}\n",
    "\n",
    "def _parse_function(example_proto):\n",
    "  # Parse the input `tf.train.Example` proto using the dictionary above.\n",
    "  return tf.io.parse_single_example(example_proto, feature_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [],
   "source": [
    "def serialize_example(nbox,nfeatures,boxes,image_raw):\n",
    "    \"\"\"\n",
    "    Creates a tf.train.Example message ready to be written to a file.\n",
    "    \"\"\"\n",
    "    # Create a dictionary mapping the feature name to the tf.train.Example-compatible\n",
    "    # data type.\n",
    "    feature = {\n",
    "      'nbox': _int64_feature(nbox),\n",
    "      'nfeatures': _int64_feature(nfeatures),\n",
    "      'boxes': _bytes_feature(boxes),\n",
    "      'image_raw': _bytes_feature(image_raw),\n",
    "    }\n",
    "\n",
    "    # Create a Features message using tf.train.Example.\n",
    "\n",
    "    example_proto = tf.train.Example(features=tf.train.Features(feature=feature))\n",
    "    return example_proto.SerializeToString()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bytes_list {\n",
       "  value: \"\"\n",
       "}"
      ]
     },
     "execution_count": 322,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to read\n",
    "def parse_proto(example_proto):\n",
    "  features = {\n",
    "    'X': tf.FixedLenFeature((345,), tf.float32),\n",
    "    'y': tf.FixedLenFeature((5,), tf.float32),\n",
    "  }\n",
    "  parsed_features = tf.parse_single_example(example_proto, features)\n",
    "  return parsed_features['X'], parsed_features['y']\n",
    "\n",
    "def read_tfrecords(file_names=(\"file1.tfrecord\", \"file2.tfrecord\", \"file3.tfrecord\"),\n",
    "                   buffer_size=10000,\n",
    "                   batch_size=100):\n",
    "    dataset = tf.contrib.data.TFRecordDataset(file_names)\n",
    "    dataset = dataset.map(parse_proto)\n",
    "    dataset = dataset.shuffle(buffer_size)\n",
    "    dataset = dataset.repeat()\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    return tf.contrib.data.Iterator.from_structure(dataset.output_types, dataset.output_shapes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'serialized_example' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/t5/9xgccmv92hnfvjwd62mk8zqh0000gn/T/ipykernel_25922/2583518312.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mexample_proto\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExample\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFromString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mserialized_example\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'serialized_example' is not defined"
     ]
    }
   ],
   "source": [
    "example_proto = tf.train.Example.FromString(serialized_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5,)"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bbox[0].reshape(1*5).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 5)"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bbox[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_dataset = tf.data.Dataset.from_tensor_slices((arr.reshape(1,arr.shape[0]*arr.shape[1]*arr.shape[2]),\n",
    "                                                       bbox[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TensorSliceDataset shapes: ((3145728,), (5,)), types: (tf.uint8, tf.float64)>"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([0 0 0 ... 0 0 0], shape=(3145728,), dtype=uint8)\n",
      "tf.Tensor([ 46. 451. 257. 473.   2.], shape=(5,), dtype=float64)\n"
     ]
    }
   ],
   "source": [
    "for f0,f1 in features_dataset.take(1):\n",
    "    print(f0)\n",
    "    print(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _float_feature(value):\n",
    "    \"\"\"Returns a float_list from a float / double.\"\"\"\n",
    "    return tf.train.Feature(float_list=tf.train.FloatList(value=[value]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "def serialize_example(feature0, feature1):\n",
    "    \"\"\"\n",
    "    Creates a tf.train.Example message ready to be written to a file.\n",
    "    \"\"\"\n",
    "    # Create a dictionary mapping the feature name to the tf.train.Example-compatible\n",
    "    # data type.\n",
    "    feature = {\n",
    "      'feature0': _float_feature(feature0),\n",
    "      'feature1': _float_feature(feature1),\n",
    "    }\n",
    "\n",
    "    # Create a Features message using tf.train.Example.\n",
    "\n",
    "    example_proto = tf.train.Example(features=tf.train.Features(feature=feature))\n",
    "    return example_proto.SerializeToString()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_serialize_example(f0,f1):\n",
    "    tf_string = tf.py_function(\n",
    "        serialize_example,\n",
    "        (f0, f1),  # Pass these args to the above function.\n",
    "        tf.string)      # The return type is `tf.string`.\n",
    "    return tf.reshape(tf_string, ()) # The result is a scalar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([0. 0. 0. ... 0. 0. 0.], shape=(3145728,), dtype=float64) tf.Tensor(\n",
      "[ 46. 451. 257. 473.   2.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.], shape=(105,), dtype=float64)\n"
     ]
    }
   ],
   "source": [
    "#tf.cast(f0,tf.float32)\n",
    "features_dataset\n",
    "for f0,f1 in features_dataset.take(1):\n",
    "    print(f0,f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "array([[0., 0., 0., ..., 0., 0., 0.]]) has type numpy.ndarray, but expected one of: int, long, float",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/t5/9xgccmv92hnfvjwd62mk8zqh0000gn/T/ipykernel_25922/4247669254.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#tf_serialize_example(f0,f1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mserialize_example\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf0np\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mf1np\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/t5/9xgccmv92hnfvjwd62mk8zqh0000gn/T/ipykernel_25922/1118630037.py\u001b[0m in \u001b[0;36mserialize_example\u001b[0;34m(feature0, feature1)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;31m# data type.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     feature = {\n\u001b[0;32m----> 8\u001b[0;31m       \u001b[0;34m'feature0'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0m_float_feature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m       \u001b[0;34m'feature1'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0m_float_feature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     }\n",
      "\u001b[0;32m/var/folders/t5/9xgccmv92hnfvjwd62mk8zqh0000gn/T/ipykernel_25922/2948262194.py\u001b[0m in \u001b[0;36m_float_feature\u001b[0;34m(value)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_float_feature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;34m\"\"\"Returns a float_list from a float / double.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFeature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfloat_list\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatList\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: array([[0., 0., 0., ..., 0., 0., 0.]]) has type numpy.ndarray, but expected one of: int, long, float"
     ]
    }
   ],
   "source": [
    "#tf_serialize_example(f0,f1)\n",
    "\n",
    "serialize_example(f0np,f1np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Dimensions 3145728 and 105 are not compatible",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/t5/9xgccmv92hnfvjwd62mk8zqh0000gn/T/ipykernel_25922/3470775508.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mf1np\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mboxout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mboxout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mboxout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mfeatures_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_tensor_slices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf0np\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mf1np\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mf1\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfeatures_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/Paper1/lib/python3.7/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36mfrom_tensor_slices\u001b[0;34m(tensors)\u001b[0m\n\u001b[1;32m    689\u001b[0m       \u001b[0mDataset\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mA\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m     \"\"\"\n\u001b[0;32m--> 691\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mTensorSliceDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    692\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    693\u001b[0m   \u001b[0;32mclass\u001b[0m \u001b[0m_GeneratorState\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/Paper1/lib/python3.7/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, element)\u001b[0m\n\u001b[1;32m   3163\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tensors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3164\u001b[0m       batch_dim.assert_is_compatible_with(tensor_shape.Dimension(\n\u001b[0;32m-> 3165\u001b[0;31m           tensor_shape.dimension_value(t.get_shape()[0])))\n\u001b[0m\u001b[1;32m   3166\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3167\u001b[0m     variant_tensor = gen_dataset_ops.tensor_slice_dataset(\n",
      "\u001b[0;32m~/anaconda3/envs/Paper1/lib/python3.7/site-packages/tensorflow/python/framework/tensor_shape.py\u001b[0m in \u001b[0;36massert_is_compatible_with\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    280\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_compatible_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m       raise ValueError(\"Dimensions %s and %s are not compatible\" %\n\u001b[0;32m--> 282\u001b[0;31m                        (self, other))\n\u001b[0m\u001b[1;32m    283\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mmerge_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Dimensions 3145728 and 105 are not compatible"
     ]
    }
   ],
   "source": [
    "\n",
    "# for each annotation\n",
    "boxout = np.zeros([maxboxes+1,5])\n",
    "boxout[:bbox[0].shape[0],:] = bbox[0]\n",
    "\n",
    "f0np = arr.reshape(arr.shape[0]*arr.shape[1]*arr.shape[2])\n",
    "f0np = f0np.astype('float')/255.\n",
    "f1np = boxout.reshape(boxout.shape[0]*boxout.shape[1])\n",
    "\n",
    "features_dataset = tf.data.Dataset.from_tensor_slices((f0np,f1np))\n",
    "\n",
    "for f0,f1 in features_dataset.take(1):\n",
    "    #print(f0)\n",
    "    #print(f1)\n",
    "    tf_serialize_example(f0,f1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<TensorSliceDataset shapes: (3145728,), types: tf.float64>,\n",
       " <TensorSliceDataset shapes: (105,), types: tf.float64>)"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f0,f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TensorSliceDataset shapes: (21, 5), types: tf.float64>"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # load 1\n",
    "# imgs_name, bbox = parse_annotation([annotations[0]], LABELS, \n",
    "#                                                    feature_dir=classDir_main_to_imgs,\n",
    "#                                                    annotation_dir=classDir_main_to)\n",
    "\n",
    "\n",
    "# # read in and keep images -- npy files\n",
    "# for im in imgs_name:\n",
    "#     arr = np.load(im)#['arr_0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.load(im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "#arr.astype('uint8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nbatch = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #dataset = ArtificialDatasetNonAug(batch_size=32,split='valid')\n",
    "# b = np.load('/Users/jillnaiman/MegaYolo/binaries_model8_noncom/1988ApJ___334__144K_p8.npy')\n",
    "# b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "#b['arr_0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# benchmark tests\n",
    "import time\n",
    "def benchmark(dataset, num_epochs=1, nbatch=32):\n",
    "    start_time = time.perf_counter()\n",
    "    for epoch_num in range(num_epochs):\n",
    "        ienum=0\n",
    "        for sample in dataset:\n",
    "            #if ienum%10==0: print(ienum)\n",
    "            #print(ienum)\n",
    "            # Performing a training step\n",
    "            time.sleep(0.01)\n",
    "            ienum+=1\n",
    "            if ienum>nbatch: break\n",
    "    print(\"Execution time:\", time.perf_counter() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple\n",
    "#benchmark(ArtificialDatasetNonAug(batch_size=10,split='valid'),nbatch=nbatch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try different methods and time\n",
    "#nbatch = 10; nloop = 10\n",
    "nbatch = 25; nloop = 4\n",
    "th = []\n",
    "\n",
    "for n in range(nloop):\n",
    "    t1 = time.perf_counter()\n",
    "    benchmark(ArtificialDatasetNonAug(batch_size=nbatch,split='valid',method=method),\n",
    "              nbatch=nbatch)\n",
    "    t2 = time.perf_counter()\n",
    "    th.append(t2-t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40.807152897499996, 0.9870036160257788)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.median(th), np.std(th)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/jillnaiman/MegaYolo/binaries_model8_pickle/'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classDir_main_to_imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for tfrecords files\n",
    "def parse_proto(example_proto):\n",
    "    features = {\n",
    "        'X': tf.io.FixedLenFeature([], tf.float32),\n",
    "        'x1': tf.io.FixedLenSequenceFeature([10], tf.float32,allow_missing = True,default_value=0),\n",
    "        'x2': tf.io.FixedLenSequenceFeature([10], tf.float32,allow_missing = True,default_value=0),\n",
    "        'y1': tf.io.FixedLenSequenceFeature([10], tf.float32,allow_missing = True,default_value=0),\n",
    "        'y2': tf.io.FixedLenSequenceFeature([10], tf.float32,allow_missing = True,default_value=0),\n",
    "        'class': tf.io.FixedLenSequenceFeature([10], tf.float32,allow_missing = True,default_value=0),\n",
    "    }\n",
    "    parsed_features = tf.io.parse_single_example(example_proto, features)\n",
    "    print(parsed_features)\n",
    "    return parsed_features['X']\n",
    "#     y_true1, y_true2, y_true3 = [],[],[]\n",
    "#     for b in bbox:\n",
    "#         y1,y2,y3= process_box(b[:,:4], b[:,4].astype('int'),anchors,CLASS)\n",
    "#         y_true1.append(y1); y_true2.append(y2); y_true3.append(y3)\n",
    "#     # if there is no box, do something different\n",
    "#     if len(bbox) == 0:\n",
    "#         # fake a box\n",
    "#         b = np.array([[111.,  59., 403., 364. ,  4.]])\n",
    "#         y1,y2,y3= process_box(b[:,:4], b[:,4].astype('int'),anchors,CLASS)\n",
    "#         y1[:] = 0; y2[:]=0;y3[:]=0\n",
    "#         y_true1.append(y1); y_true2.append(y2); y_true3.append(y3)        \n",
    "#     img = tf.cast(np.array(imgs), tf.float32)     \n",
    "#     #time5 = time.perf_counter()\n",
    "#     #print('process blocks:', time5-time4)\n",
    "#     del imgs\n",
    "#     yield img, tf.cast(y_true1, tf.float32), tf.cast(y_true2, tf.float32), tf.cast(y_true3, tf.float32)\n",
    "\n",
    "    \n",
    "#     return parsed_features['X'], parsed_features['y']\n",
    "\n",
    "def read_tfrecords(file_names=(\"file1.tfrecord\", \"file2.tfrecord\", \"file3.tfrecord\"),\n",
    "                   buffer_size=10000,\n",
    "                   batch_size=100):\n",
    "    dataset = tf.data.TFRecordDataset(file_names)\n",
    "    dataset = dataset.map(parse_proto)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'X': <tf.Tensor 'ParseSingleExample/ParseExample/ParseExampleV2:0' shape=() dtype=float32>, 'class': <tf.Tensor 'ParseSingleExample/ParseExample/ParseExampleV2:1' shape=(None, 10) dtype=float32>, 'x1': <tf.Tensor 'ParseSingleExample/ParseExample/ParseExampleV2:2' shape=(None, 10) dtype=float32>, 'x2': <tf.Tensor 'ParseSingleExample/ParseExample/ParseExampleV2:3' shape=(None, 10) dtype=float32>, 'y1': <tf.Tensor 'ParseSingleExample/ParseExample/ParseExampleV2:4' shape=(None, 10) dtype=float32>, 'y2': <tf.Tensor 'ParseSingleExample/ParseExample/ParseExampleV2:5' shape=(None, 10) dtype=float32>}\n"
     ]
    }
   ],
   "source": [
    "dirtf = '/Users/jillnaiman/MegaYolo/binaries_model8_tfrecord/'\n",
    "tffiles = [dirtf+'1981AJ_____86__206V_p1.tfrecord',\n",
    "           dirtf+'1992AJ____104_2161O_p5.tfrecord',\n",
    "           dirtf+'1997AJ____114__913N_p13.tfrecord']\n",
    "\n",
    "\n",
    "dataset = read_tfrecords(file_names=tffiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-01 18:31:54.646748: W tensorflow/core/framework/op_kernel.cc:1763] OP_REQUIRES failed at example_parsing_ops.cc:94 : Invalid argument: Key: x2.  Can't parse serialized Example.\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Key: x2.  Can't parse serialized Example.\n\t [[{{node ParseSingleExample/ParseExample/ParseExampleV2}}]]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/Paper1/lib/python3.7/site-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36mexecution_mode\u001b[0;34m(mode)\u001b[0m\n\u001b[1;32m   2112\u001b[0m       \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecutor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexecutor_new\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2113\u001b[0;31m       \u001b[0;32myield\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2114\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/Paper1/lib/python3.7/site-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m_next_internal\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    732\u001b[0m           \u001b[0moutput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flat_output_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 733\u001b[0;31m           output_shapes=self._flat_output_shapes)\n\u001b[0m\u001b[1;32m    734\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/Paper1/lib/python3.7/site-packages/tensorflow/python/ops/gen_dataset_ops.py\u001b[0m in \u001b[0;36miterator_get_next\u001b[0;34m(iterator, output_types, output_shapes, name)\u001b[0m\n\u001b[1;32m   2578\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2579\u001b[0;31m       \u001b[0m_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2580\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/Paper1/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   6861\u001b[0m   \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6862\u001b[0;31m   \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6863\u001b[0m   \u001b[0;31m# pylint: enable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/Paper1/lib/python3.7/site-packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Key: x2.  Can't parse serialized Example.\n\t [[{{node ParseSingleExample/ParseExample/ParseExampleV2}}]] [Op:IteratorGetNext]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/t5/9xgccmv92hnfvjwd62mk8zqh0000gn/T/ipykernel_25922/3134862119.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mparsed_record\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparsed_record\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/Paper1/lib/python3.7/site-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    745\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    746\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 747\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    748\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    749\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/Paper1/lib/python3.7/site-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m_next_internal\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    737\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_element_spec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_from_compatible_tensor_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    738\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 739\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mstructure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_compatible_tensor_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_element_spec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    741\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/Paper1/lib/python3.7/contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type, value, traceback)\u001b[0m\n\u001b[1;32m    128\u001b[0m                 \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthrow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraceback\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m                 \u001b[0;31m# Suppress StopIteration *unless* it's the same exception that\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/Paper1/lib/python3.7/site-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36mexecution_mode\u001b[0;34m(mode)\u001b[0m\n\u001b[1;32m   2114\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2115\u001b[0m       \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecutor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexecutor_old\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2116\u001b[0;31m       \u001b[0mexecutor_new\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/Paper1/lib/python3.7/site-packages/tensorflow/python/eager/executor.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     67\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;34m\"\"\"Waits for ops dispatched in this executor to finish.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m     \u001b[0mpywrap_tfe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTFE_ExecutorWaitForAllPendingNodes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mclear_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Key: x2.  Can't parse serialized Example.\n\t [[{{node ParseSingleExample/ParseExample/ParseExampleV2}}]]"
     ]
    }
   ],
   "source": [
    "for parsed_record in dataset.take(2):\n",
    "    print(parsed_record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now, following: https://stackoverflow.com/questions/48889482/feeding-npy-numpy-files-into-tensorflow-data-pipeline\n",
    "# try with io decode?\n",
    "def npy_header_offset(npy_path):\n",
    "    with open(str(npy_path), 'rb') as f:\n",
    "        print(f.read(6).decode('utf-8'))\n",
    "        if f.read(6) != b'\\x93NUMPY':\n",
    "            raise ValueError('Invalid NPY file.')\n",
    "        version_major, version_minor = f.read(2)\n",
    "        if version_major == 1:\n",
    "            header_len_size = 2\n",
    "        elif version_major == 2:\n",
    "            header_len_size = 4\n",
    "        else:\n",
    "            raise ValueError('Unknown NPY file version {}.{}.'.format(version_major, version_minor))\n",
    "        header_len = sum(b << (8 * i) for i, b in enumerate(f.read(header_len_size)))\n",
    "        header = f.read(header_len)\n",
    "        if not header.endswith(b'\\n'):\n",
    "            raise ValueError('Invalid NPY file.')\n",
    "        return f.tell()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PK\u0003\u0004\u0014\u0000\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Invalid NPY file.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/t5/9xgccmv92hnfvjwd62mk8zqh0000gn/T/ipykernel_25922/704836339.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m                 \u001b[0mnfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'arr_0'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m                 \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'arr_0'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m             \u001b[0mheader_offset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnpy_header_offset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mmod\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'npy'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0mb\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/t5/9xgccmv92hnfvjwd62mk8zqh0000gn/T/ipykernel_25922/838241765.py\u001b[0m in \u001b[0;36mnpy_header_offset\u001b[0;34m(npy_path)\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34mb'\\x93NUMPY'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Invalid NPY file.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0mversion_major\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mversion_minor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mversion_major\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Invalid NPY file."
     ]
    }
   ],
   "source": [
    "file_list1 = pd.read_csv(splitsDir+'train.csv',names=['filename'])['filename'].values.astype('str')\n",
    "file_list = []; nfeatures = -1; dtype = -1\n",
    "for iff,f in enumerate(file_list1):\n",
    "    f = f.rstrip('.xml').split('/')[-1]\n",
    "    if iff==0: # if first one, fill in the gaps\n",
    "        fg = glob.glob(classDir_main_to_imgs + f + '*')[0]\n",
    "        mod = fg.split('/')[-1]\n",
    "        mod = mod[mod.rfind('.')+1:]\n",
    "        # read in and keep images -- npy files/npz\n",
    "        if mod == 'npz':\n",
    "            with np.load(fg) as b:\n",
    "                nfeatures = b['arr_0'].shape[-1]\n",
    "                dtype = b['arr_0'].dtype\n",
    "            header_offset = npy_header_offset(fg)\n",
    "        elif mod == 'npy':\n",
    "            b=np.load(fg)\n",
    "            if type(b) != np.ndarray:\n",
    "                b = b['arr_0'] # WHY????\n",
    "                nfeatures = b.shape[-1]\n",
    "                dtype = b.dtype\n",
    "        elif mod == 'pickle':\n",
    "            print('not implemented!')\n",
    "            import sys; sys.exit()\n",
    "            with open(fg, 'rb') as ff:\n",
    "                b = pickle.load(ff) \n",
    "                nfeatures = np.array(b).shape[-1]\n",
    "                dtype = np.array(b).dtype\n",
    "        else:\n",
    "            print('no idea what this method is')\n",
    "        # what is dtype?\n",
    "        if dtype == np.dtype('uint8'): dtype = tf.uint8\n",
    "        if dtype == np.dtype('float64'): dtype = tf.float64\n",
    "    file_list.append(classDir_main_to_imgs+f+'.'+mod)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([], 12, dtype('uint8'))"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# alright maybe try recordio afterall?\n",
    "#https://github.com/tensorflow/tensorflow/blob/r1.3/tensorflow/examples/how_tos/reading_data/convert_to_records.py\n",
    "#https://stackoverflow.com/questions/50665144/create-tfrecord-for-object-detection-task\n",
    "#https://stackoverflow.com/questions/46820500/how-to-handle-large-amouts-of-data-in-tensorflow/47040165#47040165"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "recordIOdir = config.tmp_storage_dir + 'rio/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "def array_to_tfrecords(X, boxes, output_file):\n",
    "    if len(boxes)>0:\n",
    "        x1 = boxes[0][:,0]; y1 = boxes[0][:,1]; x2 = boxes[0][:,2]; y2 = boxes[0][:,3]\n",
    "        classes = boxes[0][:,4]\n",
    "    else:\n",
    "        x1=np.array([]);y1=np.array([]);x2=np.array([]);y2=np.array([]);classes=np.array([])\n",
    "    # do division already\n",
    "    X = X/255.\n",
    "    feature = {\n",
    "        'X': tf.train.Feature(float_list=tf.train.FloatList(value=X.flatten())),\n",
    "        'x1': tf.train.Feature(float_list=tf.train.FloatList(value=x1.flatten())),\n",
    "        'y1': tf.train.Feature(float_list=tf.train.FloatList(value=y1.flatten())),\n",
    "        'x2': tf.train.Feature(float_list=tf.train.FloatList(value=x2.flatten())),\n",
    "        'y2': tf.train.Feature(float_list=tf.train.FloatList(value=y2.flatten())),\n",
    "        'class': tf.train.Feature(float_list=tf.train.FloatList(value=classes.flatten()))\n",
    "    }\n",
    "    example = tf.train.Example(features=tf.train.Features(feature=feature))\n",
    "    serialized = example.SerializeToString()\n",
    "\n",
    "    writer = tf.io.TFRecordWriter(output_file)\n",
    "    writer.write(serialized)\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imgs_name, bbox = parse_annotation([classDir_main_to+f+'.xml'], LABELS, \n",
    "#                                                    feature_dir=classDir_main_to_imgs,\n",
    "#                                                    annotation_dir=classDir_main_to)\n",
    "\n",
    "# X = np.load(imgs_name[0])['arr_0']\n",
    "# array_to_tfrecords(X, bbox, recordIOdir+imgs_name[0].rstrip('.npz').split('/')[-1]+'.tfrecord')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 of 5515\n",
      "100 of 5515\n",
      "200 of 5515\n",
      "300 of 5515\n",
      "400 of 5515\n",
      "500 of 5515\n",
      "600 of 5515\n",
      "700 of 5515\n",
      "800 of 5515\n",
      "900 of 5515\n",
      "1000 of 5515\n",
      "1100 of 5515\n",
      "1200 of 5515\n",
      "1300 of 5515\n",
      "1400 of 5515\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/t5/9xgccmv92hnfvjwd62mk8zqh0000gn/T/ipykernel_25922/913782921.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimgs_name\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'arr_0'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0marray_to_tfrecords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbbox\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecordIOdir\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mimgs_name\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.npz'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'.tfrecord'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0;32mdel\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/t5/9xgccmv92hnfvjwd62mk8zqh0000gn/T/ipykernel_25922/1937598323.py\u001b[0m in \u001b[0;36marray_to_tfrecords\u001b[0;34m(X, boxes, output_file)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m255.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     feature = {\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0;34m'X'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFeature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfloat_list\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatList\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0;34m'x1'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFeature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfloat_list\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatList\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;34m'y1'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFeature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfloat_list\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatList\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# convert all\n",
    "for iff,f in enumerate(annotations):\n",
    "    imgs_name, bbox = parse_annotation([classDir_main_to+f.split('/')[-1]], LABELS, \n",
    "                                                       feature_dir=classDir_main_to_imgs,\n",
    "                                                       annotation_dir=classDir_main_to)\n",
    "\n",
    "    X = np.load(imgs_name[0])['arr_0']\n",
    "    array_to_tfrecords(X, bbox, recordIOdir+imgs_name[0].rstrip('.npz').split('/')[-1]+'.tfrecord')\n",
    "    del X\n",
    "    \n",
    "    if iff%100==0: print(iff,'of',len(annotations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5515"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(annotations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['/Users/jillnaiman/tmpMegaYolo/binaries/yolo_512x512_cap_ann/1952ApJ___115___82K_p1.xml',\n",
       "       '/Users/jillnaiman/tmpMegaYolo/binaries/yolo_512x512_cap_ann/1994ApJ___427L__43F_p4.xml',\n",
       "       '/Users/jillnaiman/tmpMegaYolo/binaries/yolo_512x512_cap_ann/1995ApJ___445__246C_p6.xml',\n",
       "       ...,\n",
       "       '/Users/jillnaiman/tmpMegaYolo/binaries/yolo_512x512_cap_ann/1978ApJ___220___14H_p1.xml',\n",
       "       '/Users/jillnaiman/tmpMegaYolo/binaries/yolo_512x512_cap_ann/1982ApJ___252__269Y_p2.xml',\n",
       "       '/Users/jillnaiman/tmpMegaYolo/binaries/yolo_512x512_cap_ann/1982ApJ___258__467Y_p2.xml'],\n",
       "      dtype='<U88')"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_list1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parse CSV: 6.593299985979684e-05\n",
      "parse CSV: 7.796400132065173e-05\n",
      "Parse annotation: 0.14502690299923415\n",
      "Parse annotation: 0.14677225699961127\n",
      "Load data: 0.12156924800001434\n",
      "Load data: 0.11779988200032676\n",
      "process blocks:process blocks: 0.1032422560001578\n",
      " 0.10012778300006175\n",
      "0\n",
      "parse CSV:parse CSV: 0.00013102500088280067\n",
      " 5.655000131810084e-05\n",
      "1\n",
      "Parse annotation: 0.15298127499954717\n",
      "Parse annotation: 0.15567094499965606\n",
      "Load data: 0.12399737299892877\n",
      "Load data: 0.12808158799998637\n",
      "process blocks: process blocks: 0.12697629399917787\n",
      "0.1203326869999728\n",
      "2parse CSV:parse CSV: 5.4875999921932817e-05\n",
      " 6.429899985960219e-05\n",
      "\n",
      "3\n",
      "Parse annotation: 0.14686419899953762\n",
      "Parse annotation: 0.16171898799984774\n",
      "Load data: 0.1252131900000677\n",
      "Load data: 0.12484830600078567\n",
      "process blocks:process blocks:  0.13700858399897697\n",
      "0.12357732499913254\n",
      "4parse CSV:parse CSV: 6.186500104377046e-05\n",
      "\n",
      " 8.753600013733376e-05\n",
      "5\n",
      "Parse annotation: 0.14494591199945717\n",
      "Parse annotation: 0.14763279299950227\n",
      "Load data: 0.1285129509997205\n",
      "Load data: 0.1277994949996355\n",
      "process blocks:process blocks: 0.12277748500127927\n",
      " 0.12455471900102566\n",
      "parse CSV: 6.833099905634299e-05\n",
      "parse CSV:6 \n",
      "7.652000022062566e-05\n",
      "7\n",
      "Parse annotation: 0.4687751359997492\n",
      "Parse annotation: 0.172946922999472\n",
      "Load data: 0.13505229000111285\n",
      "Load data: 0.1318750820009882\n",
      "process blocks: process blocks: 0.1395990349992644\n",
      "0.1316464569990785\n",
      "8\n",
      "parse CSV: 7.178299892984796e-05\n",
      "9parse CSV:\n",
      " 0.0015495119987463113\n",
      "Parse annotation:Parse annotation:  0.18537510900023335\n",
      "0.18108187100006035\n",
      "Load data: 0.13149454800077365\n",
      "Load data: 0.12941067999963707\n",
      "process blocks:process blocks: 0.12608493399966392\n",
      " 0.1266717679991416\n",
      "parse CSV:10parse CSV: 6.294599916145671e-05\n",
      "\n",
      " 7.425900002999697e-05\n",
      "Parse annotation: 0.14292050900075992\n",
      "Parse annotation: 0.13754443200014066\n",
      "Load data: 0.18013043599967204\n",
      "Load data: 0.20016973999918264\n",
      "process blocks: 0.10201240500100539\n",
      "process blocks: 0.4326179309991858\n",
      "Execution time: 7.084822689001157\n"
     ]
    }
   ],
   "source": [
    "benchmark(\n",
    "    tf.data.Dataset.range(2)\n",
    "    .interleave(\n",
    "        lambda _: ArtificialDatasetNonAug(batch_size=nbatch,split='valid'),\n",
    "        num_parallel_calls=tf.data.AUTOTUNE\n",
    "    )#.shuffle(3)\n",
    "    , nbatch=nbatch\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m\n",
       "\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterleave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmap_func\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mcycle_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mblock_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mnum_parallel_calls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mdeterministic\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "Maps `map_func` across this dataset, and interleaves the results.\n",
       "\n",
       "For example, you can use `Dataset.interleave()` to process many input files\n",
       "concurrently:\n",
       "\n",
       ">>> # Preprocess 4 files concurrently, and interleave blocks of 16 records\n",
       ">>> # from each file.\n",
       ">>> filenames = [\"/var/data/file1.txt\", \"/var/data/file2.txt\",\n",
       "...              \"/var/data/file3.txt\", \"/var/data/file4.txt\"]\n",
       ">>> dataset = tf.data.Dataset.from_tensor_slices(filenames)\n",
       ">>> def parse_fn(filename):\n",
       "...   return tf.data.Dataset.range(10)\n",
       ">>> dataset = dataset.interleave(lambda x:\n",
       "...     tf.data.TextLineDataset(x).map(parse_fn, num_parallel_calls=1),\n",
       "...     cycle_length=4, block_length=16)\n",
       "\n",
       "The `cycle_length` and `block_length` arguments control the order in which\n",
       "elements are produced. `cycle_length` controls the number of input elements\n",
       "that are processed concurrently. If you set `cycle_length` to 1, this\n",
       "transformation will handle one input element at a time, and will produce\n",
       "identical results to `tf.data.Dataset.flat_map`. In general,\n",
       "this transformation will apply `map_func` to `cycle_length` input elements,\n",
       "open iterators on the returned `Dataset` objects, and cycle through them\n",
       "producing `block_length` consecutive elements from each iterator, and\n",
       "consuming the next input element each time it reaches the end of an\n",
       "iterator.\n",
       "\n",
       "For example:\n",
       "\n",
       ">>> dataset = Dataset.range(1, 6)  # ==> [ 1, 2, 3, 4, 5 ]\n",
       ">>> # NOTE: New lines indicate \"block\" boundaries.\n",
       ">>> dataset = dataset.interleave(\n",
       "...     lambda x: Dataset.from_tensors(x).repeat(6),\n",
       "...     cycle_length=2, block_length=4)\n",
       ">>> list(dataset.as_numpy_iterator())\n",
       "[1, 1, 1, 1,\n",
       " 2, 2, 2, 2,\n",
       " 1, 1,\n",
       " 2, 2,\n",
       " 3, 3, 3, 3,\n",
       " 4, 4, 4, 4,\n",
       " 3, 3,\n",
       " 4, 4,\n",
       " 5, 5, 5, 5,\n",
       " 5, 5]\n",
       "\n",
       "Note: The order of elements yielded by this transformation is\n",
       "deterministic, as long as `map_func` is a pure function and\n",
       "`deterministic=True`. If `map_func` contains any stateful operations, the\n",
       "order in which that state is accessed is undefined.\n",
       "\n",
       "Performance can often be improved by setting `num_parallel_calls` so that\n",
       "`interleave` will use multiple threads to fetch elements. If determinism\n",
       "isn't required, it can also improve performance to set\n",
       "`deterministic=False`.\n",
       "\n",
       ">>> filenames = [\"/var/data/file1.txt\", \"/var/data/file2.txt\",\n",
       "...              \"/var/data/file3.txt\", \"/var/data/file4.txt\"]\n",
       ">>> dataset = tf.data.Dataset.from_tensor_slices(filenames)\n",
       ">>> dataset = dataset.interleave(lambda x: tf.data.TFRecordDataset(x),\n",
       "...     cycle_length=4, num_parallel_calls=tf.data.AUTOTUNE,\n",
       "...     deterministic=False)\n",
       "\n",
       "Args:\n",
       "  map_func: A function mapping a dataset element to a dataset.\n",
       "  cycle_length: (Optional.) The number of input elements that will be\n",
       "    processed concurrently. If not set, the tf.data runtime decides what it\n",
       "    should be based on available CPU. If `num_parallel_calls` is set to\n",
       "    `tf.data.AUTOTUNE`, the `cycle_length` argument identifies\n",
       "    the maximum degree of parallelism.\n",
       "  block_length: (Optional.) The number of consecutive elements to produce\n",
       "    from each input element before cycling to another input element. If not\n",
       "    set, defaults to 1.\n",
       "  num_parallel_calls: (Optional.) If specified, the implementation creates a\n",
       "    threadpool, which is used to fetch inputs from cycle elements\n",
       "    asynchronously and in parallel. The default behavior is to fetch inputs\n",
       "    from cycle elements synchronously with no parallelism. If the value\n",
       "    `tf.data.AUTOTUNE` is used, then the number of parallel\n",
       "    calls is set dynamically based on available CPU.\n",
       "  deterministic: (Optional.) A boolean controlling whether determinism\n",
       "    should be traded for performance by allowing elements to be produced out\n",
       "    of order.  If `deterministic` is `None`, the\n",
       "    `tf.data.Options.experimental_deterministic` dataset option (`True` by\n",
       "    default) is used to decide whether to produce elements\n",
       "    deterministically.\n",
       "\n",
       "Returns:\n",
       "  Dataset: A `Dataset`.\n",
       "\u001b[0;31mFile:\u001b[0m      ~/anaconda3/envs/Paper1/lib/python3.7/site-packages/tensorflow/python/data/ops/dataset_ops.py\n",
       "\u001b[0;31mType:\u001b[0m      function\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tf.data.Dataset.interleave?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RRyluqn6vvaW"
   },
   "source": [
    "Steps per epoch -- training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1638305356592,
     "user": {
      "displayName": "Jill Naiman",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "13328008118965652761"
     },
     "user_tz": 360
    },
    "id": "SdEYPW-tvvaW",
    "outputId": "6c038327-6fe5-439a-c5be-9cedf4f87cec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps per epoch = training: 828 , validation: 164\n"
     ]
    }
   ],
   "source": [
    "#steps = len(X_train) // batch_size\n",
    "#print(len(X_train)//batch_size)\n",
    "# can do larger with augmentation\n",
    "\n",
    "aug_fac = 2 # 2 or 3\n",
    "\n",
    "steps_training = (len(X_train)//batch_size)*aug_fac\n",
    "# factor of 2 from: https://stackoverflow.com/questions/49922252/choosing-number-of-steps-per-epoch\n",
    "\n",
    "steps_val = (len(X_valid)//batch_size)*aug_fac\n",
    "\n",
    "print('Steps per epoch = training:', steps_training, ', validation:', steps_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sExlEUTwIYBy"
   },
   "source": [
    "Save also the names of the test instances to use later:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1638305356592,
     "user": {
      "displayName": "Jill Naiman",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "13328008118965652761"
     },
     "user_tz": 360
    },
    "id": "y4LLclWnIQ49",
    "outputId": "958a13f5-ddcb-4b0f-f31f-4f7dc9795857"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hey, saved tests!\n"
     ]
    }
   ],
   "source": [
    "# save test list in an extra place... this is a bit redundant since its saved another place too\n",
    "#if not re_run_from_splits:\n",
    "np.savetxt(saveFile, X_test, delimiter=',',fmt='%s')\n",
    "print('Hey, saved tests!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7gQi6kS8nea8"
   },
   "source": [
    "# 1. Define YOLO model\n",
    "\n",
    "For v5, see: https://github.com/jahongir7174/YOLOv5-tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zoBEY7n67jy1"
   },
   "source": [
    "For creating the model -- how many features are we using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1638305356593,
     "user": {
      "displayName": "Jill Naiman",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "13328008118965652761"
     },
     "user_tz": 360
    },
    "id": "ymfe7jyL7jy1",
    "outputId": "eb9c56e9-a90b-4bec-cd53-f2747631615b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_features = get_n_features(classDir_main_to_imgs)\n",
    "n_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "executionInfo": {
     "elapsed": 2663,
     "status": "ok",
     "timestamp": 1638305359251,
     "user": {
      "displayName": "Jill Naiman",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "13328008118965652761"
     },
     "user_tz": 360
    },
    "id": "KRQsM_X7XC-V"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-30 14:53:47.147006: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "version = 'l' # large version\n",
    "model = build_model(n_features, anchors, version, len(LABELS))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yXdkSQK9Emky"
   },
   "source": [
    "Build YOLOv5:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 276,
     "status": "ok",
     "timestamp": 1638305359522,
     "user": {
      "displayName": "Jill Naiman",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "13328008118965652761"
     },
     "user_tz": 360
    },
    "id": "0Vnf5mjn7jy2",
    "outputId": "6a1d77e9-e8ef-41d8-b1b3-d11ccad3c0fa",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 512, 512, 12 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.space_to_depth (TFOpLambd (None, 256, 256, 48) 0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 256, 256, 64) 27648       tf.nn.space_to_depth[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 256, 256, 64) 256         conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 256, 256, 64) 0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d (ZeroPadding2D)  (None, 257, 257, 64) 0           activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 128, 128, 128 73728       zero_padding2d[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 128, 128, 128 512         conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 128, 128, 128 0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 128, 128, 64) 8192        activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 128, 128, 64) 256         conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 128, 128, 64) 0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 128, 128, 64) 4096        activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 128, 128, 64) 256         conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 128, 128, 64) 0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 128, 128, 64) 36864       activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 128, 128, 64) 256         conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 128, 128, 64) 0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add (TFOpLambd (None, 128, 128, 64) 0           activation_2[0][0]               \n",
      "                                                                 activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 128, 128, 64) 4096        tf.__operators__.add[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 128, 128, 64) 256         conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 128, 128, 64) 0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 128, 128, 64) 36864       activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 128, 128, 64) 256         conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 128, 128, 64) 0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_1 (TFOpLam (None, 128, 128, 64) 0           tf.__operators__.add[0][0]       \n",
      "                                                                 activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 128, 128, 64) 4096        tf.__operators__.add_1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 128, 128, 64) 256         conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 128, 128, 64) 0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 128, 128, 64) 36864       activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 128, 128, 64) 8192        activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 128, 128, 64) 256         conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 128, 128, 64) 256         conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 128, 128, 64) 0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 128, 128, 64) 0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_2 (TFOpLam (None, 128, 128, 64) 0           tf.__operators__.add_1[0][0]     \n",
      "                                                                 activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 128, 128, 128 0           activation_9[0][0]               \n",
      "                                                                 tf.__operators__.add_2[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 128, 128, 128 16384       concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 128, 128, 128 512         conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 128, 128, 128 0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_1 (ZeroPadding2D (None, 129, 129, 128 0           activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 64, 64, 256)  294912      zero_padding2d_1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 64, 64, 256)  1024        conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 64, 64, 256)  0           batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 64, 64, 128)  32768       activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 64, 64, 128)  512         conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 64, 64, 128)  0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 64, 64, 128)  16384       activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 64, 64, 128)  512         conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 64, 64, 128)  0           batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 64, 64, 128)  147456      activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 64, 64, 128)  512         conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 64, 64, 128)  0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_3 (TFOpLam (None, 64, 64, 128)  0           activation_12[0][0]              \n",
      "                                                                 activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 64, 64, 128)  16384       tf.__operators__.add_3[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 64, 64, 128)  512         conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 64, 64, 128)  0           batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 64, 64, 128)  147456      activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 64, 64, 128)  512         conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 64, 64, 128)  0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_4 (TFOpLam (None, 64, 64, 128)  0           tf.__operators__.add_3[0][0]     \n",
      "                                                                 activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 64, 64, 128)  16384       tf.__operators__.add_4[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 64, 64, 128)  512         conv2d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 64, 64, 128)  0           batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, 64, 64, 128)  147456      activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNo (None, 64, 64, 128)  512         conv2d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 64, 64, 128)  0           batch_normalization_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_5 (TFOpLam (None, 64, 64, 128)  0           tf.__operators__.add_4[0][0]     \n",
      "                                                                 activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_19 (Conv2D)              (None, 64, 64, 128)  16384       tf.__operators__.add_5[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_19 (BatchNo (None, 64, 64, 128)  512         conv2d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, 64, 64, 128)  0           batch_normalization_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_20 (Conv2D)              (None, 64, 64, 128)  147456      activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_20 (BatchNo (None, 64, 64, 128)  512         conv2d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_20 (Activation)      (None, 64, 64, 128)  0           batch_normalization_20[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_6 (TFOpLam (None, 64, 64, 128)  0           tf.__operators__.add_5[0][0]     \n",
      "                                                                 activation_20[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_21 (Conv2D)              (None, 64, 64, 128)  16384       tf.__operators__.add_6[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_21 (BatchNo (None, 64, 64, 128)  512         conv2d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_21 (Activation)      (None, 64, 64, 128)  0           batch_normalization_21[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_22 (Conv2D)              (None, 64, 64, 128)  147456      activation_21[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_22 (BatchNo (None, 64, 64, 128)  512         conv2d_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_22 (Activation)      (None, 64, 64, 128)  0           batch_normalization_22[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_7 (TFOpLam (None, 64, 64, 128)  0           tf.__operators__.add_6[0][0]     \n",
      "                                                                 activation_22[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_23 (Conv2D)              (None, 64, 64, 128)  16384       tf.__operators__.add_7[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_23 (BatchNo (None, 64, 64, 128)  512         conv2d_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_23 (Activation)      (None, 64, 64, 128)  0           batch_normalization_23[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_24 (Conv2D)              (None, 64, 64, 128)  147456      activation_23[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_24 (BatchNo (None, 64, 64, 128)  512         conv2d_24[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_24 (Activation)      (None, 64, 64, 128)  0           batch_normalization_24[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_8 (TFOpLam (None, 64, 64, 128)  0           tf.__operators__.add_7[0][0]     \n",
      "                                                                 activation_24[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_25 (Conv2D)              (None, 64, 64, 128)  16384       tf.__operators__.add_8[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_25 (BatchNo (None, 64, 64, 128)  512         conv2d_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_25 (Activation)      (None, 64, 64, 128)  0           batch_normalization_25[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_26 (Conv2D)              (None, 64, 64, 128)  147456      activation_25[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_26 (BatchNo (None, 64, 64, 128)  512         conv2d_26[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_26 (Activation)      (None, 64, 64, 128)  0           batch_normalization_26[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_9 (TFOpLam (None, 64, 64, 128)  0           tf.__operators__.add_8[0][0]     \n",
      "                                                                 activation_26[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_27 (Conv2D)              (None, 64, 64, 128)  16384       tf.__operators__.add_9[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_27 (BatchNo (None, 64, 64, 128)  512         conv2d_27[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_27 (Activation)      (None, 64, 64, 128)  0           batch_normalization_27[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_28 (Conv2D)              (None, 64, 64, 128)  147456      activation_27[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_28 (BatchNo (None, 64, 64, 128)  512         conv2d_28[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_28 (Activation)      (None, 64, 64, 128)  0           batch_normalization_28[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_10 (TFOpLa (None, 64, 64, 128)  0           tf.__operators__.add_9[0][0]     \n",
      "                                                                 activation_28[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_29 (Conv2D)              (None, 64, 64, 128)  16384       tf.__operators__.add_10[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_29 (BatchNo (None, 64, 64, 128)  512         conv2d_29[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_29 (Activation)      (None, 64, 64, 128)  0           batch_normalization_29[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_30 (Conv2D)              (None, 64, 64, 128)  147456      activation_29[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_31 (Conv2D)              (None, 64, 64, 128)  32768       activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_30 (BatchNo (None, 64, 64, 128)  512         conv2d_30[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_31 (BatchNo (None, 64, 64, 128)  512         conv2d_31[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_30 (Activation)      (None, 64, 64, 128)  0           batch_normalization_30[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_31 (Activation)      (None, 64, 64, 128)  0           batch_normalization_31[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_11 (TFOpLa (None, 64, 64, 128)  0           tf.__operators__.add_10[0][0]    \n",
      "                                                                 activation_30[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 64, 64, 256)  0           activation_31[0][0]              \n",
      "                                                                 tf.__operators__.add_11[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_32 (Conv2D)              (None, 64, 64, 256)  65536       concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_32 (BatchNo (None, 64, 64, 256)  1024        conv2d_32[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_32 (Activation)      (None, 64, 64, 256)  0           batch_normalization_32[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_2 (ZeroPadding2D (None, 65, 65, 256)  0           activation_32[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_33 (Conv2D)              (None, 32, 32, 512)  1179648     zero_padding2d_2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_33 (BatchNo (None, 32, 32, 512)  2048        conv2d_33[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_33 (Activation)      (None, 32, 32, 512)  0           batch_normalization_33[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_34 (Conv2D)              (None, 32, 32, 256)  131072      activation_33[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_34 (BatchNo (None, 32, 32, 256)  1024        conv2d_34[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_34 (Activation)      (None, 32, 32, 256)  0           batch_normalization_34[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_35 (Conv2D)              (None, 32, 32, 256)  65536       activation_34[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_35 (BatchNo (None, 32, 32, 256)  1024        conv2d_35[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_35 (Activation)      (None, 32, 32, 256)  0           batch_normalization_35[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_36 (Conv2D)              (None, 32, 32, 256)  589824      activation_35[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_36 (BatchNo (None, 32, 32, 256)  1024        conv2d_36[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_36 (Activation)      (None, 32, 32, 256)  0           batch_normalization_36[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_12 (TFOpLa (None, 32, 32, 256)  0           activation_34[0][0]              \n",
      "                                                                 activation_36[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_37 (Conv2D)              (None, 32, 32, 256)  65536       tf.__operators__.add_12[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_37 (BatchNo (None, 32, 32, 256)  1024        conv2d_37[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_37 (Activation)      (None, 32, 32, 256)  0           batch_normalization_37[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_38 (Conv2D)              (None, 32, 32, 256)  589824      activation_37[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_38 (BatchNo (None, 32, 32, 256)  1024        conv2d_38[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_38 (Activation)      (None, 32, 32, 256)  0           batch_normalization_38[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_13 (TFOpLa (None, 32, 32, 256)  0           tf.__operators__.add_12[0][0]    \n",
      "                                                                 activation_38[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_39 (Conv2D)              (None, 32, 32, 256)  65536       tf.__operators__.add_13[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_39 (BatchNo (None, 32, 32, 256)  1024        conv2d_39[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_39 (Activation)      (None, 32, 32, 256)  0           batch_normalization_39[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_40 (Conv2D)              (None, 32, 32, 256)  589824      activation_39[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_40 (BatchNo (None, 32, 32, 256)  1024        conv2d_40[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_40 (Activation)      (None, 32, 32, 256)  0           batch_normalization_40[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_14 (TFOpLa (None, 32, 32, 256)  0           tf.__operators__.add_13[0][0]    \n",
      "                                                                 activation_40[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_41 (Conv2D)              (None, 32, 32, 256)  65536       tf.__operators__.add_14[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_41 (BatchNo (None, 32, 32, 256)  1024        conv2d_41[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_41 (Activation)      (None, 32, 32, 256)  0           batch_normalization_41[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_42 (Conv2D)              (None, 32, 32, 256)  589824      activation_41[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_42 (BatchNo (None, 32, 32, 256)  1024        conv2d_42[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_42 (Activation)      (None, 32, 32, 256)  0           batch_normalization_42[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_15 (TFOpLa (None, 32, 32, 256)  0           tf.__operators__.add_14[0][0]    \n",
      "                                                                 activation_42[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_43 (Conv2D)              (None, 32, 32, 256)  65536       tf.__operators__.add_15[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_43 (BatchNo (None, 32, 32, 256)  1024        conv2d_43[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_43 (Activation)      (None, 32, 32, 256)  0           batch_normalization_43[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_44 (Conv2D)              (None, 32, 32, 256)  589824      activation_43[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_44 (BatchNo (None, 32, 32, 256)  1024        conv2d_44[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_44 (Activation)      (None, 32, 32, 256)  0           batch_normalization_44[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_16 (TFOpLa (None, 32, 32, 256)  0           tf.__operators__.add_15[0][0]    \n",
      "                                                                 activation_44[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_45 (Conv2D)              (None, 32, 32, 256)  65536       tf.__operators__.add_16[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_45 (BatchNo (None, 32, 32, 256)  1024        conv2d_45[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_45 (Activation)      (None, 32, 32, 256)  0           batch_normalization_45[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_46 (Conv2D)              (None, 32, 32, 256)  589824      activation_45[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_46 (BatchNo (None, 32, 32, 256)  1024        conv2d_46[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_46 (Activation)      (None, 32, 32, 256)  0           batch_normalization_46[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_17 (TFOpLa (None, 32, 32, 256)  0           tf.__operators__.add_16[0][0]    \n",
      "                                                                 activation_46[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_47 (Conv2D)              (None, 32, 32, 256)  65536       tf.__operators__.add_17[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_47 (BatchNo (None, 32, 32, 256)  1024        conv2d_47[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_47 (Activation)      (None, 32, 32, 256)  0           batch_normalization_47[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_48 (Conv2D)              (None, 32, 32, 256)  589824      activation_47[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_48 (BatchNo (None, 32, 32, 256)  1024        conv2d_48[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_48 (Activation)      (None, 32, 32, 256)  0           batch_normalization_48[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_18 (TFOpLa (None, 32, 32, 256)  0           tf.__operators__.add_17[0][0]    \n",
      "                                                                 activation_48[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_49 (Conv2D)              (None, 32, 32, 256)  65536       tf.__operators__.add_18[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_49 (BatchNo (None, 32, 32, 256)  1024        conv2d_49[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_49 (Activation)      (None, 32, 32, 256)  0           batch_normalization_49[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_50 (Conv2D)              (None, 32, 32, 256)  589824      activation_49[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_50 (BatchNo (None, 32, 32, 256)  1024        conv2d_50[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_50 (Activation)      (None, 32, 32, 256)  0           batch_normalization_50[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_19 (TFOpLa (None, 32, 32, 256)  0           tf.__operators__.add_18[0][0]    \n",
      "                                                                 activation_50[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_51 (Conv2D)              (None, 32, 32, 256)  65536       tf.__operators__.add_19[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_51 (BatchNo (None, 32, 32, 256)  1024        conv2d_51[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_51 (Activation)      (None, 32, 32, 256)  0           batch_normalization_51[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_52 (Conv2D)              (None, 32, 32, 256)  589824      activation_51[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_53 (Conv2D)              (None, 32, 32, 256)  131072      activation_33[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_52 (BatchNo (None, 32, 32, 256)  1024        conv2d_52[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_53 (BatchNo (None, 32, 32, 256)  1024        conv2d_53[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_52 (Activation)      (None, 32, 32, 256)  0           batch_normalization_52[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_53 (Activation)      (None, 32, 32, 256)  0           batch_normalization_53[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_20 (TFOpLa (None, 32, 32, 256)  0           tf.__operators__.add_19[0][0]    \n",
      "                                                                 activation_52[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 32, 32, 512)  0           activation_53[0][0]              \n",
      "                                                                 tf.__operators__.add_20[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_54 (Conv2D)              (None, 32, 32, 512)  262144      concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_54 (BatchNo (None, 32, 32, 512)  2048        conv2d_54[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_54 (Activation)      (None, 32, 32, 512)  0           batch_normalization_54[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_3 (ZeroPadding2D (None, 33, 33, 512)  0           activation_54[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_55 (Conv2D)              (None, 16, 16, 1024) 4718592     zero_padding2d_3[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_55 (BatchNo (None, 16, 16, 1024) 4096        conv2d_55[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_55 (Activation)      (None, 16, 16, 1024) 0           batch_normalization_55[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_56 (Conv2D)              (None, 16, 16, 512)  524288      activation_55[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_56 (BatchNo (None, 16, 16, 512)  2048        conv2d_56[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_56 (Activation)      (None, 16, 16, 512)  0           batch_normalization_56[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.max_pool2d (TFOpLambda)   (None, 16, 16, 512)  0           activation_56[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.max_pool2d_1 (TFOpLambda) (None, 16, 16, 512)  0           activation_56[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.max_pool2d_2 (TFOpLambda) (None, 16, 16, 512)  0           activation_56[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 16, 16, 2048) 0           activation_56[0][0]              \n",
      "                                                                 tf.nn.max_pool2d[0][0]           \n",
      "                                                                 tf.nn.max_pool2d_1[0][0]         \n",
      "                                                                 tf.nn.max_pool2d_2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_57 (Conv2D)              (None, 16, 16, 1024) 2097152     concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_57 (BatchNo (None, 16, 16, 1024) 4096        conv2d_57[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_57 (Activation)      (None, 16, 16, 1024) 0           batch_normalization_57[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_58 (Conv2D)              (None, 16, 16, 512)  524288      activation_57[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_58 (BatchNo (None, 16, 16, 512)  2048        conv2d_58[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_58 (Activation)      (None, 16, 16, 512)  0           batch_normalization_58[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_59 (Conv2D)              (None, 16, 16, 512)  262144      activation_58[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_59 (BatchNo (None, 16, 16, 512)  2048        conv2d_59[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_59 (Activation)      (None, 16, 16, 512)  0           batch_normalization_59[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_60 (Conv2D)              (None, 16, 16, 512)  2359296     activation_59[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_60 (BatchNo (None, 16, 16, 512)  2048        conv2d_60[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_60 (Activation)      (None, 16, 16, 512)  0           batch_normalization_60[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_61 (Conv2D)              (None, 16, 16, 512)  262144      activation_60[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_61 (BatchNo (None, 16, 16, 512)  2048        conv2d_61[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_61 (Activation)      (None, 16, 16, 512)  0           batch_normalization_61[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_62 (Conv2D)              (None, 16, 16, 512)  2359296     activation_61[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_62 (BatchNo (None, 16, 16, 512)  2048        conv2d_62[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_62 (Activation)      (None, 16, 16, 512)  0           batch_normalization_62[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_63 (Conv2D)              (None, 16, 16, 512)  262144      activation_62[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_63 (BatchNo (None, 16, 16, 512)  2048        conv2d_63[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_63 (Activation)      (None, 16, 16, 512)  0           batch_normalization_63[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_65 (Conv2D)              (None, 16, 16, 512)  524288      activation_57[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_64 (Conv2D)              (None, 16, 16, 512)  2359296     activation_63[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_65 (BatchNo (None, 16, 16, 512)  2048        conv2d_65[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_64 (BatchNo (None, 16, 16, 512)  2048        conv2d_64[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_65 (Activation)      (None, 16, 16, 512)  0           batch_normalization_65[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_64 (Activation)      (None, 16, 16, 512)  0           batch_normalization_64[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 16, 16, 1024) 0           activation_65[0][0]              \n",
      "                                                                 activation_64[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_66 (Conv2D)              (None, 16, 16, 1024) 1048576     concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_66 (BatchNo (None, 16, 16, 1024) 4096        conv2d_66[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_66 (Activation)      (None, 16, 16, 1024) 0           batch_normalization_66[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_67 (Conv2D)              (None, 16, 16, 512)  524288      activation_66[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_67 (BatchNo (None, 16, 16, 512)  2048        conv2d_67[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_67 (Activation)      (None, 16, 16, 512)  0           batch_normalization_67[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d (UpSampling2D)    (None, 32, 32, 512)  0           activation_67[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 32, 32, 1024) 0           up_sampling2d[0][0]              \n",
      "                                                                 activation_54[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_68 (Conv2D)              (None, 32, 32, 256)  262144      concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_68 (BatchNo (None, 32, 32, 256)  1024        conv2d_68[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_68 (Activation)      (None, 32, 32, 256)  0           batch_normalization_68[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_69 (Conv2D)              (None, 32, 32, 256)  65536       activation_68[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_69 (BatchNo (None, 32, 32, 256)  1024        conv2d_69[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_69 (Activation)      (None, 32, 32, 256)  0           batch_normalization_69[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_70 (Conv2D)              (None, 32, 32, 256)  589824      activation_69[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_70 (BatchNo (None, 32, 32, 256)  1024        conv2d_70[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_70 (Activation)      (None, 32, 32, 256)  0           batch_normalization_70[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_71 (Conv2D)              (None, 32, 32, 256)  65536       activation_70[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_71 (BatchNo (None, 32, 32, 256)  1024        conv2d_71[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_71 (Activation)      (None, 32, 32, 256)  0           batch_normalization_71[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_72 (Conv2D)              (None, 32, 32, 256)  589824      activation_71[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_72 (BatchNo (None, 32, 32, 256)  1024        conv2d_72[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_72 (Activation)      (None, 32, 32, 256)  0           batch_normalization_72[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_73 (Conv2D)              (None, 32, 32, 256)  65536       activation_72[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_73 (BatchNo (None, 32, 32, 256)  1024        conv2d_73[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_73 (Activation)      (None, 32, 32, 256)  0           batch_normalization_73[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_75 (Conv2D)              (None, 32, 32, 256)  262144      concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_74 (Conv2D)              (None, 32, 32, 256)  589824      activation_73[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_75 (BatchNo (None, 32, 32, 256)  1024        conv2d_75[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_74 (BatchNo (None, 32, 32, 256)  1024        conv2d_74[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_75 (Activation)      (None, 32, 32, 256)  0           batch_normalization_75[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_74 (Activation)      (None, 32, 32, 256)  0           batch_normalization_74[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 32, 32, 512)  0           activation_75[0][0]              \n",
      "                                                                 activation_74[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_76 (Conv2D)              (None, 32, 32, 512)  262144      concatenate_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_76 (BatchNo (None, 32, 32, 512)  2048        conv2d_76[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_76 (Activation)      (None, 32, 32, 512)  0           batch_normalization_76[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_77 (Conv2D)              (None, 32, 32, 256)  131072      activation_76[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_77 (BatchNo (None, 32, 32, 256)  1024        conv2d_77[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_77 (Activation)      (None, 32, 32, 256)  0           batch_normalization_77[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_1 (UpSampling2D)  (None, 64, 64, 256)  0           activation_77[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)     (None, 64, 64, 512)  0           up_sampling2d_1[0][0]            \n",
      "                                                                 activation_32[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_78 (Conv2D)              (None, 64, 64, 128)  65536       concatenate_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_78 (BatchNo (None, 64, 64, 128)  512         conv2d_78[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_78 (Activation)      (None, 64, 64, 128)  0           batch_normalization_78[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_79 (Conv2D)              (None, 64, 64, 128)  16384       activation_78[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_79 (BatchNo (None, 64, 64, 128)  512         conv2d_79[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_79 (Activation)      (None, 64, 64, 128)  0           batch_normalization_79[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_80 (Conv2D)              (None, 64, 64, 128)  147456      activation_79[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_80 (BatchNo (None, 64, 64, 128)  512         conv2d_80[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_80 (Activation)      (None, 64, 64, 128)  0           batch_normalization_80[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_81 (Conv2D)              (None, 64, 64, 128)  16384       activation_80[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_81 (BatchNo (None, 64, 64, 128)  512         conv2d_81[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_81 (Activation)      (None, 64, 64, 128)  0           batch_normalization_81[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_82 (Conv2D)              (None, 64, 64, 128)  147456      activation_81[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_82 (BatchNo (None, 64, 64, 128)  512         conv2d_82[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_82 (Activation)      (None, 64, 64, 128)  0           batch_normalization_82[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_83 (Conv2D)              (None, 64, 64, 128)  16384       activation_82[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_83 (BatchNo (None, 64, 64, 128)  512         conv2d_83[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_83 (Activation)      (None, 64, 64, 128)  0           batch_normalization_83[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_85 (Conv2D)              (None, 64, 64, 128)  65536       concatenate_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_84 (Conv2D)              (None, 64, 64, 128)  147456      activation_83[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_85 (BatchNo (None, 64, 64, 128)  512         conv2d_85[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_84 (BatchNo (None, 64, 64, 128)  512         conv2d_84[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_85 (Activation)      (None, 64, 64, 128)  0           batch_normalization_85[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_84 (Activation)      (None, 64, 64, 128)  0           batch_normalization_84[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_8 (Concatenate)     (None, 64, 64, 256)  0           activation_85[0][0]              \n",
      "                                                                 activation_84[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_86 (Conv2D)              (None, 64, 64, 256)  65536       concatenate_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_86 (BatchNo (None, 64, 64, 256)  1024        conv2d_86[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_86 (Activation)      (None, 64, 64, 256)  0           batch_normalization_86[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_4 (ZeroPadding2D (None, 65, 65, 256)  0           activation_86[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_87 (Conv2D)              (None, 32, 32, 256)  589824      zero_padding2d_4[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_87 (BatchNo (None, 32, 32, 256)  1024        conv2d_87[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_87 (Activation)      (None, 32, 32, 256)  0           batch_normalization_87[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_9 (Concatenate)     (None, 32, 32, 512)  0           activation_87[0][0]              \n",
      "                                                                 activation_77[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_88 (Conv2D)              (None, 32, 32, 256)  131072      concatenate_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_88 (BatchNo (None, 32, 32, 256)  1024        conv2d_88[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_88 (Activation)      (None, 32, 32, 256)  0           batch_normalization_88[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_89 (Conv2D)              (None, 32, 32, 256)  65536       activation_88[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_89 (BatchNo (None, 32, 32, 256)  1024        conv2d_89[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_89 (Activation)      (None, 32, 32, 256)  0           batch_normalization_89[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_90 (Conv2D)              (None, 32, 32, 256)  589824      activation_89[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_90 (BatchNo (None, 32, 32, 256)  1024        conv2d_90[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_90 (Activation)      (None, 32, 32, 256)  0           batch_normalization_90[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_91 (Conv2D)              (None, 32, 32, 256)  65536       activation_90[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_91 (BatchNo (None, 32, 32, 256)  1024        conv2d_91[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_91 (Activation)      (None, 32, 32, 256)  0           batch_normalization_91[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_92 (Conv2D)              (None, 32, 32, 256)  589824      activation_91[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_92 (BatchNo (None, 32, 32, 256)  1024        conv2d_92[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_92 (Activation)      (None, 32, 32, 256)  0           batch_normalization_92[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_93 (Conv2D)              (None, 32, 32, 256)  65536       activation_92[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_93 (BatchNo (None, 32, 32, 256)  1024        conv2d_93[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_93 (Activation)      (None, 32, 32, 256)  0           batch_normalization_93[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_95 (Conv2D)              (None, 32, 32, 256)  131072      concatenate_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_94 (Conv2D)              (None, 32, 32, 256)  589824      activation_93[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_95 (BatchNo (None, 32, 32, 256)  1024        conv2d_95[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_94 (BatchNo (None, 32, 32, 256)  1024        conv2d_94[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_95 (Activation)      (None, 32, 32, 256)  0           batch_normalization_95[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_94 (Activation)      (None, 32, 32, 256)  0           batch_normalization_94[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_10 (Concatenate)    (None, 32, 32, 512)  0           activation_95[0][0]              \n",
      "                                                                 activation_94[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_96 (Conv2D)              (None, 32, 32, 512)  262144      concatenate_10[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_96 (BatchNo (None, 32, 32, 512)  2048        conv2d_96[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_96 (Activation)      (None, 32, 32, 512)  0           batch_normalization_96[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_5 (ZeroPadding2D (None, 33, 33, 512)  0           activation_96[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_97 (Conv2D)              (None, 16, 16, 512)  2359296     zero_padding2d_5[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_97 (BatchNo (None, 16, 16, 512)  2048        conv2d_97[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_97 (Activation)      (None, 16, 16, 512)  0           batch_normalization_97[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_11 (Concatenate)    (None, 16, 16, 1024) 0           activation_97[0][0]              \n",
      "                                                                 activation_67[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_98 (Conv2D)              (None, 16, 16, 512)  524288      concatenate_11[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_98 (BatchNo (None, 16, 16, 512)  2048        conv2d_98[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_98 (Activation)      (None, 16, 16, 512)  0           batch_normalization_98[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_99 (Conv2D)              (None, 16, 16, 512)  262144      activation_98[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_99 (BatchNo (None, 16, 16, 512)  2048        conv2d_99[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_99 (Activation)      (None, 16, 16, 512)  0           batch_normalization_99[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_100 (Conv2D)             (None, 16, 16, 512)  2359296     activation_99[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_100 (BatchN (None, 16, 16, 512)  2048        conv2d_100[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_100 (Activation)     (None, 16, 16, 512)  0           batch_normalization_100[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_101 (Conv2D)             (None, 16, 16, 512)  262144      activation_100[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_101 (BatchN (None, 16, 16, 512)  2048        conv2d_101[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_101 (Activation)     (None, 16, 16, 512)  0           batch_normalization_101[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_102 (Conv2D)             (None, 16, 16, 512)  2359296     activation_101[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_102 (BatchN (None, 16, 16, 512)  2048        conv2d_102[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_102 (Activation)     (None, 16, 16, 512)  0           batch_normalization_102[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_103 (Conv2D)             (None, 16, 16, 512)  262144      activation_102[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_103 (BatchN (None, 16, 16, 512)  2048        conv2d_103[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_103 (Activation)     (None, 16, 16, 512)  0           batch_normalization_103[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_105 (Conv2D)             (None, 16, 16, 512)  524288      concatenate_11[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_104 (Conv2D)             (None, 16, 16, 512)  2359296     activation_103[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_105 (BatchN (None, 16, 16, 512)  2048        conv2d_105[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_104 (BatchN (None, 16, 16, 512)  2048        conv2d_104[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_105 (Activation)     (None, 16, 16, 512)  0           batch_normalization_105[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_104 (Activation)     (None, 16, 16, 512)  0           batch_normalization_104[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_12 (Concatenate)    (None, 16, 16, 1024) 0           activation_105[0][0]             \n",
      "                                                                 activation_104[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_106 (Conv2D)             (None, 16, 16, 1024) 1048576     concatenate_12[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_106 (BatchN (None, 16, 16, 1024) 4096        conv2d_106[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_106 (Activation)     (None, 16, 16, 1024) 0           batch_normalization_106[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "p5_4 (Conv2D)                   (None, 16, 16, 27)   27675       activation_106[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "p4_4 (Conv2D)                   (None, 32, 32, 27)   13851       activation_96[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "p3_4 (Conv2D)                   (None, 64, 64, 27)   6939        activation_86[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 46,729,809\n",
      "Trainable params: 46,668,241\n",
      "Non-trainable params: 61,568\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "executionInfo": {
     "elapsed": 34,
     "status": "ok",
     "timestamp": 1638305359523,
     "user": {
      "displayName": "Jill Naiman",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "13328008118965652761"
     },
     "user_tz": 360
    },
    "id": "6tXmyitL7jy2"
   },
   "outputs": [],
   "source": [
    "# plot if you wanna\n",
    "#tf.keras.utils.plot_model(model_v5, \"yolo_v5.png\", show_shapes=True, show_layer_names=True, expand_nested=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AS4ZcPQg7jy2"
   },
   "source": [
    "For optimizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "executionInfo": {
     "elapsed": 34,
     "status": "ok",
     "timestamp": 1638305359523,
     "user": {
      "displayName": "Jill Naiman",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "13328008118965652761"
     },
     "user_tz": 360
    },
    "id": "IWLTDSz77jy2"
   },
   "outputs": [],
   "source": [
    "#LRrate = 0.004\n",
    "LRrate = 0.002\n",
    "\n",
    "class CosineLR(tf.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self,steps):\n",
    "        super().__init__()\n",
    "        self.lr = LRrate * batch_size / 64\n",
    "        self.warmup_init = LRrate/10.\n",
    "        self.warmup_step = steps\n",
    "        self.decay_steps = tf.cast((num_epochs - 1) * self.warmup_step, tf.float32)\n",
    "\n",
    "    def __call__(self, step):\n",
    "        linear_warmup = tf.cast(step, dtype=tf.float32) / self.warmup_step * (self.lr - self.warmup_init)\n",
    "        cosine_lr = 0.5 * self.lr * (1 + tf.cos(math.pi * tf.cast(step, tf.float32) / self.decay_steps))\n",
    "        return tf.where(step < self.warmup_step, self.warmup_init + linear_warmup, cosine_lr)\n",
    "\n",
    "    def get_config(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "executionInfo": {
     "elapsed": 34,
     "status": "ok",
     "timestamp": 1638305359524,
     "user": {
      "displayName": "Jill Naiman",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "13328008118965652761"
     },
     "user_tz": 360
    },
    "id": "JpreDZDZvvaW"
   },
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(CosineLR(steps_training), 0.937)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "executionInfo": {
     "elapsed": 34,
     "status": "ok",
     "timestamp": 1638305359524,
     "user": {
      "displayName": "Jill Naiman",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "13328008118965652761"
     },
     "user_tz": 360
    },
    "id": "_l3BLK6XDRms"
   },
   "outputs": [],
   "source": [
    "# load weights if you wanna\n",
    "if saved_weights_file is not None:\n",
    "    weightsFiles = glob.glob(weightsDir + 'weights/' + '*h5')\n",
    "    # OR\n",
    "    if saved_weights_file is not None:\n",
    "      weightsFiles = [classDirMain+saved_weights_file]\n",
    "    model.load_weights(weightsFiles[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 34,
     "status": "ok",
     "timestamp": 1638305359524,
     "user": {
      "displayName": "Jill Naiman",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "13328008118965652761"
     },
     "user_tz": 360
    },
    "id": "3MAgw3DHD_nA",
    "outputId": "df181c03-e403-4537-e6fc-05fc98946489"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0003125"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print(weightsFiles)\n",
    "optimizer.learning_rate.lr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e6Y0CbKb0T76"
   },
   "source": [
    "For saving checkpoints:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1638305359525,
     "user": {
      "displayName": "Jill Naiman",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "13328008118965652761"
     },
     "user_tz": 360
    },
    "id": "REcMXF7W0T76"
   },
   "outputs": [],
   "source": [
    "# for saving model\n",
    "#save_model_name = chksDir + 'checkpoints/'+'model' + str(DATE.today().year).zfill(4) + str(DATE.today().month).zfill(2) + str(DATE.today().day).zfill(2) +'.h5'\n",
    "#checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(save_model_name, save_best_only=True)\n",
    "# today = str(DATE.today().year).zfill(4) + str(DATE.today().month).zfill(2) + str(DATE.today().day).zfill(2)\n",
    "# if not os.path.exists(chksDir + 'checkpoints/'+today):\n",
    "#     os.mkdir(chksDir + 'checkpoints/'+today)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1638305359525,
     "user": {
      "displayName": "Jill Naiman",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "13328008118965652761"
     },
     "user_tz": 360
    },
    "id": "h2o2DlWC0T77"
   },
   "outputs": [],
   "source": [
    "#model2 = tf.saved_model.load(save_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1638305359526,
     "user": {
      "displayName": "Jill Naiman",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "13328008118965652761"
     },
     "user_tz": 360
    },
    "id": "gBbSfeKP0T77"
   },
   "outputs": [],
   "source": [
    "# if restart_from_checkpoints:\n",
    "#     if saved_model_file is None:\n",
    "#         files = glob.glob(chksDir + 'checkpoints/*')\n",
    "#         files.sort()\n",
    "#         model = tf.saved_model.load(files[-1])\n",
    "#         fname = files[-1]\n",
    "#     else:\n",
    "#         model = tf.saved_model.load(chksDir + 'checkpoints/' +saved_model_file)\n",
    "#         fname = chksDir + 'checkpoints/' +saved_model_file\n",
    "#     print('Loading model from', fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RKk7HxgC7jy3"
   },
   "source": [
    "# For processing data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1638305359526,
     "user": {
      "displayName": "Jill Naiman",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "13328008118965652761"
     },
     "user_tz": 360
    },
    "id": "KFOuGj5l0T77",
    "outputId": "0060855f-3b00-47a4-b3bb-9f6eb4786ff2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/jillnaiman/MegaYolo/'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classDirMain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1638305359526,
     "user": {
      "displayName": "Jill Naiman",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "13328008118965652761"
     },
     "user_tz": 360
    },
    "id": "qqdxpzdoBb6H",
    "outputId": "04774847-7c14-4b6f-a99f-902d04403e71"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/Users/jillnaiman/MegaYolo/binaries_model8/',\n",
       " '/Users/jillnaiman/MegaYolo/yolo_512x512_ann/')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classDir_main_to_imgs, classDir_main_to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1638305359527,
     "user": {
      "displayName": "Jill Naiman",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "13328008118965652761"
     },
     "user_tz": 360
    },
    "id": "-L428Dy7VxBV"
   },
   "outputs": [],
   "source": [
    "#import mega_yolo_utils\n",
    "#reload(mega_yolo_utils)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1638305359527,
     "user": {
      "displayName": "Jill Naiman",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "13328008118965652761"
     },
     "user_tz": 360
    },
    "id": "RiM8WUcKGrFn"
   },
   "outputs": [],
   "source": [
    "from mega_yolo_utils import augmentation_generator, csv_gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1638305360505,
     "user": {
      "displayName": "Jill Naiman",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "13328008118965652761"
     },
     "user_tz": 360
    },
    "id": "ewFdU9-77jy3"
   },
   "outputs": [],
   "source": [
    "splitsDirIn = None\n",
    "if onGoogle: splitsDirIn = classDirMain\n",
    "train_gen_csv = csv_gen('train', splitsDir=splitsDirIn)\n",
    "valid_gen_csv = csv_gen('valid',splitsDir=splitsDirIn)\n",
    "test_gen_csv = csv_gen('test',splitsDir=splitsDirIn)\n",
    "\n",
    "\n",
    "def dataset_gen(split, batch_size):\n",
    "    while True:\n",
    "        true_boxes = []; imgs = []; files = []\n",
    "        \n",
    "        while len(files) < batch_size:\n",
    "            if type(split) == str:\n",
    "                if b'train' in bytes(split, encoding='utf8'): # NOTE, must be bytes here!!!\n",
    "                    line = next(train_gen_csv)\n",
    "                elif b'valid' in bytes(split, encoding='utf8'):\n",
    "                    line = next(valid_gen_csv)\n",
    "                elif b'test' in bytes(split, encoding='utf8'):\n",
    "                    line = next(test_gen_csv)\n",
    "            else:\n",
    "                if b'train' in split: # NOTE, must be bytes here!!!\n",
    "                    line = next(train_gen_csv)\n",
    "                elif b'valid' in split:\n",
    "                    line = next(valid_gen_csv)\n",
    "                elif b'test' in split:\n",
    "                    line = next(test_gen_csv)\n",
    "            \n",
    "            files.append(line.strip())\n",
    "\n",
    "        # parse and get full names\n",
    "        try:\n",
    "            imgs_name, bbox = parse_annotation(files, LABELS, \n",
    "                                               feature_dir=classDir_main_to_imgs,\n",
    "                                               annotation_dir=classDir_main_to)\n",
    "        except:\n",
    "            print('error parsing:', imgs_name)\n",
    "        # do a debug check\n",
    "        for im in imgs_name:\n",
    "            if '.npz' not in im:\n",
    "                print('no np file')\n",
    "                import sys; sys.exit()\n",
    "        \n",
    "        # read in and keep images -- npy files\n",
    "        for im in imgs_name:\n",
    "            b = np.load(im)['arr_0']\n",
    "            \n",
    "            ########### DEBUGGING ##########\n",
    "            #b = b[:,:,:3]\n",
    "            ################################\n",
    "            \n",
    "            # convert 0-1\n",
    "            b = b/255.0\n",
    "            imgs.append(b)\n",
    "        \n",
    "        # finally, format for output\n",
    "        y_true1, y_true2, y_true3 = [],[],[]\n",
    "        for b in bbox:\n",
    "            y1,y2,y3= process_box(b[:,:4], b[:,4].astype('int'),anchors,CLASS)\n",
    "            y_true1.append(y1); y_true2.append(y2); y_true3.append(y3)\n",
    "        # if there is no box, do something different\n",
    "        if len(bbox) == 0:\n",
    "            # fake a box\n",
    "            b = np.array([[111.,  59., 403., 364. ,  4.]])\n",
    "            y1,y2,y3= process_box(b[:,:4], b[:,4].astype('int'),anchors,CLASS)\n",
    "            y1[:] = 0; y2[:]=0;y3[:]=0\n",
    "            y_true1.append(y1); y_true2.append(y2); y_true3.append(y3)        \n",
    "        img = tf.cast(np.array(imgs), tf.float32)        \n",
    "        yield img, tf.cast(y_true1, tf.float32), tf.cast(y_true2, tf.float32), tf.cast(y_true3, tf.float32)\n",
    "        \n",
    "\n",
    "def dataset_gen_for_aug(split, batch_size): # for training/validation datasets\n",
    "    while True:\n",
    "        true_boxes = []; imgs = []; files = []\n",
    "        \n",
    "        while len(files) < batch_size:\n",
    "            if type(split) == str:\n",
    "                if b'train' in bytes(split, encoding='utf8'): # NOTE, must be bytes here!!!\n",
    "                    line = next(train_gen_csv)\n",
    "                elif b'valid' in bytes(split, encoding='utf8'):\n",
    "                    line = next(valid_gen_csv)\n",
    "                elif b'test' in bytes(split, encoding='utf8'):\n",
    "                    line = next(test_gen_csv)\n",
    "            else:\n",
    "                if b'train' in split: # NOTE, must be bytes here!!!\n",
    "                    line = next(train_gen_csv)\n",
    "                elif b'valid' in split:\n",
    "                    line = next(valid_gen_csv)\n",
    "                elif b'test' in split:\n",
    "                    line = next(test_gen_csv)\n",
    "            \n",
    "            files.append(line.strip())\n",
    "\n",
    "        # parse and get full names\n",
    "        imgs_name, bbox = parse_annotation(files, LABELS, \n",
    "                                           feature_dir=classDir_main_to_imgs,\n",
    "                                            annotation_dir=classDir_main_to)\n",
    "        # do a debug check\n",
    "        for im in imgs_name:\n",
    "            if '.npz' not in im:\n",
    "                print('no np file')\n",
    "                import sys; sys.exit()\n",
    "        \n",
    "        # read in and keep images -- npy files\n",
    "        for im in imgs_name:\n",
    "            b = np.load(im)['arr_0']\n",
    "            # convert 0-1\n",
    "            b = b/255.0\n",
    "            imgs.append(b)\n",
    "                \n",
    "        img = tf.cast(np.array(imgs), tf.float32)        \n",
    "        yield img, tf.cast(bbox, tf.float32)\n",
    "        \n",
    "\n",
    "def get_dataset(split, labels, batch_size, use_aug=True):\n",
    "    if use_aug and ('test' not in split.lower()):\n",
    "        dataset = tf.data.Dataset.from_generator(dataset_gen_for_aug, args=[split, batch_size],\n",
    "                                                 output_types = (tf.float32, tf.float32))\n",
    "    else:\n",
    "        dataset = tf.data.Dataset.from_generator(dataset_gen, args=[split, batch_size],\n",
    "                                             output_types = (tf.float32, tf.float32, \n",
    "                                                             tf.float32, tf.float32))\n",
    "                                             \n",
    "    #dataset = dataset.prefetch(10)\n",
    "    dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
    "    \n",
    "    # maybe?\n",
    "    iterator = iter(dataset)\n",
    "\n",
    "    #return dataset\n",
    "    return iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitsDirIn = None\n",
    "if onGoogle: splitsDirIn = classDirMain\n",
    "train_gen_csv = csv_gen('train', splitsDir=splitsDirIn)\n",
    "valid_gen_csv = csv_gen('valid',splitsDir=splitsDirIn)\n",
    "test_gen_csv = csv_gen('test',splitsDir=splitsDirIn)\n",
    "\n",
    "class ArtificialDatasetNonAug(tf.data.Dataset):\n",
    "\n",
    "    #@tf.function\n",
    "    def _dataset_gen(split, batch_size):\n",
    "        while True:\n",
    "            true_boxes = []; imgs = []; files = []\n",
    "\n",
    "            while len(files) < batch_size:\n",
    "                if type(split) == str:\n",
    "                    if b'train' in bytes(split, encoding='utf8'): # NOTE, must be bytes here!!!\n",
    "                        line = next(train_gen_csv)\n",
    "                    elif b'valid' in bytes(split, encoding='utf8'):\n",
    "                        line = next(valid_gen_csv)\n",
    "                    elif b'test' in bytes(split, encoding='utf8'):\n",
    "                        line = next(test_gen_csv)\n",
    "                else:\n",
    "                    if b'train' in split: # NOTE, must be bytes here!!!\n",
    "                        line = next(train_gen_csv)\n",
    "                    elif b'valid' in split:\n",
    "                        line = next(valid_gen_csv)\n",
    "                    elif b'test' in split:\n",
    "                        line = next(test_gen_csv)\n",
    "\n",
    "                files.append(line.strip())\n",
    "\n",
    "            # parse and get full names\n",
    "            try:\n",
    "                imgs_name, bbox = parse_annotation(files, LABELS, \n",
    "                                                   feature_dir=classDir_main_to_imgs,\n",
    "                                                   annotation_dir=classDir_main_to)\n",
    "            except:\n",
    "                print('error parsing:', imgs_name)\n",
    "            # do a debug check\n",
    "            for im in imgs_name:\n",
    "                if '.npz' not in im:\n",
    "                    print('no np file')\n",
    "                    import sys; sys.exit()\n",
    "\n",
    "            # read in and keep images -- npy files\n",
    "            for im in imgs_name:\n",
    "                b = np.load(im)['arr_0']\n",
    "\n",
    "                ########### DEBUGGING ##########\n",
    "                #b = b[:,:,:3]\n",
    "                ################################\n",
    "\n",
    "                # convert 0-1\n",
    "                b = b/255.0\n",
    "                imgs.append(b)\n",
    "\n",
    "            # finally, format for output\n",
    "            y_true1, y_true2, y_true3 = [],[],[]\n",
    "            for b in bbox:\n",
    "                y1,y2,y3= process_box(b[:,:4], b[:,4].astype('int'),anchors,CLASS)\n",
    "                y_true1.append(y1); y_true2.append(y2); y_true3.append(y3)\n",
    "            # if there is no box, do something different\n",
    "            if len(bbox) == 0:\n",
    "                # fake a box\n",
    "                b = np.array([[111.,  59., 403., 364. ,  4.]])\n",
    "                y1,y2,y3= process_box(b[:,:4], b[:,4].astype('int'),anchors,CLASS)\n",
    "                y1[:] = 0; y2[:]=0;y3[:]=0\n",
    "                y_true1.append(y1); y_true2.append(y2); y_true3.append(y3)        \n",
    "            img = tf.cast(np.array(imgs), tf.float32)        \n",
    "            yield img, tf.cast(y_true1, tf.float32), tf.cast(y_true2, tf.float32), tf.cast(y_true3, tf.float32)\n",
    "\n",
    "\n",
    "\n",
    "    def __new__(cls,batch_size=32,split='valid'):\n",
    "        dataset = tf.data.Dataset.from_generator(cls._dataset_gen, args=[split, batch_size],\n",
    "                                             output_types = (tf.float32, tf.float32, \n",
    "                                                             tf.float32, tf.float32))\n",
    "        return dataset\n",
    "\n",
    "#         #dataset = dataset.prefetch(10)\n",
    "#         dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "#         # maybe?\n",
    "#         iterator = iter(dataset)\n",
    "\n",
    "#         #return dataset\n",
    "#         return iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1638305360505,
     "user": {
      "displayName": "Jill Naiman",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "13328008118965652761"
     },
     "user_tz": 360
    },
    "id": "iCHTV-qu7jy4"
   },
   "outputs": [],
   "source": [
    "# # grab data!\n",
    "# train_dataset = None\n",
    "# # train_dataset= get_dataset('train', LABELS, TRAIN_BATCH_SIZE)\n",
    "\n",
    "# val_dataset = None\n",
    "# val_dataset= get_dataset('valid', LABELS,VAL_BATCH_SIZE,use_aug=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ArtificialDatasetNonAug(batch_size=32,split='valid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# benchmark tests\n",
    "import time\n",
    "t1 = time.perf_counter()\n",
    "print(t1)\n",
    "@tf.function\n",
    "def benchmark(dataset, num_epochs=1, nbatch=32):\n",
    "    start_time = time.perf_counter()\n",
    "    for epoch_num in range(num_epochs):\n",
    "        ienum=0\n",
    "        for sample in dataset:\n",
    "            if ienum%10==0: print(ienum)\n",
    "            # Performing a training step\n",
    "            time.sleep(0.01)\n",
    "            ienum+=1\n",
    "            if ienum>nbatch: break\n",
    "    print(\"Execution time:\", time.perf_counter() - start_time, time.perf_counter())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"args_1:0\", shape=(), dtype=int32)\n",
      "Execution time: 0.04838154900016889 2586.00342969\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/t5/9xgccmv92hnfvjwd62mk8zqh0000gn/T/ipykernel_9839/3746369835.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbenchmark\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mArtificialDatasetNonAug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'valid'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# new paradigm, no prefetch 95.88\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/Paper1/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/Paper1/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    893\u001b[0m       \u001b[0;31m# If we did not create any variables the trace we have is good enough.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       return self._concrete_stateful_fn._call_flat(\n\u001b[0;32m--> 895\u001b[0;31m           filtered_flat_args, self._concrete_stateful_fn.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    896\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfn_with_cond\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minner_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minner_kwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minner_filtered_flat_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/Paper1/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1917\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1918\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1919\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1921\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/Paper1/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    558\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 560\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    561\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m~/anaconda3/envs/Paper1/lib/python3.7/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "benchmark(ArtificialDatasetNonAug(batch_size=32,split='valid')) # new paradigm, no prefetch 95.88"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "Execution time: 99.58878045300003\n"
     ]
    }
   ],
   "source": [
    "benchmark(ArtificialDatasetNonAug(batch_size=32,split='valid').prefetch(tf.data.AUTOTUNE)) # new paradigm, with prefetch, 99.58, but maybe better on other systems?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /var/folders/t5/9xgccmv92hnfvjwd62mk8zqh0000gn/T/ipykernel_9839/425625053.py:76: calling DatasetV2.from_generator (from tensorflow.python.data.ops.dataset_ops) with output_types is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use output_signature instead\n",
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "Execution time: 97.36569318700003\n"
     ]
    }
   ],
   "source": [
    "benchmark(\n",
    "    tf.data.Dataset.range(2)\n",
    "    .interleave(lambda _: ArtificialDatasetNonAug(batch_size=32,split='valid'))\n",
    ") # 97.365"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "Execution time: 73.844264458\n"
     ]
    }
   ],
   "source": [
    "benchmark(\n",
    "    tf.data.Dataset.range(2)\n",
    "    .interleave(\n",
    "        lambda _: ArtificialDatasetNonAug(batch_size=32,split='valid'),\n",
    "        num_parallel_calls=tf.data.AUTOTUNE\n",
    "    )\n",
    ") # 73.844"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "Execution time: 85.55944432500019\n"
     ]
    }
   ],
   "source": [
    "benchmark(\n",
    "    tf.data.Dataset.range(2)\n",
    "    .interleave(\n",
    "        lambda _: ArtificialDatasetNonAug(batch_size=32,split='valid'),\n",
    "        num_parallel_calls=tf.data.AUTOTUNE\n",
    "    ).shuffle(3)\n",
    ") # 85.59"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time: 27.734012822000068\n"
     ]
    }
   ],
   "source": [
    "benchmark(val_dataset) \n",
    "# naive approach gives: 19.24 sec\n",
    "# with prefetch(tf.data.AUTOTUNE): 27.73 sec (maybe its better with different systems?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "`map_func` must return a `Dataset` object. Got <class 'tensorflow.python.data.ops.iterator_ops.IteratorSpec'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/t5/9xgccmv92hnfvjwd62mk8zqh0000gn/T/ipykernel_9839/2554311210.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m     .interleave(\n\u001b[1;32m      4\u001b[0m         \u001b[0;32mlambda\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mval_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0mnum_parallel_calls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAUTOTUNE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     )\n\u001b[1;32m      7\u001b[0m )\n",
      "\u001b[0;32m~/anaconda3/envs/Paper1/lib/python3.7/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36minterleave\u001b[0;34m(self, map_func, cycle_length, block_length, num_parallel_calls, deterministic)\u001b[0m\n\u001b[1;32m   1947\u001b[0m           \u001b[0mblock_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1948\u001b[0m           \u001b[0mnum_parallel_calls\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1949\u001b[0;31m           deterministic=deterministic)\n\u001b[0m\u001b[1;32m   1950\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1951\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/Paper1/lib/python3.7/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, input_dataset, map_func, cycle_length, block_length, num_parallel_calls, buffer_output_elements, prefetch_input_elements, deterministic)\u001b[0m\n\u001b[1;32m   4365\u001b[0m       raise TypeError(\n\u001b[1;32m   4366\u001b[0m           \"`map_func` must return a `Dataset` object. Got {}\".format(\n\u001b[0;32m-> 4367\u001b[0;31m               type(self._map_func.output_structure)))\n\u001b[0m\u001b[1;32m   4368\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_structure\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_map_func\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_structure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_element_spec\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4369\u001b[0m     self._cycle_length = ops.convert_to_tensor(\n",
      "\u001b[0;31mTypeError\u001b[0m: `map_func` must return a `Dataset` object. Got <class 'tensorflow.python.data.ops.iterator_ops.IteratorSpec'>"
     ]
    }
   ],
   "source": [
    "benchmark(\n",
    "    tf.data.Dataset.range(2)\n",
    "    .interleave(\n",
    "        lambda _: val_dataset,\n",
    "        num_parallel_calls=tf.data.AUTOTUNE\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1638305360506,
     "user": {
      "displayName": "Jill Naiman",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "13328008118965652761"
     },
     "user_tz": 360
    },
    "id": "lsQQuI7dzZJw"
   },
   "outputs": [],
   "source": [
    "#next(valid_gen_csv)\n",
    "#next(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1638305360506,
     "user": {
      "displayName": "Jill Naiman",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "13328008118965652761"
     },
     "user_tz": 360
    },
    "id": "OfxcXCqPyXZj"
   },
   "outputs": [],
   "source": [
    "#val_dataset= get_dataset('valid', LABELS, VAL_BATCH_SIZE,use_aug=True)\n",
    "#next(val_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uvgdY6ly7jy4"
   },
   "source": [
    "Including Augmentation like a boss!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1638305360506,
     "user": {
      "displayName": "Jill Naiman",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "13328008118965652761"
     },
     "user_tz": 360
    },
    "id": "nm5833D_7jy5"
   },
   "outputs": [],
   "source": [
    "aug_train_dataset = augmentation_generator(train_dataset, anchors, CLASS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b5doQu9r7jy5"
   },
   "source": [
    "For calculating the loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1638305360507,
     "user": {
      "displayName": "Jill Naiman",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "13328008118965652761"
     },
     "user_tz": 360
    },
    "id": "oWeoMEdE7jy5"
   },
   "outputs": [],
   "source": [
    "class ComputeLoss(object):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    @staticmethod\n",
    "    def compute_loss(y_pred, y_true, anchors):\n",
    "        grid_size = tf.shape(y_pred)[1:3]\n",
    "        ratio = tf.cast(tf.constant([image_size, image_size]) / grid_size, tf.float32)\n",
    "        batch_size = tf.cast(tf.shape(y_pred)[0], tf.float32)\n",
    "\n",
    "        x_y_offset, pred_boxes, pred_conf, pred_prob = process_layer(y_pred, anchors,CLASS)\n",
    "\n",
    "        object_mask = y_true[..., 4:5]\n",
    "\n",
    "        def cond(idx, _):\n",
    "            return tf.less(idx, tf.cast(batch_size, tf.int32))\n",
    "\n",
    "        def body(idx, mask):\n",
    "            valid_true_boxes = tf.boolean_mask(y_true[idx, ..., 0:4],\n",
    "                                               tf.cast(object_mask[idx, ..., 0], 'bool'))\n",
    "            iou = box_iou(pred_boxes[idx], valid_true_boxes)\n",
    "            return idx + 1, mask.write(idx, tf.cast(tf.reduce_max(iou, axis=-1) < 0.2, tf.float32))\n",
    "\n",
    "        ignore_mask = tf.TensorArray(dtype=tf.float32, size=0, dynamic_size=True)\n",
    "        \n",
    "        #print('here1.1')\n",
    "        #print(cond, body, ignore_mask)\n",
    "\n",
    "        _, ignore_mask = tf.while_loop(cond=cond, body=body, loop_vars=[0, ignore_mask])\n",
    "        \n",
    "        #print('here1.2')\n",
    "        ignore_mask = ignore_mask.stack()\n",
    "        ignore_mask = tf.expand_dims(ignore_mask, -1)\n",
    "\n",
    "        true_xy = y_true[..., 0:2] / ratio[::-1] - x_y_offset\n",
    "        pred_xy = pred_boxes[..., 0:2] / ratio[::-1] - x_y_offset\n",
    "\n",
    "        true_tw_th = y_true[..., 2:4] / anchors\n",
    "        pred_tw_th = pred_boxes[..., 2:4] / anchors\n",
    "        true_tw_th = tf.where(tf.equal(true_tw_th, 0), tf.ones_like(true_tw_th), true_tw_th)\n",
    "        pred_tw_th = tf.where(tf.equal(pred_tw_th, 0), tf.ones_like(pred_tw_th), pred_tw_th)\n",
    "        true_tw_th = tf.math.log(tf.clip_by_value(true_tw_th, 1e-9, 1e+9))\n",
    "        pred_tw_th = tf.math.log(tf.clip_by_value(pred_tw_th, 1e-9, 1e+9))\n",
    "\n",
    "        box_loss_scale = y_true[..., 2:3] * y_true[..., 3:4]\n",
    "        box_loss_scale = 2. - box_loss_scale / tf.cast(image_size ** 2, tf.float32)\n",
    "\n",
    "        xy_loss = tf.reduce_sum(tf.square(true_xy - pred_xy) * object_mask * box_loss_scale)\n",
    "        wh_loss = tf.reduce_sum(tf.square(true_tw_th - pred_tw_th) * object_mask * box_loss_scale)\n",
    "\n",
    "        conf_pos_mask = object_mask\n",
    "        conf_neg_mask = (1 - object_mask) * ignore_mask\n",
    "        conf_loss_pos = conf_pos_mask * tf.nn.sigmoid_cross_entropy_with_logits(labels=object_mask, logits=pred_conf)\n",
    "        conf_loss_neg = conf_neg_mask * tf.nn.sigmoid_cross_entropy_with_logits(labels=object_mask, logits=pred_conf)\n",
    "        # try this\n",
    "        #conf_loss_pos = conf_pos_mask * -tf.reduce_sum(object_mask*tf.math.log(tf.clip_by_value(pred_conf,1e-10,1.0)))\n",
    "        #conf_loss_neg = conf_neg_mask * -tf.reduce_sum(object_mask*tf.math.log(tf.clip_by_value(pred_conf,1e-10,1.0)))\n",
    "\n",
    "\n",
    "        conf_loss = tf.reduce_sum((conf_loss_pos + conf_loss_neg))\n",
    "\n",
    "        true_conf = y_true[..., 5:]\n",
    "\n",
    "        class_loss = object_mask * tf.nn.sigmoid_cross_entropy_with_logits(true_conf, pred_prob)\n",
    "        #class_loss = object_mask * -tf.reduce_sum(true_conf*tf.math.log(tf.clip_by_value(pred_conf,1e-10,1.0)))\n",
    "        #class_loss = object_mask * tf.losses.categorical_crossentropy(true_conf, pred_prob)\n",
    "        #tf.losses.sparse_softmax_cross_entropy(y, logits)\n",
    "        class_loss = tf.reduce_sum(class_loss) # sum across all -- 1 number for loss\n",
    "\n",
    "        if np.isnan(xy_loss):\n",
    "          print('xy_loss is NaN')\n",
    "        if np.isnan(wh_loss):\n",
    "          print('wh_loss is NaN')\n",
    "        if np.isnan(conf_loss):\n",
    "          print('conf_loss is NaN')#, conf_loss_pos, conf_loss_neg)\n",
    "        if np.isnan(class_loss):\n",
    "          print('class_loss is NaN')\n",
    "\n",
    "        if np.isnan(xy_loss + wh_loss + conf_loss + class_loss):\n",
    "          print('--- object mask ---')\n",
    "          print(object_mask.numpy().shape, pred_conf.numpy().shape, true_conf.numpy().shape)\n",
    "          print(object_mask.numpy().max(), object_mask.numpy().min())\n",
    "          print(object_mask)\n",
    "          print(' ')\n",
    "          print('--------')\n",
    "        #else:\n",
    "        #  print(object_mask.numpy().max(), object_mask.numpy().min())\n",
    "\n",
    "        return xy_loss + wh_loss + conf_loss + class_loss\n",
    "\n",
    "    def __call__(self, y_pred, y_true):\n",
    "        loss = 0.\n",
    "        anchor_group = [anchors[6:9], anchors[3:6], anchors[0:3]]\n",
    "\n",
    "        for i in range(len(y_pred)):\n",
    "            loss += self.compute_loss(y_pred[i], y_true[i], anchor_group[i])\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1638305360507,
     "user": {
      "displayName": "Jill Naiman",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "13328008118965652761"
     },
     "user_tz": 360
    },
    "id": "XZZsr5247jy6"
   },
   "outputs": [],
   "source": [
    "loss_object = ComputeLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1638305360507,
     "user": {
      "displayName": "Jill Naiman",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "13328008118965652761"
     },
     "user_tz": 360
    },
    "id": "9lBB1U-k0T7-"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1638305360508,
     "user": {
      "displayName": "Jill Naiman",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "13328008118965652761"
     },
     "user_tz": 360
    },
    "id": "ZbiMja467jy6"
   },
   "outputs": [],
   "source": [
    "def compute_loss(y_true, y_pred):\n",
    "    total_loss = loss_object(y_pred, y_true)\n",
    "    return tf.reduce_sum(total_loss) / batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1638305360508,
     "user": {
      "displayName": "Jill Naiman",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "13328008118965652761"
     },
     "user_tz": 360
    },
    "id": "fbZopKn57jy6"
   },
   "outputs": [],
   "source": [
    "def train_step(image, y_true):\n",
    "    with tf.GradientTape() as tape:\n",
    "        y_pred = model(image, training=True)\n",
    "        loss = compute_loss(y_true, y_pred)\n",
    "    if not np.isnan(loss):\n",
    "        variables = model.trainable_variables\n",
    "        gradients = tape.gradient(loss, variables)\n",
    "        #if np.isnan(loss):\n",
    "        #  print('nan')\n",
    "        #  print\n",
    "        optimizer.apply_gradients(zip(gradients, variables))\n",
    "    else: # this will stop if we have non-convergence \n",
    "        print('is NaN -- probably want to lower your learning rate!!!!')\n",
    "        import sys; sys.exit()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1638305360508,
     "user": {
      "displayName": "Jill Naiman",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "13328008118965652761"
     },
     "user_tz": 360
    },
    "id": "l1ftaeHUvvab"
   },
   "outputs": [],
   "source": [
    "# save weights\n",
    "def save_best_weights(model, name, val_loss_avg):\n",
    "    # delete existing weights file\n",
    "    files = glob.glob(os.path.join(weightsDir + 'weights/', name + '*'))\n",
    "    for file in files:\n",
    "        os.remove(file)\n",
    "    # create new weights file\n",
    "    name = name + '_model_' +version + str(val_loss_avg) + '.h5'\n",
    "    path_name = os.path.join(weightsDir +'weights/', name)\n",
    "    model.save_weights(path_name)\n",
    "    \n",
    "    \n",
    "# log (tensorboard)\n",
    "def log_loss(loss, val_loss, step):\n",
    "    tf.summary.scalar('loss', loss, step)\n",
    "    tf.summary.scalar('val_loss', val_loss, step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1638305360509,
     "user": {
      "displayName": "Jill Naiman",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "13328008118965652761"
     },
     "user_tz": 360
    },
    "id": "C19AoZV9loKh",
    "outputId": "e1b08851-f8e4-42ce-da75-ec9bd3dfcfce"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'./classifications/yolo_512x512_ann/'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classDir_main_to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1638305360509,
     "user": {
      "displayName": "Jill Naiman",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "13328008118965652761"
     },
     "user_tz": 360
    },
    "id": "omK8Zjfc7jy6"
   },
   "outputs": [],
   "source": [
    "# training\n",
    "def train(epochs, model, train_dataset, val_dataset, steps_per_epoch_train, \n",
    "          steps_per_epoch_val, optimizer, train_name = 'train'):\n",
    "    '''\n",
    "    Train YOLO model for n epochs.\n",
    "    Eval loss on training and validation dataset.\n",
    "    Log training loss and validation loss for tensorboard.\n",
    "    Save best weights during training (according to validation loss).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    - epochs : integer, number of epochs to train the model.\n",
    "    - model : YOLO model.\n",
    "    - train_dataset : YOLO ground truth and image generator from training dataset.\n",
    "    - val_dataset : YOLO ground truth and image generator from validation dataset.\n",
    "    - steps_per_epoch_train : integer, number of batch to complete one epoch for train_dataset.\n",
    "    - steps_per_epoch_val : integer, number of batch to complete one epoch for val_dataset.\n",
    "    - train_name : string, training name used to log loss and save weights.\n",
    "    \n",
    "    Notes :\n",
    "    - train_dataset and val_dataset generate YOLO ground truth tensors : detector_mask,\n",
    "      matching_true_boxes, class_one_hot, true_boxes_grid. Shape of these tensors (batch size, tensor shape).\n",
    "    - steps per epoch = number of images in dataset // batch size of dataset\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    - loss history : [train_loss_history, val_loss_history] : list of average loss for each epoch.\n",
    "    '''\n",
    "    num_epochs1 = epochs\n",
    "    steps_per_epoch_train = steps_per_epoch_train\n",
    "    steps_per_epoch_val = steps_per_epoch_val\n",
    "    train_loss_history = []\n",
    "    val_loss_history = []\n",
    "    best_val_loss = 1e6\n",
    "    \n",
    "    # optimizer\n",
    "    #optimizer = tf.keras.optimizers.Adam(learning_rate=1e-5, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
    "    \n",
    "    # log (tensorboard)\n",
    "    summary_writer = tf.summary.create_file_writer(os.path.join(logsDir+'logs/', train_name), \n",
    "                                                   flush_millis=20000)\n",
    "    summary_writer.set_as_default()\n",
    "    \n",
    "    # training\n",
    "    for epoch in range(num_epochs1):\n",
    "        epoch_loss = []\n",
    "        epoch_val_loss = []\n",
    "        epoch_val_sub_loss = []\n",
    "        print('Epoch {} :'.format(epoch))\n",
    "        # train\n",
    "        for batch_idx in range(steps_per_epoch_train):        \n",
    "            image, y_true_1, y_true_2, y_true_3 = next(train_dataset)\n",
    "            y_true = (y_true_1, y_true_2, y_true_3)\n",
    "            loss = train_step(image, y_true)\n",
    "            # check for nans\n",
    "            optOrig = optimizer.learning_rate.lr\n",
    "            while np.isnan(loss):\n",
    "              print('loss nan')\n",
    "              optimizer.learning_rate.lr *= 0.5\n",
    "              loss = train_step(image, y_true)\n",
    "            epoch_loss.append(loss)\n",
    "            print('-', end='')\n",
    "        print(' | ', end='')\n",
    "        # val\n",
    "        for batch_idx in range(steps_per_epoch_val): \n",
    "            image, y_true_1, y_true_2, y_true_3 = next(val_dataset)\n",
    "            y_true = (y_true_1, y_true_2, y_true_3)\n",
    "            loss = train_step(image, y_true)\n",
    "            epoch_val_loss.append(loss)\n",
    "            print('-', end='')\n",
    "\n",
    "        loss_avg = np.mean(np.array(epoch_loss))\n",
    "        val_loss_avg = np.mean(np.array(epoch_val_loss))\n",
    "        train_loss_history.append(loss_avg)\n",
    "        val_loss_history.append(val_loss_avg)\n",
    "        \n",
    "        # log\n",
    "        log_loss(loss_avg, val_loss_avg, epoch)\n",
    "        \n",
    "        # save\n",
    "        if val_loss_avg < best_val_loss:\n",
    "            save_best_weights(model, train_name, val_loss_avg)\n",
    "            best_val_loss = val_loss_avg\n",
    "            #tf.saved_model.save(model, chksDir + 'checkpoints/'+today)        \n",
    "        print(' loss = {:.4f}, val_loss = {:.4f}'.format(loss_avg, val_loss_avg))\n",
    "        \n",
    "    return [train_loss_history, val_loss_history]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "l2FpED3Dvvac",
    "outputId": "53dfa0c1-8492-4858-d7c6-e9e5ae480a29"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 :\n",
      "-----------------------"
     ]
    }
   ],
   "source": [
    "results = train(num_epochs, model, aug_train_dataset, val_dataset, \n",
    "               steps_training, steps_val, optimizer,'training_1'+extraName)\n",
    "\n",
    "# debug\n",
    "#results = train(num_epochs, model, aug_train_dataset, val_dataset, \n",
    "#               1, 1, optimizer,'training_1'+extraName)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oqGPwg0R7jy6"
   },
   "source": [
    "Plot diagnostics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "68HC1xUF7jy6"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(30,14))\n",
    "ax.plot(results[0], label='Training Loss')\n",
    "ax.plot(results[1], label='Validation Loss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EjYMDlLo7jy7"
   },
   "outputs": [],
   "source": [
    "# imgs_name, bbox = parse_annotation([X_train[0]], LABELS, \n",
    "#                                    classDir_main_to_imgs=classDir_main_to_imgs,\n",
    "#                                        classDir_main_to_ann=classDir_main_to)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "theCs-467jy8"
   },
   "outputs": [],
   "source": [
    "# from importlib import reload\n",
    "# reload(parse_annotation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "orYNx10Y7jy8"
   },
   "outputs": [],
   "source": [
    "#import general_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1OXBhSDS7jy8"
   },
   "outputs": [],
   "source": [
    "#reload(general_utils)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tKhsRWNh7jy8"
   },
   "outputs": [],
   "source": [
    "#from general_utils import parse_annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8eIrQ2HH5C8Z"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wXSKvTs95Pp6"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ihigRX4m5QaB"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "mega_yolo_train.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
