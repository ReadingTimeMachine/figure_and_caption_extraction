{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8f20a82-45ee-4db1-8ba1-b5764e187a58",
   "metadata": {},
   "source": [
    "# Test how well we can write and read TFrecords files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "439e9b1f-5816-465e-8ea5-182696c4e4af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# where to save tmp files?\n",
    "binaries_file = '/Users/jillnaiman/Downloads/tmp/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "89a22796-de1e-46ec-ab12-b3c4ff59dd34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sys import path\n",
    "path.append('../')\n",
    "import config\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "import tensorflow as tf\n",
    "from annotation_utils import get_all_ocr_files, make_ann_directories, collect_ocr_process_results, \\\n",
    "   get_makesense_info_and_years, get_years, get_cross_index, get_pdffigures_info, get_annotation_name, \\\n",
    "   true_box_caption_mod\n",
    "from post_processing_utils import parse_annotations_to_labels\n",
    "from feature_generation_utils import generate_single_feature\n",
    "from general_utils import parse_annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf0c163-f54b-440b-a75f-f8f65c1f8efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's get all of the ocr files\n",
    "ocrFiles = get_all_ocr_files()\n",
    "# get important quantities from these files\n",
    "print('retreiving OCR data, this can take a moment...')\n",
    "ws, paragraphs, squares, html, rotations,colorbars = collect_ocr_process_results(ocrFiles)\n",
    "# create dataframe\n",
    "df = pd.DataFrame({'ws':ws, 'paragraphs':paragraphs, 'squares':squares, \n",
    "                   'hocr':html, 'rotation':rotations, 'colorbars':colorbars})#, 'pdfwords':pdfwords})\n",
    "df = df.drop_duplicates(subset='ws')\n",
    "df = df.set_index('ws')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cb228957-4b40-4bbf-a65d-90eaf739f839",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get annotations\n",
    "imgDirAnn = config.save_binary_dir + config.ann_name + str(int(config.IMAGE_H)) + 'x' + str(int(config.IMAGE_W))  + '_ann/'\n",
    "# get all annotations\n",
    "annotations = glob(imgDirAnn+'*.xml')\n",
    "\n",
    "LABELS, labels, slabels, \\\n",
    "  CLASS, annotations, Y_full, maxboxes = parse_annotations_to_labels(imgDirAnn, \n",
    "                                                           '', \n",
    "                                                           benchmark=True,\n",
    "                                                          return_max_boxes=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "809c8307-cd6d-4a8e-95dd-ff737910c528",
   "metadata": {},
   "outputs": [],
   "source": [
    "iw = 5\n",
    "img_resize=(config.IMAGE_H, config.IMAGE_W)\n",
    "\n",
    "fname = annotations[iw].split('/')[-1].split('.xml')[0]\n",
    "#floc = binaries_file + fname + '.npz'\n",
    "\n",
    "dfsingle = df.loc[fname+'.jpeg']\n",
    "\n",
    "# if we've made it this far, let's generate features\n",
    "feature_list = ['grayscale']\n",
    "feature_name = generate_single_feature(dfsingle, LABELS, maxboxes, \n",
    "                                       feature_list = feature_list, \n",
    "                                       binary_dir = binaries_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7213fdde-b378-425b-bf28-b036fc077579",
   "metadata": {},
   "outputs": [],
   "source": [
    "# different kinds of features\n",
    "def _bytes_feature(value):\n",
    "    \"\"\"Returns a bytes_list from a string / byte.\"\"\"\n",
    "    if isinstance(value, type(tf.constant(0))):\n",
    "        value = value.numpy() # BytesList won't unpack a string from an EagerTensor.\n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "\n",
    "def _float_feature(value):\n",
    "    \"\"\"Returns a float_list from a float / double.\"\"\"\n",
    "    return tf.train.Feature(float_list=tf.train.FloatList(value=[value]))\n",
    "\n",
    "def _int64_feature(value):\n",
    "    \"\"\"Returns an int64_list from a bool / enum / int / uint.\"\"\"\n",
    "    return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7ee415e0-d8b0-4c8d-9eb9-3510cbe12c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary with features that may be relevant.\n",
    "def image_example(image, boxes, img_name):\n",
    "    #image_shape = tf.io.decode_jpeg(image_string).shape\n",
    "    image_string = image.astype('float32')/255.0\n",
    "    image_string = image.reshape(image.shape[0]*image.shape[1]*image.shape[2])\n",
    "\n",
    "    nfeatures = image.shape[2]\n",
    "    nboxes = boxes.shape[0]\n",
    "    if nboxes>0:\n",
    "        boxout = boxes.reshape(boxes.shape[0]*boxes.shape[1])\n",
    "    else:\n",
    "        boxout = np.array([])\n",
    "\n",
    "    feature = {\n",
    "      'nbox': _float_feature(np.float32(nboxes)),\n",
    "      'nfeatures': _float_feature(np.float32(nfeatures)),\n",
    "      'boxes': _bytes_feature(boxout.astype('float32').tobytes()),\n",
    "      'image_raw': _bytes_feature(image_string.astype('float32').tobytes()),\n",
    "      #'image_name': _bytes_feature(img_name.tobytes()),\n",
    "      'image_name': _bytes_feature(img_name.encode('utf-8')),\n",
    "    }\n",
    "    return tf.train.Example(features=tf.train.Features(feature=feature))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "cb8f988b-8572-4f8d-8d8d-f08c81a03db5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'hi'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'hi'.encode('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "056ae4cd-050b-4e50-950b-6caa80fbf4ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a temp record file to see how big each file is, on avearge\n",
    "# write one image file and see how big it is\n",
    "record_file = binaries_file+'TMPTFRECORD/test.tfrecords'\n",
    "compress = 'GZIP'\n",
    "tf_record_options = tf.io.TFRecordOptions(compression_type = compress) \n",
    "\n",
    "#with tf.io.TFRecordWriter(record_file) as writer:\n",
    "with tf.io.TFRecordWriter(record_file, options=tf_record_options) as writer:\n",
    "    a = imgDirAnn + annotations[iw].split('/')[-1]\n",
    "    imgs_name, bbox = parse_annotation([a], LABELS,\n",
    "                                           feature_dir=config.tmp_storage_dir+'TMPTFRECORD/',\n",
    "                                           annotation_dir=imgDirAnn) \n",
    "    arr = np.load(imgs_name[0])['arr_0']\n",
    "\n",
    "    # fake boxes\n",
    "    fakebox = np.random.random([maxboxes,5])\n",
    "    tf_example = image_example(arr,fakebox,imgs_name[0])\n",
    "    writer.write(tf_example.SerializeToString())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8d81139-f2b1-4898-b6d0-21f8b2954908",
   "metadata": {},
   "source": [
    "## Now, let's try to read this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "25a3dc6f-1085-4f22-b774-b283a837d4ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for tfrecrords, get datasets\n",
    "test_list = [binaries_file+'TMPTFRECORD/test.tfrecords']\n",
    "test_raw_data = tf.data.TFRecordDataset(filenames=test_list, \n",
    "                                         compression_type='GZIP', \n",
    "                                         buffer_size=None, \n",
    "                                        num_parallel_reads=tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6f4b35a3-dbe4-4f61-a7e7-d4f3196401eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary describing the features.\n",
    "image_feature_description = {\n",
    "    'nbox': tf.io.FixedLenFeature([], tf.float32),\n",
    "    'nfeatures': tf.io.FixedLenFeature([], tf.float32),\n",
    "    'boxes': tf.io.FixedLenFeature([], tf.string),\n",
    "    'image_raw': tf.io.FixedLenFeature([], tf.string),\n",
    "    'image_name': tf.io.FixedLenFeature([], tf.string),\n",
    "}\n",
    "\n",
    "def _parse_image_function_test(example_proto):\n",
    "    image_features = tf.io.parse_single_example(example_proto, image_feature_description)\n",
    "    # parse the data\n",
    "    nboxes = image_features['nbox']\n",
    "    nfeatures = image_features['nfeatures']\n",
    "    boxes = tf.io.decode_raw(image_features['boxes'],tf.float32)\n",
    "    boxes = tf.reshape(boxes,[nboxes,5])\n",
    "    images_raw = image_features['image_raw']\n",
    "    image = tf.io.decode_raw(images_raw,tf.float32)\n",
    "    image = tf.reshape(image,[config.IMAGE_H,config.IMAGE_W,nfeatures])\n",
    "    img_name = tf.cast(image_features['image_name'],tf.string)\n",
    "    #print(img_name)\n",
    "    #image_name = tf.io.decode_raw(image_features['image_name'],tf.string) \n",
    "    #print(image_name)\n",
    "    return image,img_name,nboxes,nfeatures,boxes\n",
    "\n",
    "test_dataset = test_raw_data.interleave(lambda x: test_raw_data.map(lambda example_proto:_parse_image_function_test(example_proto), \n",
    "                                    num_parallel_calls=tf.data.AUTOTUNE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e06a478e-1bef-493a-80c1-f2660b2f9003",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(1.0, shape=(), dtype=float32)\n",
      "tf.Tensor(b'/\\x00\\x00\\x00U\\x00\\x00\\x00s\\x00\\x00\\x00e\\x00\\x00\\x00r\\x00\\x00\\x00s\\x00\\x00\\x00/\\x00\\x00\\x00j\\x00\\x00\\x00i\\x00\\x00\\x00l\\x00\\x00\\x00l\\x00\\x00\\x00n\\x00\\x00\\x00a\\x00\\x00\\x00i\\x00\\x00\\x00m\\x00\\x00\\x00a\\x00\\x00\\x00n\\x00\\x00\\x00/\\x00\\x00\\x00D\\x00\\x00\\x00o\\x00\\x00\\x00w\\x00\\x00\\x00n\\x00\\x00\\x00l\\x00\\x00\\x00o\\x00\\x00\\x00a\\x00\\x00\\x00d\\x00\\x00\\x00s\\x00\\x00\\x00/\\x00\\x00\\x00t\\x00\\x00\\x00m\\x00\\x00\\x00p\\x00\\x00\\x00/\\x00\\x00\\x00T\\x00\\x00\\x00M\\x00\\x00\\x00P\\x00\\x00\\x00T\\x00\\x00\\x00F\\x00\\x00\\x00R\\x00\\x00\\x00E\\x00\\x00\\x00C\\x00\\x00\\x00O\\x00\\x00\\x00R\\x00\\x00\\x00D\\x00\\x00\\x00/\\x00\\x00\\x001\\x00\\x00\\x008\\x00\\x00\\x009\\x00\\x00\\x005\\x00\\x00\\x00A\\x00\\x00\\x00p\\x00\\x00\\x00J\\x00\\x00\\x00_\\x00\\x00\\x00_\\x00\\x00\\x00_\\x00\\x00\\x00_\\x00\\x00\\x00_\\x00\\x00\\x001\\x00\\x00\\x00_\\x00\\x00\\x00_\\x00\\x00\\x003\\x00\\x00\\x000\\x00\\x00\\x005\\x00\\x00\\x00P\\x00\\x00\\x00_\\x00\\x00\\x00p\\x00\\x00\\x000\\x00\\x00\\x00.\\x00\\x00\\x00n\\x00\\x00\\x00p\\x00\\x00\\x00z\\x00\\x00\\x00', shape=(), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "# show\n",
    "for image,img_name,nbox,nfeatures,boxes in test_dataset.take(1):\n",
    "    #print(nbox,nfeatures,boxes)\n",
    "    print(nfeatures)\n",
    "    print(img_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a3dfaa-c8ea-4148-a40f-65e675270824",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
